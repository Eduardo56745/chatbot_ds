
### 1._fundamentos_de_hojas_de_clculo.pdf ###
1. Fundamentos de hojas de cálculo
1
1. Fundamentos de hojas de 
cálculo
1.1 Cómo acceder a los datos de una hoja de cálculo
Formatos comunes de archivos de hojas de cálculo
.xlsx  (Excel): adecuado para análisis de datos complejos con funciones 
de fórmulas, gráficos y distintas opciones de formato.
.csv  (es decir, "comma-separated values" o valores separados por 
comas): formato simplificado, solo texto, ideal para la representación 
tabular de datos y una amplia compatibilidad de programas.
Cómo importar datos a Google Sheets
Para importar un archivo de Excel ( .xlsx ) a Google Sheets, solo ve a 
"Archivo > Importar > Subir", elige tu archivo y selecciona "Reemplazar 
hoja de cálculo" para agregar los datos de Excel a tu hoja.
Para un archivo CSV ( .csv ) sigue los mismos pasos, pero al momento de 
importar asegúrate de seleccionar "Coma" como el tipo de separador para 
que tus datos tengan el formato correcto.
Cómo guardar Google Sheets como archivos de Excel o CSV
Utiliza "Archivo > Descargar" para guardar tus datos en diferentes 
formatos.
La selección entre los formatos de Excel y CSV depende de las 
necesidades específicas del usuario para la complejidad de los datos y su 
intercambio.
1.2 Estructura de las hojas de cálculo
Conceptos básicos:
Fila. Línea de datos horizontal identificada por números.
Columna. Línea de datos vertical identificada por letras.
1. Fundamentos de hojas de cálculo
2
Celda. Intersección de una fila y una columna, identificada por una letra y 
un número (p. ej., A1).
Hoja. Pestaña individual dentro de un archivo de hoja de cálculo que 
contiene datos relacionados entre sí.
Gestión de las hojas
Creación de hojas: agrega más hojas utilizando el botón "+".
Renombrar hojas: haz doble clic en la pestaña de la hoja o usa el menú 
que se despliega con un clic derecho del ratón.
Navegación: haz clic en las pestañas de la hoja para cambiar las vistas.
Eliminar: elimina hojas con un clic derecho y selecciona "Eliminar".
1.3 Tipos de datos
Los tipos de datos categorizan la clase de datos que podemos almacenar y 
manipular en una hoja de cálculo. El procesamiento de datos preciso requiere de 
una correcta identificación de los tipos de datos, lo que permite cálculos y 
operaciones adecuadas.
Tipos de datos en Google Sheets
Datos numéricos
Enteros: números completos sin partes decimales, como 42 , 0 , y -99 .
Números decimales: números con partes decimales, que se tratan de forma 
similar en Google Sheets, pero que se distinguen en los lenguajes de 
programación, como 3.14 , -0.001  y 2.718 .
Datos de texto
Strings: son colecciones de caracteres que incluyen texto, números como 
texto y símbolos, lo que permite varias manipulaciones de texto. Ejemplos de 
strings o cadenas son "Hola"  y "A100B" .
Datos de Fecha/Hora
Fecha: fechas específicas del calendario.
Hora: horas específicas del día.
1. Fundamentos de hojas de cálculo
3
Fecha/Hora: una combinación de fecha con hora.
Duración: longitud de tiempo entre dos puntos, que puede ser positiva o 
negativa.
Tipo
Descripción
Formatos de ejemplo
Valores de ejemplo
Fecha
Fecha del calendario
MM/DD/AAAA
AAAA-MM-DD
06/13/2015
2015-06-13
Hora
Hora del día
HH:MM:SS xx
HH:MM:SS
05:31:06 PM
17:31:06
Fecha/Hora
Fecha del calendario
con hora del día
MM/DD/YYYY
HH:MM:SS
06/13/2025
17:31:06
Duración
Cantidad de tiempo
transcurrido entre
dos fechas, horas o
fecha y hora
hours:MM:SS
1269:50:30
(derivado del
2022/03/04 1:53:41
- 2022/01/10
4:03:11 )
1.4 Funciones y fórmulas
Funciones vs. Fórmulas
Fórmula: cualquier cálculo ingresado en una celda, que comienza con = .
Función: una operación predefinida que se usa en las fórmulas, que se 
reconoce por nombres seguidos de paréntesis (p. ej., SUM() ).
Funciones comunes:
=SUM(range) : suma todos los números en un rango.
=AVERAGE(range) : calcula el promedio de los números en un rango.
=COUNT(range) : cuenta el número de celdas en un rango que contienen 
números.
=MAX(range) : encuentra el número más grande en un rango.
=MIN(range) : encuentra el número más pequeño en un rango.
Cómo escribir fórmulas efectivas
1. Fundamentos de hojas de cálculo
4
La sintaxis esencial incluye el comenzar con = , referenciar las celdas 
correctamente (p. ej., A1 ) e incorporar funciones de forma adecuada. 
Ejemplo: =SUM(D2, D6, D12)  suma los valores en las celdas D2 , D6  y D12 .
Cómo aplicar las fórmulas en las celdas
Para extender los cálculos en una columna, arrastra la fórmula de una 
celda a otras.
Referencias fijas de celdas
Usa $  para bloquear filas o columnas al copiar fórmulas, para lo que 
asegura puntos de referencia consistentes. Ejemplo: =$A$1  mantiene 
constante la referencia a A1  a través de múltiples celdas.
Buenas prácticas
Escribe los nombres de las funciones en mayúsculas, usa espacios para 
facilitar la lectura y asegúrate de que las referencias a las celdas sean 
claras. Ejemplo: =AVERAGE(D5, D7, D11)  claramente calcula el promedio de los 
valores en las celdas D5 , D7  y D11 .
Errores comunes
#ERROR! : indica un error de sintaxis.
#VALUE! : ocurre cuando se usa un tipo de dato equivocado en una fórmula.
#DIV/0! : aparece al dividir entre cero.

### 1_Resumen_del_captulo_Lectura_y_visualizacin_de_datos.pdf ###
Resumen del capítulo: Lectura y visualización de datos
1
Resumen del capítulo: Lectura y 
visualización de datos
Solucionar problemas con archivos CSV
Recuerda que CSV significa valores separados por comas. Sin embargo, un archivo 
CSV no tiene que usar solo una coma como delimitador; se puede usar cualquier 
carácter. Por ejemplo, los valores separados por tabuladores son otro formato común.
Puedes cambiar el delimitador utilizando el parámetro sep= . También puedes 
establecer los nombres de las columnas y crear un encabezado con los parámetros 
names=  y header= :
import pandas as pd 
 
column_names = [ 
    'country', 
    'name', 
    'capacity_mw', 
    'latitude', 
    'longitude', 
    'primary_fuel', 
    'owner' 
] 
data = pd.read_csv('/datasets/gpp_modified.csv', sep='|', header=None, names=column_names) 
 
print(data.head())
Subir diferentes hojas de archivos Excel
Los archivos de Excel pueden constar de varias hojas. Al usar la función read_excel() , 
importas solo la primera hoja de forma predeterminada. Si necesitas trabajar con la otra 
hoja, utiliza el parámetro sheet_name= :
import pandas as pd 
 
df = pd.read_excel('/datasets/product_reviews.xlsx', sheet_name='reviewers') 
 
print(df.head())
Resumen del capítulo: Lectura y visualización de datos
2
Observación de los datos
Nunca es mala idea llamar al método info()  cada vez que empieces a trabajar con un 
nuevo DataFrame. El método info()  no devuelve nada, sino que imprime información 
general sobre el DataFrame. Pero para obtener una visión aún más completa de tus 
datos, también puede ser necesario usar el método sample()  que selecciona filas 
aleatorias del DataFrame:
import pandas as pd 
 
column_names = [ 
    'country', 
    'name', 
    'capacity_mw', 
    'latitude', 
    'longitude', 
    'primary_fuel', 
    'owner' 
] 
data = pd.read_csv( 
    '/datasets/gpp_modified.csv', 
    sep='|', 
    header=None, 
    names=column_names, 
    decimal=',', 
) 
 
print(data.sample(5))
Descripciones numéricas y describe()
Y una comprensión aún más avanzada de los datos se puede obtener con el método 
describe() , que muestra las principales medidas estadísticas del conjunto de datos, 
incluida la desviación estándar, los valores mínimos y máximos, cuando se solicitan 
valores numéricos:
 
print(data.describe())
        capacity_mw      latitude     longitude 
count  34936.000000  34936.000000  34936.000000 
mean     163.355148     32.816637     -6.972803 
Resumen del capítulo: Lectura y visualización de datos
3
std      489.636072     22.638603     78.405850 
min        1.000000    -77.847000   -179.977700 
25%        4.900000     29.256475    -77.641550 
50%       16.745000     39.727750     -2.127100 
75%       75.344250     46.263125     49.502675 
max    22500.000000     71.292000    179.388700
Sin embargo, para los valores categóricos, el método describe()  produce un efecto 
ligeramente diferente, centrándose principalmente en la cantidad de valores únicos y 
los más frecuentes:
print(data['country'].describe())
count                        34936 
unique                         167 
top       United States of America 
freq                          9833 
Name: country, dtype: object

### 2.Resumen_del_captulo_Teora_de_la_probabilidad.pdf ###
Resumen del capítulo: Teoría de la probabilidad
1
Resumen del capítulo: Teoría de 
la probabilidad
Experimentos, resultados elementales y eventos
Un experimento es una prueba repetible en la que se produce uno de varios 
resultados. Un resultado puede estar compuesto por varios resultados elementales 
que, por definición, no pueden desglosarse más.
En las situaciones más sencillas todos los resultados elementales son igualmente 
probables. Podemos llamar a tal experimento un "experimento justo". En un 
experimento justo con n resultados elementales, la probabilidad de cada resultado es 
1/n.
El conjunto de todos los posibles resultados elementales de un experimento se llama 
espacio muestral. Puede seleccionar un subconjunto que contenga varios resultados 
elementales. Esto se llama un evento.
Un evento imposible es un evento que nunca puede suceder por lo que la 
probabilidad de que ocurra es 0. Un evento seguro es un evento que definitivamente 
ocurrirá por lo que su probabilidad es igual a 1. La probabilidad de otros eventos está 
entre 0 y 1.
Siempre que mantengas la condición de que todos los resultados elementales tengan 
la misma probabilidad, la probabilidad del evento será el número de resultados 
elementales en el evento dividido entre el número total de resultados (es decir, el 
tamaño del espacio de muestra). De manera más general (incluso cuando los 
resultados elementales no son igualmente probables), la probabilidad de un evento 
será equivalente a la suma de las probabilidades de sus resultados elementales 
constituyentes.
La ley de los grandes números
La ley de los grandes números dice que cuantas más veces repitas un experimento, 
más se acercará la frecuencia de un evento dado a su probabilidad.
Resumen del capítulo: Teoría de la probabilidad
2
Podemos usar esta regla a la inversa. Si no conocemos la probabilidad de un evento, 
pero podemos repetir el experimento muchas veces, podemos estimar su probabilidad 
a partir de la frecuencia de los resultados.
Eventos mutuamente excluyentes e independientes; 
Multiplicar probabilidades
Para ilustrar la intersección entre eventos, puedes usar un diagrama de Venn:
Si los eventos A y B se cruzan, significa que hay resultados elementales que ocurren 
tanto en A como en B.
Los eventos que no pueden ocurrir simultáneamente en el mismo experimento se 
denominan mutuamente excluyentes: su diagrama de Venn no muestra ninguna 
intersección:
Resumen del capítulo: Teoría de la probabilidad
3
La probabilidad de que ambos eventos mutuamente excluyentes ocurran es cero.
Los eventos se denominan independientes si la ocurrencia de uno no afecta la 
probabilidad del otro. Si los eventos son independientes, entonces la probabilidad de su 
intersección es igual al producto de sus probabilidades de ocurrencia. Esta regla 
funciona en sentido inverso.
Si los eventos mutuamente excluyentes ocupan todo el espacio muestral, la suma de 
sus probabilidades será 1.
Puedes saber si los eventos son mutuamente excluyentes a partir de un diagrama de 
Venn. No es tan fácil ilustrar la independencia; necesitas comprobar si el producto de 
las probabilidades de los eventos es igual a la probabilidad de la intersección de estos 
eventos.
Probabilidad de éxito en experimentos binomiales
Los experimentos con dos resultados posibles se conocen como experimentos 
binomiales. Por lo general, aunque no siempre, uno de los resultados se llama "éxito" 
Resumen del capítulo: Teoría de la probabilidad
4
y el otro "fracaso". Si la probabilidad de éxito es p, la probabilidad de fracaso es 1 - p, 
ya que la suma de las probabilidades de los resultados debe ser igual a 1.
La distribución binomial
El número de formas de obtener k éxitos a partir de n repeticiones de un experimento 
se puede encontrar utilizando la fórmula:
donde ! (leído como "factorial") es igual al producto de números naturales desde 1 
hasta el número dado: n! = 1 ⋅ 2 ⋅ 3 ⋅ 4 ⋅ … ⋅ (n-1) ⋅ n.
Puedes calcular el factorial usando la librería de matemáticas y su método factorial :
from math import factorial 
x = factorial(5)
Aquí están las condiciones que nos permiten confirmar que la variable aleatoria se 
distribuye de forma binomial:
Se realiza un número fijo y finito de intentos (n)
Cada intento es un simple experimento binomial con exactamente dos resultados
Los intentos son independientes entre sí
La probabilidad de éxito (p) es la misma para todos los n intentos
La distribución normal
El teorema del límite central es un teorema clave en estadística. En términos algo 
simplificados, establece que "Muchas variables aleatorias independientes, sumadas, 
dan una distribución normal".
La distribución normal describe valores continuos reales. Cuenta con dos parámetros, 
media y varianza:
X ∼ ℕ(μ, σ2)
C
=
n
k
k!(n −k)!
n!
Resumen del capítulo: Teoría de la probabilidad
5
Esta notación se puede leer como: La variable X tiene una distribución normal con una 
media de mu (μ) y una varianza de sigma al cuadrado (σ²) (que corresponde a una 
desviación estándar de sigma).
Para encontrar la probabilidad de que ocurra cualquier intervalo determinado a partir de 
parámetros de distribución conocidos, llamamos a dos métodos del paquete scipy.stats: 
norm.ppf y norm.cdf.
ppf : percent point function (función de punto porcentual).
cdf : cumulative distribution function (función de distribución acumulada).
Ambos funcionan con la distribución normal, dada una media particular (valor 
esperado) y una desviación estándar.
norm.ppf  proporciona el valor de una variable cuando se conoce la probabilidad del 
intervalo a la izquierda de ese valor.
norm.cdf , por otro lado, proporciona la probabilidad del intervalo a la izquierda del 
valor cuando este valor es conocido.
Calcula la distribución normal usando el método norm()  del paquete scipy.stats con dos 
argumentos: valor esperado y desviación estándar. Vamos a encontrar la probabilidad 
de obtener un valor particular, x:
from scipy import stats as st 
 
# establece una distribución normal 
distr = st.norm(1000, 100)  
 
x = 1000 
 
result = distr.cdf(x) # calcula la probabilidad de obtener el valor x
Utilizando la función norm.cdf  podemos calcular la probabilidad de obtener un valor en 
el intervalo entre x1 y x2:
from scipy import stats as st 
 
# establece una distribución normal 
distr = st.norm(1000, 100)  
 
x1 = 900 
x2 = 1100 
Resumen del capítulo: Teoría de la probabilidad
6
 
result = distr.cdf(x2) - distr.cdf(x1)  
# calcula la probabilidad de obtener un valor entre x1 y x2
Para encontrar un valor que tiene una cierta probabilidad, usamos el método norm.ppf :
from scipy import stats as st 
 
# establece una distribución normal 
distr = st.norm(1000, 100)  
 
p1 = 0.841344746 
 
result = distr.ppf(p1)

### 2._limpieza_de_datos_y_preprocesamiento.pdf ###
2. Limpieza de datos y preprocesamiento
1
2. Limpieza de datos y 
preprocesamiento
2.1 Comprensión de los datos limpios
Los datos limpios se refieren a conjuntos de datos bien organizados, fáciles de 
analizar y estructurados de acuerdo a reglas específicas que facilitan un análisis 
preciso y evita errores.
Los pilares de los datos limpios
Cada variable forma una columna
Una variable representa una característica o atributo específico, como el 
color de un auto o la edad de una persona, y cada una debe ocupar su 
propia columna dentro de un conjunto de datos.
Name
Age
Sex
Height
Ocean
11
F
4'3"
Donald
14
M
4'5"
Charles
31
M
6'1"
Betsy
31
F
5'5"
Cada observación forma una fila
Una observación es un único registro o instancia de datos que combina 
diferentes variables. En un conjunto de datos, cada fila debe representar 
una única observación.
Name
Join Date
Order Date
Raheem
01/20/2020
9/14/2021
Fatima
05/30/2019
10/6/2021
Carla
12/11/2022
1/29/2022
Denis
04/05/2021
3/4/2020
2. Limpieza de datos y preprocesamiento
2
Cada tipo de unidad de observación forma una tabla
Diferentes tipos de datos (p. ej. "orders" (pedidos) y "customers" 
(clientes)) deben segregarse en sus propias tablas para mantener claridad 
y evitar la duplicidad.
Order ID
Order Date
Customer ID
1
9/14/2021
1
2
10/6/2021
2
3
1/29/2022
3
4
3/4/2020
4
2.2 Cómo limpiar datos importados
Cómo dividir columnas mediante un separador
La función "Dividir texto en columnas" de Google Sheets te permite dividir una 
columna en varias, utilizando un carácter específico como el separador. Esto es 
especialmente útil para separar strings de datos complejos en componentes más 
manejables y analizables. 
Antes de dividir:
User_ID
Contact_info
Registration_date
U001
"JohnDoe|john.doe@example.com"
2022-01-15
U002
"JaneSmith|jane.smith@example.com"
2022-03-22
U003
"AlexJohnson|alex.johnson@example.net"
2022-07-05
Después de dividir:
User_ID
Name
Email
Registration_date
U001
JohnDoe
john.doe@example.com
2022-01-15
U002
JaneSmith
jane.smith@example.com
2022-03-22
U003
AlexJohnson
alex.johnson@example.net
2022-07-05
Cómo eliminar filas duplicadas
2. Limpieza de datos y preprocesamiento
3
Las filas duplicadas pueden distorsionar el análisis de datos. "Eliminar duplicados" 
se usa para ayudarte a identificar y eliminar estas entradas redundantes con base 
en columnas seleccionadas que identifican cada renglón de forma única.
Antes de eliminar duplicados:
TransactionID
Date and Time
Amount
TX123
2023-04-01T10:00:00.000Z
$150
TX123
2023-04-01T10:00:00.000Z
$150
TX456
2023-04-02T10:00:00.000Z
$200
TX456
2023-04-02T10:00:00.000Z
$200
Después de eliminar duplicados:
TransactionID
Date and Time
Amount
TX123
2023-04-01T10:00:00.000Z
$150
TX456
2023-04-02T10:00:00.000Z
$200
Cómo buscar y reemplazar texto
"Buscar y reemplazar es una herramienta para hacer cantidades grandes de 
modificaciones en tu conjunto de datos. Ya sea corregir errores ortográficos 
comunes o actualizar información en varias entradas, esta función agiliza el 
proceso.
Antes de Buscar y reemplazar:
Product ID
Product Name
Price
P001
Grahpical Card
$299
P002
Grahpical Card
$299
P003
Grahpical Card
$299
P004
Grahpical Card
$299
Después de Buscar y reemplazar:
Product ID
Product Name
Price
P001
Graphical Card
$299
2. Limpieza de datos y preprocesamiento
4
P002
Graphical Card
$299
P003
Graphical Card
$299
P004
Graphical Card
$299
2.3 Funciones de fecha y hora
En Google Sheets, los valores fecha/hora son numéricos y comienzan desde el 
12/30/1899. Esta representación numérica permite manipular fechas y horas 
fácilmente, así como sumar, restar y extraer componentes específicos como el 
mes o el año.
Funciones de fecha y hora
Cómo extraer los componentes de fecha/hora:
YEAR(date) : devuelve el componente de año de una fecha. Para 1/25/2021 , 
devuelve 2021 .
MONTH(date) : recupera el componente de mes de una fecha. Para 1/25/2021 , 
arroja 1 .
DAY(date) : extrae el componente de día de una fecha. Para 1/25/2021 , el 
resultado es 25 .
HOUR(date) , MINUTE(date) , SECOND(date) : devuelve la hora, minuto y segundo 
de un valor de tiempo. Para 9:35:32 AM , la función devuelve 9 , 35  y 32 , 
respectivamente.
WEEKNUM(date) : proporciona el número de semanas del año para una fecha 
dada. Por ejemplo, WEEKNUM("1/25/2021")  podría devolver 4 , indicando la 
cuarta semana del año.
WEEKDAY(date) : devuelve el día de la semana como un número (Domingo = 1, 
Sábado = 7) para una fecha especificada.
Cómo convertir entre texto y fecha:
DATEVALUE("date_string") : convierte un string que representa una fecha en un 
valor de datos numérico que Google Sheets reconoce.
2. Limpieza de datos y preprocesamiento
5
DATE(year, month, day) : crea un valor de fecha a partir de los componentes 
año, mes y día. Es útil para volver a armar fechas a partir de campos de 
datos independientes.
Rangos de fechas y horas:
DATEDIF(start_date, end_date, "unit") : calcula la diferencia entre dos fechas. 
"unit"  puede ser "Y" , "M" , "D" , "MD" , "YM" , o "YD" , dedependiendo de si 
estás midiendo en años, meses, días, etc.
TODAY() : devuelve la fecha actual.
NOW() : proporciona la fecha y hora actuales.
2.4 Validación de datos
La validación de datos es un paso importante en la preparación de datos, 
asegurándonos que todas las entradas en un conjunto de datos satisfacen 
criterios específicos y, por lo tanto, conservando la integridad y precisión de los 
datos.
Cómo personalizar los validadores
Selección del rango de celdas
Especifica en qué celdas de la hoja de cálculo aplican las reglas de 
validación.
Normalmente, involucra seleccionar columnas completas, excluyendo los 
encabezados (p. ej., A2:A  para la primera columna) para aplicar las reglas 
de manera uniforme.
Cómo establecer criterios de validación
Al especificar las condiciones que cada valor de celda debe satisfacer, 
estos criterios determinan que datos se consideran válidos dentro del 
rango seleccionado.
Tipos de criterio:
Listas desplegables: restringe los valores de las celdas a una lista 
predefinida de opciones válidas.
2. Limpieza de datos y preprocesamiento
6
Condiciones de texto: valida las entradas de texto con base en el 
contenido, como el comprobar correos electrónicos o URL válidas.
Condiciones de fechas: asegura que las entradas de fecha caigan 
dentro de un rango específico o relativo a la fecha actual.
Condiciones numéricas: restringe a los números para que caigan 
dentro de ciertos límites o coincidan con criterios numéricos 
específicos.
Fórmulas personalizadas: utiliza las fórmulas de Google Sheets para 
crear condiciones de validación personalizadas y complejas que 
devuelvan TRUE  o FALSE .
Cómo implementar validadores personalizados
1. Navega hacia Validación de datos: selecciona "Datos > Validación de datos" 
desde el menú.
2. Escoge el rango de celdas: ingresa el rango al que quieres aplicarle la 
validación en la casilla "Aplicar a un rango".
3. Selecciona el tipo de criterio: elige los criterios de validación adecuados del 
menú desplegable o ingresa una fórmula personalizada.
4. Configura ajustes adicionales: de forma opcional, ajusta configuraciones 
avanzadas como los mensajes de entrada o el tratamiento de datos no válidos 
(p. ej., mostrar advertencia o rechazar entrada).

### 2_Hoja_informativa_Estadstica_descriptiva.pdf ###
Hoja informativa: Estadística descriptiva
1
Hoja informativa: Estadística 
descriptiva
Práctica
# Creación de histogramas con límites de intervalo establecidos 
 
data.hist(bins=[value1, value2, value3, value4, ..., valueN]) 
 
# Importación de una librería de funciones matemáticas de alto nivel 
 
import numpy as np 
 
# Encontrar dispersión 
 
import numpy as np 
np.var(x) 
 
# Sacar la raíz cuadrada 
 
import numpy as np 
np.sqrt(x)
Teoría
Variables cuantitativas (numéricas) toman valores numéricos.
Variables cualitativas (categóricas) toman valores no numéricos.
Una variable continua es una variable cuantitativa que puede tomar cualquier valor 
numérico (con cualquier grado de precisión) en algún rango (por ejemplo, cualquier 
valor entre 0 y 1).
Una variable discreta es cualquier variable que no es continua en ningún rango (por 
ejemplo, una variable que toma los valores enteros de 0 a 100).
Hoja informativa: Estadística descriptiva
2
Densidad de frecuencia: un valor equivalente a la altura de una columna de 
histograma cuya área refleja la frecuencia relativa de una variable continua.
Histograma de densidad: un histograma que usa la densidad de frecuencia.
Las métricas de posición te ayudan a calcular aproximadamente dónde se encuentra 
el conjunto de datos en el eje numérico.
Métrica algebraica de posición: la media, a menudo representada por la letra griega 
mu, μ.
Métrica estructural de posición: la mediana.
La varianza se usa para medir qué tan "dispersos" están los datos de la media. Se 
calcula tomando la distancia promedio elevada al cuadrado de la media de todos los 
puntos en el conjunto de datos:
La desviación estándar es la raíz cuadrada de la varianza y se representa con la letra 
griega sigma, σ. Se calcula con la ecuación siguiente:
Regla de las tres sigma: casi todos los valores (99,7%) se encuentran dentro del 
intervalo:
(μ − 3σ, μ + 3σ)
Sesgo es una medida de la asimetría de un dataset.
Los datos con asimetría positiva (sesgo a la derecha) tienen una media mayor que la 
mediana. Los datos tendrán más valores superiores a la media que inferiores.
Los datos con asimetría negativa (sesgo a la izquierda) tienen una media que es 
menor que la mediana. Los datos tendrán más valores inferiores a la media que 
superiores.
σ =
2
n
μ −x
∑(
i)2
σ =
n
μ −x
∑(
i)2

### 2_Resumen_del_captulo_Estadstica_descriptiva.pdf ###
Resumen del capítulo: Estadística descriptiva
1
Resumen del capítulo: 
Estadística descriptiva
Variables continuas y discretas
Las variables categóricas (cualitativas) toman sus valores de un conjunto limitado. Si 
se les asignan valores numéricos, esto es puramente simbólico y solo para facilitar el 
procesamiento (por ejemplo, rojo = 1, azul = 2).
Las variables cuantitativas (numéricas) toman valores numéricos. Existen dos 
formas:
Variables continuas que pueden tomar cualquier valor (con cualquier grado de 
precisión) dentro de un rango (por ejemplo, cualquier valor entre 0 y 1)
Variables discretas que no son continuas en ningún rango (por ejemplo, una 
variable que toma valores enteros entre 0 y 100)
Histogramas de frecuencia
Los histogramas funcionan bien para variables discretas. Para visualizar la frecuencia 
de las variables continuas necesitamos algo más.
Una forma de visualizar la distribución de variables continuas es dividir el conjunto de 
valores posibles en intervalos y contar el número de valores en cada intervalo.
Al trazar un histograma en pandas, puedes establecer el número de intervalos 
(contenedores) y establecer sus límites de forma explícita:
data.hist(bins=[value1, value2, value3, value4, ..., valueN])
No obstante, recuerda siempre que el éxito de este enfoque depende de qué tan bien 
elijas los límites del intervalo, una tarea que puede ser difícil incluso para analistas de 
datos experimentados.
Resumen del capítulo: Estadística descriptiva
2
Histogramas de densidad
Para superar las dificultades de crear un histograma para una variable continua, 
podemos usar una técnica un poco diferente que representa la frecuencia no como la 
altura de una columna, sino como su área (la longitud del intervalo multiplicada por la 
altura de la columna). Esta área es la frecuencia de la variable continua y la altura de 
la columna es la densidad de frecuencia. Un histograma que usa densidad de 
frecuencia se llama histograma de densidad.
Para estimar cuántos valores se encuentran en un intervalo particular, toma dos valores 
y encuentra el área total de los histogramas de densidad entre ellos. El número que 
obtengas será una estimación del numero de valores en ese intervalo.
También podemos mostrar la densidad de frecuencia de las variables continuas 
mediante líneas continuas. Se aplica el mismo principio: el área bajo la curva entre dos 
valores es proporcional a la frecuencia de los valores en un intervalo dado.
Medidas de posición
Podemos usar medidas de posición, como la mediana y la media, para estimar 
aproximadamente dónde se encuentra un dataset en el eje numérico.
La media y la mediana se denominan de manera más formal como medida de 
posición algebraica y medida de posición estructural, respectivamente.
¿Quién dispersó los datos?
Las medidas de posición no son suficientes si realmente quieres entender los datos. 
También necesitas saber cómo se dispersan o están dispersos los datos alrededor 
de estas medidas.
Para las medianas, la dispersión puede medirse en términos de cuartiles.
Varianza
La varianza es otra medida común de dispersión. Se puede calcular elevando al 
cuadrado la distancia media de los datos desde la media:
2
Resumen del capítulo: Estadística descriptiva
3
donde la letra griega mu, μ, corresponde a la media aritmética de los datos.
La librería NumPy en Python contiene una gran librería de funciones matemáticas de 
alto nivel. Así es como se importa:
import numpy as np
La varianza se calcula con el método var():
import numpy as np 
variance = np.var(x)
Desviación estándar
La varianza tiene un inconveniente: sus unidades de medida son cuadrados de las 
unidades originales de la variable. Para volver a las unidades de medida originales 
necesitamos sacar la raíz cuadrada de la varianza. El valor resultante se conoce como 
desviación estándar.
La desviación estándar se puede encontrar con el método std()  de NumPy:
import numpy as np 
standard_deviation = np.std(x)
Recuerda también que si ya conoces la varianza, puedes usar el método sqrt()  de 
NumPy para obtener la desviación estándar.
σ =
2
n
μ −x
∑(
i)2
μ =
n
x
∑( i)
σ =
n
μ −x
∑(
i)2
Resumen del capítulo: Estadística descriptiva
4
import numpy as np 
variance = 2.9166666666666665 
standard_deviation = np.sqrt(variance)
La regla de las tres desviaciones estándar, o la regla de las tres sigma, es válida para 
las distribuciones más utilizadas. Esta regla dice que casi todos los valores 
(aproximadamente el 99 %) se encuentran dentro de las tres desviaciones estándar de 
la media:
(μ − 3σ, μ + 3σ)
Esta regla no solo te ayuda a encontrar el intervalo en el que se encuentran la mayoría 
de los valores que te interesan, sino que también te ayuda a encontrar valores fuera de 
ese intervalo (valores atípicos).
Datos sesgados
En muchos casos, los datos se distribuyen normalmente y de forma simétrica. Pero los 
datasets también pueden ser asimétricos o "sesgados" en una dirección positiva o 
negativa. Es fácil reconocer la asimetría si observas un histograma. Puedes visualizar 
la asimetría como una cola de un lado al otro de la "masa" simétrica de los datos.
Resumen del capítulo: Estadística descriptiva
5
Se dice que una distribución de datos con valores adicionales a la derecha sesga a la 
derecha. Esto a menudo se denomina asimetría positiva porque hay valores 
adicionales a medida que te mueves a lo largo del eje en una dirección positiva.
Por el contrario, se dice que un dataset que se diferencia de uno simétrico en que tiene 
valores adicionales a la izquierda está sesgado a la izquierda o tiene asimetría 
negativa.
Los diagramas de caja también muestran la asimetría de una distribución.
Hay una manera de determinar la asimetría de un dataset sin trazar gráficos: 
simplemente compara la media y la mediana. Dado que la mediana no se ve afectada 
por los valores atípicos tanto como la media, la media es mayor que la mediana para 
datasets sesgados a la derecha, y viceversa para datasets sesgados a la izquierda.

### 3.Gua_rpida_para_valores_ausentes.pdf ###
Guía rápida para valores ausentes
1
Guía rápida para valores 
ausentes
Cuándo completar valores ausentes... y cuándo no
Algunas veces te encontrarás con reglas estrictas, como "Si falta más del 20% de una 
variable, borra la variable por completo". Pero dudamos en dar reglas estrictas, ya que 
mucho depende del contexto del problema. Como analista de datos, parte de tu trabajo 
es tener en cuenta los matices al momento de tomar decisiones. Dicho esto, he aquí 
algunas directrices:
1. Nunca completes la principal variable de interés, y nunca uses la principal 
variable de interés para completar valores ausentes. Con frecuencia, tu objetivo 
final es entender las relaciones entre una variable principal y otras variables. Este 
es el objetivo, no un paso de preprocesamiento de datos.
2. Siempre lleva un registro (documenta) sobre cuándo, dónde, por qué y cómo 
se completaron los valores ausentes. Siempre ten una razón justificable para 
cualquier cosa que completes. Si se te pregunta cómo, debes tener una respuesta.
3. Los valores completados no deberían tener un impacto significativo en tu 
análisis. En caso de duda, ejecuta tu análisis dos veces, una con los valores 
completados y otra con los valores ausentes eliminados. Si los resultados son 
sustancialmente diferentes, los valores completados están impulsando el cambio. Y 
no quieres hacer eso. Este es un ejemplo de análisis de sensibilidad.
Antes de hablar sobre cómo remplazar valores categóricos ausentes, toma una postura 
poderosa y repite después de mí: "No permitiré que los valores completados cambien 
drásticamente los resultados de mi análisis".
Ocuparse de los valores ausentes
Guía rápida para valores ausentes
2
Así que has determinado que un conjunto de datos tiene valores ausentes. ¿Qué 
haces?
1. Reportar el problema y averiguar si hay una manera de obtener los datos completos. 
Si no lo hay, continúa con el paso 2.
2. Determina cuántos valores ausentes hay: llama el método value_counts()  y print()  .
print(file_name['column_name'].value_counts())
3. Determina qué tan importante es esa ausencia para el conjunto de datos. ¿Qué 
proporción de datos representan? En la mayoría de los casos, si no es mucho 
(digamos, 5-10%, dependiendo de la situación), puedes borrarlos.
4. Comprueba qué tan importante es su ausencia para su categoría o columna: llama a 
los métodos isnull()  y count()  y muéstralo.
print(file_name[file_name['row_name'].isnull()].count())
5. Determina si los valores ausentes pertenecen a las variables categóricas o 
cuantitativas.
6. Si son categóricas:
Determina si los valores ausentes muestran o no un patrón; es decir, si su 
apariencia en el conjunto de datos es o no aleatoria. Si no se puede detectar una 
correlación con otros valores en las filas en las que aparecen (por ejemplo, en el 
caso de los encuestados menores de 21 años, una pregunta sobre el alcohol no 
Guía rápida para valores ausentes
3
tiene respuesta), entonces probablemente sean aleatorios. Existen tres tipos de 
valores ausentes:
Ausencia completamente al azar (MCAR, por sus siglas en inglés).
Ausencia al azar (MAR, por sus siglas en inglés).
Ausencia no por azar (MNAR, por sus siglas en inglés).
Dependiendo del patrón, decide cómo manejarlas:
Si los valores son MCAR o MAR, no hay un patrón, así que puedes 
remplazarlos con valores predeterminados: un string vacío o una palabra en 
particular. Utiliza el método loc[]  y la indexación booleana. El método fillna()  
también podría funcionar, pero no en todos los casos.
Con valores MNAR, hay un patrón. Este es un caso más complejo, y no nos 
sumergiremos en sus complejidades en este capítulo.
7. Si son cuantitativas:
Determina si tus datos tienen valores atípicos significativos.
Si no hay valores atípicos significativos, calcula la media de tus datos: aplica el 
método mean()  a la columna o al conjunto de datos completo.
Si tus datos tienen valores atípicos significativos, calcula la mediana de tus 
datos: aplica el método median()  a la columna o al conjunto de datos completo.
Reemplaza los valores ausentes con la media o la mediana utilizando el método 
fillna() .

### 3._tablas_dinmicas.pdf ###
3. Tablas dinámicas
1
3. Tablas dinámicas
3.1 ¿Qué es una tabla dinámica?
Una tabla dinámica es una tabla interactiva que extrae, organiza y resume tus 
datos de forma automática. Es ideal para comparar y analizar conjuntos de datos, 
identificar tendencias y resumir datos pertenecientes a diferentes categorías.
Cómo crear una tabla dinámica
1. Selecciona tus datos: resalta el rango de celdas que quieres analizar. Evita 
seleccionar la hoja completa para evitar datos incorrectos a medida que el 
conjunto de datos crece.
2. Inserta la tabla dinámica:
Ve a "Insertar > Tabla dinámica".
Elige colocar la tabla dinámica en una "Nueva hoja" para que tu análisis 
esté por separado.
3. Personaliza tu tabla con el Editor de tablas dinámicas:
Aparecerá una nueva tabla llamada "Tabla dinámica 1" con el editor de 
tablas dinámicas del lado derecho.
Utiliza el editor para agregar filas, columnas, valores y filtros a tu tabla 
dinámica.
3. Tablas dinámicas
2
3.2 Cómo dividir datos en las tablas dinámicas
Variables de los datos
Variables cuantitativas: son valores numéricos que miden información, como 
el ingreso o la edad. Es ideal para la sección "Valores" de una tabla dinámica.
Variables categóricas: se emplean para agrupar información, como la 
nacionalidad o la categoría del producto. Son convenientes en las secciones 
"Filas" o "Columnas" para organizar y resumir los datos.
Pasos para dividir los datos de forma efectiva
1. Identifica tus variables: determina cuáles columnas de tu conjunto de datos 
son categóricas y cuáles son cuantitativas. Esto te dará la pauta para 
configurar tu tabla dinámica.
2. Abre el Editor de tablas dinámicas: asegúrate de que el editor de tablas 
dinámicas está visible para comenzar a dividir tus datos. Si está oculto, 
actívalo utilizando el botón "Editar" en la esquina inferior izquierda de tu tabla 
dinámica.
3. Organiza tus datos:
Filas/Columnas: agrega variables categóricas aquí para segmentar tus 
datos.
Valores: coloca aquí las variables cuantitativas para hacer cálculos como 
sumas y promedios.
4. Variables anidadas: permite la subcategorización dentro de tus tablas 
dinámicas, proporcionando información más profunda de tus datos. Por 
ejemplo, anidar "Categoría" bajo "Operación" muestra los gastos o los 
ingresos desglosados por categorías específicas.
3. Tablas dinámicas
3
3.3 Cómo aplicar funciones de agregación
Funciones de agregación comunes
SUM() : calcula la suma total de un conjunto de valores numéricos.
COUNT() : cuenta el número total de celdas que contienen valores numéricos.
COUNTA() : cuenta el número total de celdas que contienen cualquier valor 
(numérico o de texto).
COUNTUNIQUE() : cuenta el número de valores únicos en un conjunto de celdas.
AVERAGE() : calcula el promedio de un conjunto de valores numéricos.
MAX() : encuentra el valor máximo en un conjunto de valores numéricos.
MIN() : determina el valor mínimo en un conjunto de valores numéricos.
MEDIAN() : calcula la mediana en un conjunto de valores numéricos.
Cómo aplicar funciones de agregación en las tablas dinámicas
1. Selecciona los datos: elige tus variables categóricas para las "Filas" y las 
"Columnas" y tus variables cuantitativas para "Valores".
2. Aplica la función:
3. Tablas dinámicas
4
En el editor de tablas dinámicas, en "Valores", selecciona "Resumir por" en 
el menú desplegable.
Elige la función de agregación que deses, como COUNT()  para contar los 
artículos de línea por categoría de cada mes.
3. Analiza los resultados: observa el resultado en tu tabla dinámica para obtener 
información.
3.4 Cómo filtrar y ordenar
Cómo ordenar tablas dinámicas
El ordenar ayuda a organizar los datos de una tabla dinámica de manera 
significativa para mejorar la legibilidad de los datos y la extracción de información.
Pasos clave:
1. Elegir el criterio de ordenación: determina la variable respecto a la que 
quieres ordenar tus datos. Esta puede ser cualquier variable categórica que 
esté en tus filas o columnas.
2. Tipo de orden: decide si quieres ordenar tus datos de forma ascendente (de la 
A a la Z, de menor a mayor) o descendente (de la Z a la A o del mayor al 
menor).
Cómo filtrar tablas dinámicas
Filtrar te ayuda a desplegar solo los datos que satisfacen criterios concretos, lo 
que hace el análisis más específico.
Pasos clave:
1. Aplicar filtros manuales: selecciona directamente qué datos incluir o excluir 
con base en los valores únicos de las variables.
2. Utilizar filtros de condiciones: aplica condiciones (p. ej., mayor que, o que 
contenga texto) para filtrar los datos de forma dinámica con base en pruebas 
lógicas.
Slicers
3. Tablas dinámicas
5
Los slicers proporcionan una herramienta de filtrado directamente en la hoja, lo 
que permite hacer ajustes a los datos desplegados en una tabla dinámica de 
forma fácil y sin tener que acceder al editor.
Pasos clave:
1. Agregar un slicer: selecciona el rango de datos de la tabla dinámica y ve a 
"Datos > Agregar slicer" para crear un nuevo slicer para una variable 
seleccionada, como Descuento %.
2. Personalizar los ajustes del slicer: configura qué valores incluir en el slicer, lo 
que ofrece una forma rápida para ajustar la vista de la tabla dinámica con 
base en criterios seleccionados.

### 3_Hoja_informativa_Trabajar_con_valores_ausentes_y_duplicados.pdf ###
Hoja informativa: Trabajar con valores ausentes y duplicados
1
Hoja informativa: Trabajar con 
valores ausentes y duplicados
Práctica
# Valores únicos de columna y conteos 
data['column'].value_counts()
# Operaciones aritméticas con columnas 
# ¡NB!: las columnas deben tener valores numéricos 
data['column1'] = data['column2'] + data['column3'] 
data['column1'] = data['column2'] - data['column3'] 
data['column1'] = data['column2'] * data['column3'] 
data['column1'] = data['column2'] / data['column3']
# Operaciones importantes para columnas con valores numéricos 
data['column'].sum() 
data['column'].min() 
data['column'].max() 
data['column'].mean() 
data['column'].median() 
 
# Número de valores en una columna 
data['column'].count()
Teoría
Fuentes de tráfico: canales a través de los cuales los visitantes llegan a los sitios web.
Tasa de conversión: una medición métrica de la efectividad de una fuente de tráfico 
(ya sea el porcentaje de visitantes que realizan acciones focalizadas o el porcentaje de 
Hoja informativa: Trabajar con valores ausentes y duplicados
2
acciones focalizadas comparadas con el total de las acciones).
Tasa de cliente frecuente: la proporción de clientes que realizan más de una compra 
frente al número total de clientes.
Sistema de analítica web: un sistema que reúne datos automáticamente de los 
contadores en sitio para registrar el comportamiento del usuario.
ID de usuario: un número asignado a un visitante para distinguirlo de los demás.
Cookies: archivos especiales de texto que permanecen en una memoria del dispositivo 
tras la primera visita del usuario y que se envían al servidor en las visitas repetidas.
Registros: archivos de texto con datos sobre las visitas al sitio.
NaN ("no es un número"): un valor especial float que se usa cuando no se puede 
mostrar o realizar un cálculo (p. ej., =0/0).
None: un valor especial None Type ("sin tipo") usado cuando está ausente un valor.
Variable categórica: una variable que toma sus valores de un conjunto limitado.
Variable cuantitativa: una variable que toma valores numéricos dentro de un rango.
Variable lógica (booleana): una variable que toma los valores Verdadero o Falso.
Diccionario: una estructura de datos que contiene pares clave-valor.

### 3_Resumen_del_captulo_Trabajar_con_valores_ausentes_y_duplicados.pdf ###
Resumen del capítulo: Trabajar con valores ausentes y duplicados
1
Resumen del capítulo: Trabajar 
con valores ausentes y 
duplicados
Métricas para evaluar fuentes de tráfico
En pandas puedes realizar operaciones matemáticas en las columnas: suma, resta, 
multiplicación y división. Por ejemplo:
data['column1'] = data['column12'] + data['column3']
IDs de usuarios y cookies
El método unique()  se usa para buscar valores únicos en una columna: 
data['column'].unique() .
Para borrar las filas con valores ausentes, llama el método dropna() . Para volver a 
numerar, llama el método reset_index()  con el argumento drop=True .
NaN  y None
NaN  y None  indican que no hay valores en una celda. NaN  significa "no es un número" 
("not a number"), así que puedes realizar en él operaciones matemáticas. None  es 
"ningún tipo", lo que significa que no puedes realizar operaciones matemáticas en él. 
Los valores NaN  pueden llevarnos a resultados incorrectos al agrupar datos. No 
elimines filas con estos valores: con frecuencia, los valores ausentes pueden 
restablecerse.
En pandas, el método value_counts()  devuelve valores únicos y sus conteos.
Resumen del capítulo: Trabajar con valores ausentes y duplicados
2
El método isnull()  devuelve una lista booleana, en donde "true" significa que un valor 
está ausente en la columna.
Para sustituir un valor por uno ausente, utiliza el método fillna()  con el argumento 
value .
Variables categóricas y cuantitativas
Existen dos tipos de variables: categóricas y cuantitativas. Las variables categóricas 
toman sus valores de un conjunto limitado, mientras que las variables cuantitativas 
toman valores numéricos de un rango limitado.
Las variables también pueden ser lógicas (booleanas), lo que significa que indican si 
una sentencia es verdadera o falsa. Si una sentencia es verdadera, la variable toma el 
valor 1. Si una sentencia es falsa, es 0.
Valores ausentes en variables categóricas
Cuando tratamos con valores ausentes en variables categóricas:
1. Determina si los valores ausentes presentan un patrón, es decir, si su aparición en 
el conjunto de datos es aleatoria o no. Si no se puede detectar una correlación con 
otros valores en las filas en las que aparecen (por ejemplo, en el caso de los 
encuestados menores de 21 años, una pregunta sobre el alcohol no tiene 
respuesta), entonces probablemente sean aleatorios.
2. Dependiendo del patrón, decide cómo manejarlos:
Si los valores están ausentes de manera aleatoria, no hay un patrón, entonces 
puedes remplazarlos con valores predeterminados: un string vacío, una palabra 
en particular. Utiliza el método loc[]  y la indexación booleana. El método 
fillna()  también podría funcionar, pero solo si los valores ausentes son NaN  o 
None .
Si los valores ausentes no son aleatorios, entonces hay un patrón. Este es el 
caso más complicado, y en este capítulo no nos enfocamos en él.
Aplicar funciones a columnas con diccionarios y agg()
Resumen del capítulo: Trabajar con valores ausentes y duplicados
3
El método agg()  se utiliza para aplicar funciones a columnas particulares. El nombre de 
la columna y las funciones mismas se registran en una estructura de datos llamada 
diccionario. Los diccionarios se componen de claves y valores. La clave es el nombre 
de la columna en la que se deben usar las funciones mientras que el valor es la lista de 
nombres de funciones.
{'column':['function1','function2']}
Cuando estás utilizando el método agg() , los nombres de las columnas se vuelven 
complejos. Para hacer referencia al resultado de usar ['function1']  en ['column'] , 
simplemente escríbelos uno tras otro:
data['column']['function1']
Valores ausentes en variables cuantitativas
Digamos que sabes que hay valores ausentes para las variables cuantitativas. Utiliza 
valores representativos (la media o mediana) para completar los vacíos.
1. Determina si tus datos tienen valores atípicos significativos.
2. Si no hay valores atípicos significativos, calcula la media de tus datos: aplica el 
método mean()  a la columna o al conjunto de datos completo.
3. Si tus datos tienen valores atípicos significativos, calcula la mediana de tus datos: 
aplica el método median()  a la columna o al conjunto de datos completo.
4. Remplaza los valores ausentes con la media o mediana usando el método 
fillna() .
Buscar duplicados a mano
Al analizar datos, con frecuencia te encontrarás con entradas duplicadas. Si no las 
identificas a tiempo, podrías terminar con conclusiones erróneas.
Hay dos formas de buscar datos duplicados.
Resumen del capítulo: Trabajar con valores ausentes y duplicados
4
Método 1
Podemos usar el método duplicated()  junto con sum()  para obtener el número de 
duplicados. Recuerda que si llamas duplicated()  sin calcular el total, se mostrará cada 
fila en la pantalla, y verás el valor True  en donde esté un duplicado, y False  donde no 
lo haya.
Método 2
Llama al método value_counts() . Este método analiza una columna, selecciona todos 
los valores únicos, y después calcula con qué frecuencia aparecen. Podemos aplicar 
este método a los objetos Series para obtener listas de pares valor-frecuencia en orden 
descendiente. Las entradas que se duplican con más frecuencia se encuentran en la 
parte superior de la lista.
Buscar duplicados a mano con distinción 
de mayúsculas y minúsculas
Los duplicados en datos de string requieren de una atención especial. Desde el punto 
de vista de Python, una 'A'  mayúscula y una 'a'  minúscula son símbolos diferentes.
Para detectar entradas duplicadas como estas, podemos cambiar todos los caracteres 
en el string a letras minúsculas llamando el método lower() .
En pandas, cambiamos los caracteres a letra minúscula usando un método que sigue 
una sintaxis parecida: str.lower() .

### 4.Hoja_informativa_Prueba_de_hiptesis.pdf ###
Hoja informativa: Prueba de hipótesis
1
Hoja informativa: Prueba de 
hipótesis
Práctica
# probar una hipótesis sobre la media de la población igualando un valor dado 
# matriz es la muestra 
# interested_value es la media propuesta para la prueba 
 
from scipy import stats as st 
 
results = st.ttest_1samp( 
    array,  
    interested_value) 
 
print('p-value: ', results.pvalue) 
 
# Probar una hipótesis sobre las medias de dos poblaciones estadísticas en función de mues
tras tomadas de ellas 
# sample_1 es la muestra de la primera población estadística 
# sample_2 es la muestra de la segunda población estadística 
# equal_var define si las varianzas de las muestras se consideran iguales o no; el valor p
redeterminado es Verdadero 
 
from scipy import stats as st 
 
sample_1 = [...] 
sample_2 = [...] 
 
results = st.ttest_ind( 
    sample_1,  
    sample_2, 
        equal_var = True) 
 
print('p-value: ', results.pvalue) 
 
# Probar una hipótesis sobre las medias de dos poblaciones estadísticas que son iguales pa
ra muestras dependientes (pareadas) 
# pair_1 es la primera muestra pareada 
# pair_2 es la segunda muestra pareada 
 
from scipy import stats as st 
 
results = st.ttest_rel( 
    before,  
    after) 
Hoja informativa: Prueba de hipótesis
2
 
print('p-value: ', results.pvalue)
Teoría
Población estadística es un gran conjunto de datos para estudios estadísticos.
Una muestra es una porción seleccionada de la población estadística.
Una muestra representativa es una porción de los datos que representa a la 
población estadística completa.
Una muestra aleatoria es una porción de la población estadística seleccionada 
aleatoriamente.
Estratos son grupos en una población estadística que están unidos por una 
característica común.
Una muestra estratificada es una muestra compuesta por muestras proporcionales de 
diferentes estratos.
La media muestral es la media de una muestra.
La varianza muestral es la varianza de una muestra.
El error estándar estimado es la desviación estándar de la media muestral con 
respecto a la media real de la población estadística (S es la desviación estándar 
estimada de la población estadística; n es el tamaño de la muestra):
La hipótesis nula (H₀) es una hipótesis que se prueba con la muestra.
La hipótesis alternativa (H₁) es la hipótesis de significado opuesto a la hipótesis nula.
La significación estadística es la probabilidad total de que un valor medido 
empíricamente se encuentre alejado de la media.
La estadística de diferencia es el número de desviaciones estándar entre los valores 
comparados, si ambas distribuciones se convierten en una distribución normal estándar 
con media 0 y desviación estándar 1.
E.S.E. =
n
S
Hoja informativa: Prueba de hipótesis
3
El valor p es la probabilidad de obtener un resultado al menos tan extremo como el 
que estás considerando, suponiendo que la hipótesis nula sea correcta.
Una muestra pareada es una muestra utilizada para medir variables para las mismas 
unidades.

### 4._grficas_y_diagramas.pdf ###
4. Gráficas y diagramas
1
4. Gráficas y diagramas
4.1 Gráficos básicos en Google Sheets
1. Gráficos de líneas: ideal para destacar tendencias en el tiempo, revelando 
movimientos ascendentes o descendentes dentro del conjunto de datos.
2. Gráficos de barras: efectivo para comparar cantidades en diferentes 
categorías, mostrando fácilmente las disparidades o similitudes.
3. Gráficos circulares: los más adecuados para desplegar la proporción de las 
partes en el todo, proporcionando una vista clara de la distribución de las 
categorías.
Cómo crear gráficos en Google Sheets
Ejemplo de operación: seleccionar una columna como "Operación" y usar 
"Insertar > Gráfico" puede generar de forma instantánea un gráfico; por 
defecto, normalmente es un gráfico de líneas.
4. Gráficas y diagramas
2
Conversión del tipo de gráfico: cambia fácilmente de un gráfico de líneas a 
uno de barras para ajustarse mejor a la comparación de datos categóricos.
Gráficos circulares: sirven para representar las proporciones de los datos.
4. Gráficas y diagramas
3
4.2 Cómo formatear los gráficos
Título del gráfico: modificar el título ayuda a reflejar de forma más precisa la 
historia de los datos.
Etiquetas de los ejes: etiquetar los ejes X y Y hace más claras las variables 
categóricas, mejorando la comprensión.
Leyendas: ajustar y formatear la posición de las leyendas puede mejorar la 
legibilidad y la interpretación.
Antes de formatear:
4. Gráficas y diagramas
4
Después de formatear:

### 4_Hoja_informativa_Filtrado_por_mltiples_condiciones.pdf ###
Hoja informativa: Filtrado por múltiples condiciones
1
Hoja informativa: Filtrado por 
múltiples condiciones
Operadores lógicos de Python
Operador
Descripción
==
Igual a
<
Menor que
>
Mayor que
! =
No igual a
< =
Menor o igual que
> =
Mayor o igual que
&
and
|
or
~
not
Ejemplos
1. Supongamos que queremos ver todos los juegos de Wii que no son de deportes:
import pandas as pd 
 
df = pd.read_csv('/datasets/vg_sales.csv') 
df['user_score'] = pd.to_numeric(df['user_score'], errors='coerce') 
 
print(df[(df['platform'] == 'Wii') & ~(df['genre'] == 'Sports')].head())
o también se puede hacer lo mismo con el método query() :
import pandas as pd 
 
df = pd.read_csv('/datasets/vg_sales.csv') 
df['user_score'] = pd.to_numeric(df['user_score'], errors='coerce') 
Hoja informativa: Filtrado por múltiples condiciones
2
 
print(df.query("platform == 'Wii' and genre != 'Sports'").head())
2. Ahora obtengamos todos los juegos que superaron el millón de dólares en ventas 
en al menos una de las tres regiones:
import pandas as pd 
 
df = pd.read_csv('/datasets/vg_sales.csv') 
df['user_score'] = pd.to_numeric(df['user_score'], errors='coerce') 
 
print(df[(df['na_sales'] >= 1) | (df['eu_sales'] >= 1) | (df['jp_sales'] >= 1)].head())

### 4_Resumen_del_capítulo_Filtrado_de_datos.pdf ###
Resumen del capítulo: Filtrado de datos
1
Resumen del capítulo: Filtrado de datos
Filtrado personalizado usando query()  e isin()
Este método query()  es llamado en un DataFrame y requiere una cadena como entrada. La cadena representa la consulta que 
quieres hacer en tu DataFrame, lo que básicamente significa que le dice a Python qué filas debe filtrar. Filtremos para que solo se 
seleccionen los juegos cuyas ventas en Japón fueron superiores a un millón de dólares e imprimamos solo las columnas 'name'  y 
'jp_sales' :
import pandas as pd 
 
df = pd.read_csv('/datasets/vg_sales.csv') 
 
print(df.query("jp_sales > 1")[['name', 'jp_sales']])
Para filtrar con query()  basándose en comparaciones de cadenas, es necesario poner comillas alrededor de la cadena. Por 
ejemplo, seleccionemos solo los juegos publicados por Nintendo:
import pandas as pd 
 
df = pd.read_csv('/datasets/vg_sales.csv') 
 
print(df.query("publisher == 'Nintendo'")[['name', 'publisher']].head())
El método que podemos utilizar para filtrar los datos se llama isin() . En lugar de utilizar los operadores lógicos conocidos, isin()  
comprueba si los valores de una columna coinciden con alguno de los valores de otra matriz, como una lista o un diccionario.
Podemos utilizar una lista de consolas de videojuegos portátiles para obtener solo las filas de los juegos de una de esas consolas:
import pandas as pd 
 
df = pd.read_csv('/datasets/vg_sales.csv') 
 
handhelds = ['3DS', 'DS', 'GB', 'GBA', 'PSP'] 
print(df[df['platform'].isin(handhelds)][['name', 'platform']])
Uso de estructuras de datos externas para filtrar DataFrames
El método query()  también ayuda a averiguar si los valores de la columna 'a'  están en la lista our_list .
Resumen del capítulo: Filtrado de datos
2
Este método, a veces en combinación con otros atributos, es aplicable no solo a listas externas, sino también a diccionarios, Series 
e incluso DataFrames externos:
print(df.query("a in @our_dict.values()"))  
#comprobará si los valores de la columna a de una estructura de datos externa están presentes en los valores de nuestro diccionario 
#fíjate que necesitamos usar el método values() del diccionario, ya que necesitamos buscar entre los valores del diccionario, no las claves
 
print(df.query("a in @our_series")) 
#comprobará si los valores de la columna a de una estructura de datos externa están presentes en los valores de nuestro objeto Series 
 
print(df.query("c in @our_df.index")) 
#comprobará si los valores de la columna c de una estructura de datos externa están presentes en los índices de nuestro DataFrame
Reemplazo de valores con where()
Cuando el procesamiento de nuestros datos implica la modificación de los valores de las columnas, podemos utilizar el método 
where()  para filtrar y modificar al mismo tiempo, de modo que solo cambiemos los valores bajo ciertas condiciones.
Supongamos que queremos cambiar todos los valores 'NES'  de la columna 'platform'  por el nombre completo, 'Nintendo 
Entertainment System'
import pandas as pd 
 
df = pd.read_csv('/datasets/vg_sales.csv') 
 
df['platform'] = df['platform'].where(df['platform'] != 'NES', 'Nintendo Entertainment System') 
print(df.iloc[:, :2].head())
Llamamos a where()  en la columna 'platform'  y le pasamos dos argumentos de posición:
Resumen del capítulo: Filtrado de datos
3
1. La condición lógica: df['platform'] != 'NES' . Como sabemos, comprueba todos los valores de la columna 'platform'  y devuelve 
True  para las filas en las que 'NES'  NO es un valor, y False  en caso contrario. La salida es una serie de booleanos.
2. Un nuevo valor para reemplazar los valores de 'platform'  para los que la condición lógica es False .
El método where()  comprueba la condición para cada valor de la columna. Si la condición es True , where()  no hace nada; si es 
False , where()  sustituye el valor actual por el nuevo.

### 4_Resumen_del_captulo_Prueba_de_hiptesis.pdf ###
Resumen del capítulo: Prueba de hipótesis
1
Resumen del capítulo: Prueba de 
hipótesis
Muestreo aleatorio y medias muestrales
La lógica de la prueba de hipótesis estadística es un poco diferente a la de la teoría de 
la probabilidad, donde se asumen condiciones ideales. En primer lugar, cuando 
probamos las hipótesis, evaluamos un gran dataset o población estadística, en 
función de las muestras.
No es necesario analizar el dataset completo. Todo lo que necesitas es una porción 
pequeña pero representativa de los datos que refleje las características de la 
población en su conjunto. La forma más fácil de asegurar la representatividad es tomar 
una muestra aleatoria. Utilizando elementos seleccionados aleatoriamente, podemos 
sacar conclusiones sobre la población en su conjunto.
Algunos datasets pueden tener varias partes de tamaños desiguales que difieren 
mucho con respecto al parámetro que estás estudiando. En estos casos, es una buena 
idea tomar muestras aleatorias proporcionales de cada parte y luego combinarlas. El 
resultado es una muestra estratificada que es más representativa que una muestra 
aleatoria normal. Lo llamamos "estratificado" porque dividimos la población en estratos 
o grupos que tienen algo en común. Estos estratos luego se usan para producir 
muestras aleatorias.
Con una muestra, puedes sacar conclusiones sobre la población, o sea sobre sus 
parámetros estadísticos, para ser precisos. Por lo general, es suficiente estimar los 
valores medios y la varianza para sacar una conclusión sobre igualdad o desigualdad 
con respecto a los valores medios de las poblaciones.
¿Qué podemos aprender acerca de la media y la varianza de una población en función 
de la media y la varianza que calculamos para una muestra (también denominadas 
media muestral y varianza muestral)? Casi todo, siempre y cuando nuestra muestra 
sea suficientemente grande.
Esta es una manera de establecer el teorema del límite central: si hay suficientes 
observaciones en una muestra, la distribución muestral de la media muestral de 
cualquier población estadística se distribuye normalmente alrededor de la media de 
Resumen del capítulo: Prueba de hipótesis
2
esta población. "Cualquier población estadística" significa que la población estadística 
puede tener cualquier distribución. Los valores medios de las muestras seguirán 
distribuidos normalmente alrededor de la media de toda la población estadística.
La medida del grado en que la media muestral se desvía de la media de la población se 
llama error estándar y se calcula mediante la fórmula:
E.S.E. significa error estándar estimado (estimated standard error). Es "estimado" 
porque solo tenemos una muestra. No sabemos el error exacto, solo lo estimamos en 
función de los datos que tenemos.
S es la desviación estándar estimada de la población.
n es el tamaño de la muestra. Dado que la raíz cuadrada de n está en el denominador, 
el error estándar disminuye a medida que aumenta el tamaño de la muestra.
Formular hipótesis
Ningún dato obtenido experimentalmente confirmará ninguna hipótesis. Esta es nuestra 
limitación fundamental. Los datos solo pueden contradecir la hipótesis o, por el 
contrario, mostrar que los resultados son extremadamente improbables (suponiendo 
que la hipótesis sea verdadera). Pero en ambos casos no tenemos motivos para 
afirmar que la hipótesis ha sido probada.
Si los datos no contradicen la hipótesis, simplemente no la rechazamos. Pero si, 
suponiendo que la hipótesis sea verdadera, es muy poco probable que obtengamos 
esos datos, tenemos una razón para rechazar la hipótesis.
Las hipótesis típicas pertenecen a las medias de poblaciones estadísticas y se ven así:
La media de una población es igual a un valor determinado.
Las medias de dos poblaciones son iguales entre sí.
La prueba de una hipótesis estadística siempre comienza con un enunciado de la 
hipótesis. En primer lugar, establecemos la hipótesis nula, H₀. Por ejemplo, "La media 
de la población en cuestión es igual a A", donde A es un número. La hipótesis 
alternativa, H₁, se basa en H₀. Para este H₀, H₁ sería "La media de la población no es 
igual a A". H₀ siempre se indica con un signo igual.
E.S.E. =
n
S
Resumen del capítulo: Prueba de hipótesis
3
Construimos una distribución basada en la suposición de que H₀ es verdadera. En este 
caso, se trataría de una distribución normal alrededor del parámetro de interés, o sea la 
media. La varianza o su raíz cuadrada, la desviación estándar (una medida de la 
dispersión de la distribución), se estima en función de la muestra.
Para la distribución normal, la probabilidad de estar comprendido en algún intervalo es 
igual al área bajo la curva de ese intervalo. Habrá valores dentro de un cierto rango de 
la media que es muy probable que se obtengan aleatoriamente.
¿Cómo determinamos si rechazamos o no la hipótesis nula? Se especifica un valor 
crítico para la muestra del nivel de significación de la prueba de hipótesis. El nivel de 
significación es la probabilidad total de que un valor medido empíricamente se 
encuentre lejos de la media. Digamos que el valor observado en la muestra se 
encuentra dentro de este rango. Si asumimos que la hipótesis nula es correcta, la 
Resumen del capítulo: Prueba de hipótesis
4
probabilidad de que tal evento ocurra se considera demasiado baja (con respecto al 
nivel de significación). Por lo tanto, tenemos motivos para rechazar la hipótesis nula. 
Cuando el valor se encuentra en el rango de "No rechazar H₀", no hay motivos para 
rechazar la hipótesis nula. Llegamos a la conclusión de que los datos obtenidos 
empíricamente no refutan la hipótesis nula.
Hay un método en Python que devuelve la estadística de diferencia entre la media y 
el valor para comparar. La más importante es la significación estadística entre ellos, 
representada por el valor p.
La estadística de diferencia es el número de desviaciones estándar entre los valores 
comparados, si ambas distribuciones se convierten en una distribución normal estándar 
con media 0 y desviación estándar 1. Sin embargo, este valor no te brinda suficiente 
información para llegar a una conclusión sobre tu hipótesis nula.
En cambio, usa el valor p para decidir si rechazar la hipótesis nula. Representa la 
probabilidad de obtener el resultado observado o un resultado más alejado de lo 
esperado, asumiendo que la hipótesis nula sea correcta. Los valores de umbral 
convencionales son 5% y 1%. En última instancia, la decisión sobre qué umbral 
considerar suficiente depende del analista.
Para probar la hipótesis de que la media de una población estadística es igual a algún 
valor, puedes usar el método scipy.stats.ttest_1samp() . Los parámetros del método son 
array  (la matriz que contiene la muestra) y popmean  (la media propuesta que usamos 
para la prueba). El método devuelve la estadística de la diferencia entre popmean  y la 
media muestral de la matriz, así como el nivel de significación:
from scipy import stats as st 
 
interested_value = 120 
 
results = st.ttest_1samp( 
    array,  
    interested_value) 
 
print('p-value: ', results.pvalue)
Resumen del capítulo: Prueba de hipótesis
5
Hipótesis sobre la igualdad de las medias de dos 
poblaciones
A veces necesitas comparar las medias de dos poblaciones estadísticas diferentes. 
Para probar tu hipótesis de que las medias de dos poblaciones estadísticas son iguales 
según las muestras tomadas de ellas, aplica el método scipy.stats.ttest_ind() . El 
método toma los parámetros siguientes:
matriz1  y matriz2  son matrices que contienen las muestras.
equal_var  es un parámetro opcional que especifica si las varianzas de las 
poblaciones deben considerarse iguales o no.
Si hay motivos para creer que las muestras se tomaron de poblaciones con parámetros 
similares, configura equal_var = True  y la varianza de cada muestra se estimará a partir 
del dataset combinado de las dos muestras y no a partir de los valores de cada 
muestra por separado. Esto nos proporciona resultados más precisos. Sin embargo, lo 
hacemos solamente si las varianzas de las poblaciones estadísticas de las que se 
toman las muestras son aproximadamente iguales. De lo contrario, tenemos que 
configurar equal_var = False ; de forma predeterminada, es Hipótesis sobre la igualdad 
de las medias de muestras pareadas equal_var = True .
from scipy import stats as st 
 
sample_1 = [...] 
sample_2 = [...] 
 
results = st.ttest_ind( 
    sample_1,  
    sample_2) 
 
print('p-value: ', results.pvalue)
Hipótesis sobre la igualdad de las medias de muestras 
pareadas
Cuando trabajamos con una población estadística, es útil saber si los cambios tienen 
un efecto en la media de la población. Una muestra pareada significa que estamos 
midiendo una variable de la misma entidad. En Python, para probar la hipótesis de que 
Resumen del capítulo: Prueba de hipótesis
6
las medias de dos poblaciones estadísticas son iguales para muestras dependientes 
(pareadas) usamos la función scipy.stats.ttest_rel() :
from scipy import stats as st 
 
before = [...] 
after = [...] 
 
results = st.ttest_rel( 
    before,  
    after) 
 
print('p-value: ', results.pvalue)

### 4_Resumen_loc_vs._iloc.pdf ###
Resumen: loc[] vs. iloc[]
1
Resumen: loc[] vs. iloc[]
Los objetos Series y DataFrame en pandas siempre tienen índices que se almacenan en el atributo index . 
Puedes acceder a diferentes índices y así filtrar el DataFrame utilizando los métodos loc[]  y iloc[] . 
Método loc[]
Puedes acceder a los elementos del DataFrame con loc[]  pasando los valores de los índices y los 
nombres de las columnas como df.loc[index_value, col_name] .
import pandas as pd 
 
states  = ['Alabama', 'Alaska', 'Arizona', 'Arkansas'] 
flowers = ['Camellia', 'Forget-me-not', 'Saguaro cactus blossom', 'Apple blossom'] 
insects = ['Monarch butterfly', 'Four-spotted skimmer dragonfly', 'Two-tailed swallowtail', 'European honey bee'] 
index   = ['state 1', 'state 2', 'state 3', 'state 4'] 
 
df = pd.DataFrame({'state': states, 'flower': flowers, 'insect': insects}, index=index) 
 
print(df.loc['state 4', 'insect'])
European honey bee
Método iloc[]
Mientras que loc[]  utiliza el índice y las etiquetas de columnas para acceder a los elementos, iloc[]  
utiliza enteros para designar las posiciones de los elementos que necesitas obtener.
import pandas as pd 
 
states  = ['Alabama', 'Alaska', 'Arizona', 'Arkansas'] 
flowers = ['Camellia', 'Forget-me-not', 'Saguaro cactus blossom', 'Apple blossom'] 
insects = ['Monarch butterfly', 'Four-spotted skimmer dragonfly', 'Two-tailed swallowtail', 'European honey bee'] 
index   = ['state 1', 'state 2', 'state 3', 'state 4'] 
 
df = pd.DataFrame({'state': states, 'flower': flowers, 'insect': insects}, index=index) 
 
print(df.iloc[3, 2])
European honey bee
Obtención de un rango de índices
Puede que necesites obtener varias columnas y filas para filtrar el DataFrame y, en lugar de escribir todas 
las etiquetas o incluso las posiciones, Python nos permite especificar el primer y el último índice separados 
por : . Aquí tenemos un ejemplo que devuelve las flores de los tres primeros índices de un DataFrame:
Resumen: loc[] vs. iloc[]
2
import pandas as pd 
 
states  = ['Alabama', 'Alaska', 'Arizona', 'Arkansas'] 
flowers = ['Camellia', 'Forget-me-not', 'Saguaro cactus blossom', 'Apple blossom'] 
insects = ['Monarch butterfly', 'Four-spotted skimmer dragonfly', 'Two-tailed swallowtail', 'European honey bee'] 
index   = ['state 1', 'state 2', 'state 3', 'state 4'] 
 
df = pd.DataFrame({'state': states, 'flower': flowers, 'insect': insects}, index=index) 
 
print(df.loc['state 1': 'state 3', 'flower'])
state 1                  Camellia 
state 2             Forget-me-not 
state 3    Saguaro cactus blossom 
Name: flower, dtype: object

### 6_Resumen_del_captulo_Visualizacin_de_los_datos.pdf ###
Resumen del capítulo: Visualización de los datos
1
Resumen del capítulo: 
Visualización de los datos
Visualización de los datos
La visualización de los datos nos permite ver patrones y relaciones de forma 
instantánea, lo cual nos ayuda a entender los datos. También nos permite sacar 
conclusiones, formular nuevas preguntas y comunicar mejor nuestros resultados a 
otras personas.
Hay dos razones principales para crear visualizaciones como profesional de los datos:
1. Para analizar grandes cantidades de datos revelando sus propiedades visualmente, 
en otras palabras, visualizar para analizar. Al visualizar datos, puedes obtener una 
percepción que sería imposible si estuvieras leyendo cifras sin procesar.
2. Para comunicar los resultados de tu análisis a tus colegas o clientes de forma 
comprensible y eficaz.
Elegir el tipo de gráfico correcto
Gráfico de barras. Los gráficos de barras nos permiten comparar propiedades 
numéricas (por ejemplo, población) entre categorías (por ejemplo, estados).
Histograma. Un histograma es un gráfico que muestra la frecuencia con la que 
aparecen diferentes valores para una variable en tu conjunto de datos. Aunque 
pueden parecerse a los gráficos de barras, hay diferencias.
Gráfico de líneas. Son excelentes cuando tienes datos que se conectan 
cronológicamente y cada punto de tiempo de los datos tiene alguna dependencia 
con el punto anterior. Cosas como datos de temperatura, datos de tráfico y datos 
del mercado de valores son buenos candidatos para los gráficos de líneas.
Gráfico de dispersión. Es simplemente un gráfico en el que se traza un solo punto 
para cada conjunto de variables, pero los puntos no están conectados por líneas. 
Los gráficos de dispersión son excelentes para visualizar la relación entre las 
variables.
Resumen del capítulo: Visualización de los datos
2
Sugerencias de gráficos
Graficar con pandas
plot()  crea gráficos utilizando los valores en las columnas de DataFrame. Los índices 
están en el eje X y los valores de las columnas están en el eje Y.
df.plot()
Podemos cambiar los índices de los ejes, asignando al eje correspondiente los valores 
de la columna que necesitemos.
df.plot(x='column_x', y='column_y')
Personalización de gráficos con parámetros 
plot()
Podemos gestionar el tamaño del gráfico con el parámetro figsize . El ancho y el alto 
en pulgadas se pasan como una tupla:
df.plot(figsize=(x_size, y_size))
El método plot()  también tiene el parámetro  style  que se encarga del aspecto de los 
puntos:
'o' : en lugar de una línea continua, cada valor se marcará como un punto;
'x' : en lugar de una línea continua, cada punto se marcará como una x;
'o-' : mostrará tanto líneas como puntos.
Podemos arreglar los bordes usando los parámetros  xlim  y  ylim , que aprendiste al 
estudiar diagramas de caja:
df.plot(xlim=(x_min, x_max), ylim=(y_min, y_max))
Resumen del capítulo: Visualización de los datos
3
Para mostrar líneas de cuadrícula, establece el parámetro  grid  en  True :
df.plot(grid=True)
Uso de varias opciones juntas
data.plot( 
    x='days', 
    y='revenue', 
    style='o-', 
    xlim=(0, 30), 
    ylim=(0, 120000), 
    figsize=(4, 5), 
    title='A vs B', 
    grid=True)
Guardar figuras
La función savefig()  se puede utilizar para guardar la última figura.
import matplotlib.pyplot as plt 
import pandas as pd 
 
df = pd.DataFrame({'a':[2, 3, 4, 5], 'b':[4, 9, 16, 25]}) 
 
df['b'].plot() 
plt.savefig('myplot.png', dpi=300) 
Graficar con matplotlib y pandas
Probablemente, la combinación más óptima:
crear un objeto Figure y un objeto Axes con  matplotlib ;
trazar el núcleo del gráfico con pandas;
ajustar el gráfico llamando a los métodos de Axes y/o Figure.
Resumen del capítulo: Visualización de los datos
4
import matplotlib.pyplot as plt 
import pandas as pd 
 
df = pd.DataFrame({'a':[2, 3, 4, 5], 'b':[4, 9, 16, 25]}) 
 
fig, ax = plt.subplots(figsize=(12, 8)) 
 
df.plot(x='days', y='revenue', style='o-', ax=ax) 
 
ax.set_xlim(0, 30) 
ax.set_ylim(0, 120000) 
 
ax.set_title('A vs B') 
ax.grid(True) 
 
fig.savefig('myplot.png', dpi=300)
Consulta muchos métodos para el objeto Axes aquí.
Gráficos de barras
A veces usamos gráficos de barras para representar gráficamente datos cuantitativos. 
Cada barra en dicho gráfico corresponde a un valor: cuanto mayor sea el valor, mayor 
será la barra. Las diferencias entre los valores son claramente visibles.
En pandas, los gráficos se trazan con el método  plot() . Se pasan varios tipos de 
gráficos en el parámetro  kind . Para trazar un gráfico de barras, indica  'bar' .
import matplotlib.pyplot as plt 
import pandas as pd 
 
df = pd.read_csv('/datasets/west_coast_pop.csv') 
 
df.plot(x='year', kind='bar')
Histogramas
Diferencias clave entre gráficos de barras e histogramas:
Los gráficos de barras se utilizan para comparar valores de variables discretas; los 
histogramas se utilizan para trazar distribuciones de variables numéricas continuas.
Resumen del capítulo: Visualización de los datos
5
El orden de las barras en los gráficos de barras se puede modificar por estilo o 
comunicación; el orden de las barras en los histogramas no se puede cambiar.
En un histograma, el eje X representa la variable y tiene el rango de valores que puede 
tomar la variable. El eje Y representa la frecuencia con la que ocurre cada valor.
En pandas, los histogramas se trazan con el método  hist() . Se puede aplicar a una 
lista o una columna de un DataFrame, en cuyo caso la columna se pasa como 
argumento. El método hist()  encuentra los valores más altos y más bajos en un 
conjunto y divide el rango resultante en intervalos o contenedores igualmente 
espaciados. Luego, el método encuentra la cantidad de valores dentro de cada 
contenedor y la representa en el gráfico. El número predeterminado de contenedores 
es 10, que se puede cambiar utilizando el parámetro  bins= .
Otra forma de trazar un histograma es llamar al método  plot()  con el 
parámetro  kind='hist' .
Visualización de un histograma con 16 contenedores y valores mínimos y máximos 
(min_value y max_value):
COPYPYTHON 
import matplotlib.pyplot as plt 
import pandas as pd 
 
df['age'].hist(bins=16, range=(0, 120)
Se pueden mostrar varios histogramas en un solo gráfico.
import matplotlib.pyplot as plt 
import pandas as pd 
 
df = pd.read_csv('/datasets/height_weight.csv') 
 
df[df['male'] == 1]['height'].plot(kind='hist', bins=30) 
# Incluye un valor alfa para que podamos ver por completo los dos histogramas 
df[df['male'] == 0]['height'].plot(kind='hist', bins=30, alpha=0.8) 
 
plt.legend(['Male', 'Female'])
Gráficos de líneas
Resumen del capítulo: Visualización de los datos
6
import matplotlib.pyplot as plt 
import pandas as pd 
 
df = pd.read_csv('sbux.csv') 
 
df.plot(x='date', y='open')
Diagramas de dispersión
Puede haber tantos puntos que muchos de ellos se superponen, por lo que no 
podemos tener una buena idea de la densidad de puntos en los gráficos anteriores. Se 
puede arreglar usando el parámetro  alpha= . Este parámetro controla la transparencia 
de los puntos y puede tomar cualquier valor entre 0 (transparencia total) y 1 (sin 
transparencia). De forma predeterminada, se establece en 1 y no hay transparencia.
import matplotlib.pyplot as plt 
import pandas as pd 
 
df = pd.read_csv('/datasets/height_weight.csv') 
 
df.plot(x='height', y='weight', kind='scatter', alpha=0.36)
Matrices de dispersión
Podemos construir diagramas de dispersión para cada posible par de parámetros. Este 
conjunto de diagramas por pares se denomina matriz de dispersión.
import matplotlib.pyplot as plt 
import pandas as pd 
 
df = pd.read_csv('/datasets/height_weight.csv') 
 
pd.plotting.scatter_matrix(df, figsize=(9, 9))

### 7_Resumen_del_captulo_Tipos_de_datos.pdf ###
Resumen del capítulo: Tipos de datos
1
Resumen del capítulo: Tipos de 
datos
Variables categóricas y cuantitativas
Hay dos tipos básicos de variables estadísticas: categóricas y cuantitativas. Las 
variables categóricas toman sus valores de un conjunto limitado, mientras que las 
variables cuantitativas toman valores numéricos de un rango. Es importante 
diferenciarlos porque los métodos de análisis de datos pueden estar limitados según el 
tipo de datos o incluso ser aplicables a solo uno de los tipos de datos.
También hay otros tipos de variables, pero esto tiene más que ver con conceptos 
centrados en el software que en el análisis de datos. Estos son algunos ejemplos de 
otros tipos de datos:
Lógicos (booleanos), es decir, indican si una sentencia es verdadera o falsa. Si una 
sentencia es verdadera, la variable toma el valor 1. Si una sentencia es falsa, es 0.
Strings.
Fechas y marcas temporales.
Conversión de los valores de columna a un 
tipo diferente
La conversión de valores de columna a un tipo diferente (int para números enteros, por 
ejemplo) reemplazando la columna actual.
df['column'] = df['column'].astype('int')
La conversión de valores de columna a un tipo diferente (int para números enteros, por 
ejemplo) sin reemplazar la columna actual.
df['new_column'] = df['column'].astype('int')
Resumen del capítulo: Tipos de datos
2
Conversión de los valores de string a 
números
Usamos un método estándar de pandas para convertir los valores de string a 
números:  to_numeric() . Convierte los valores de columna al tipo float64 (número de 
coma flotante) o int64 (número entero), según el valor de entrada.
El método to_numeric()  tiene un parámetro  errors . Este parámetro determina qué 
hará  to_numeric  al encontrar un valor no válido:
errors='raise'  (predeterminado): se genera una excepción cuando se encuentra un 
valor incorrecto, lo que detiene la conversión a números.
errors='coerce' : los valores incorrectos se reemplazan por  NaN .
errors='ignore' : los valores incorrectos no se modifican.
pd.to_numeric(df['column']) 
pd.to_numeric(df['column'], errors='raise' ) 
pd.to_numeric(df['column'], errors='coerce') 
pd.to_numeric(df['column'], errors='ignore')
Conversión de strings a horas y fechas
Python tiene un tipo de datos especial que utilizamos cuando trabajamos con fechas y 
horas: datetime.
Para convertir los strings a fechas y horas, usamos el método  to_datetime()  de pandas. 
Los parámetros del método incluyen el nombre de la columna que contiene strings y el 
formato de fecha en un string.
Establecemos el formato de fecha utilizando un sistema de designación especial:
%d : día del mes (01 a 31);
%m : mes (01 a 12);
%Y : año en cuatro dígitos (por ejemplo, 1994);
%y : año en dos dígitos (por ejemplo, 94);
Z  o  T : separador estándar para la fecha y la hora;
Resumen del capítulo: Tipos de datos
3
%H : hora en formato de 24 horas;
%I : hora en formato de 12 horas;
%M : minutos (00 a 59);
%S : segundos (00 a 59).
df['column']= pd.to_datetime(df['column'], format='%d.%m.%Y %H:%M:%S')
Puedes consultar la documentación aquí para obtener más información sobre los 
códigos de formato de fecha y hora.
Extracción de componentes de tiempo
A menudo tenemos que estudiar las estadísticas por mes, día o año. Para hacerlo, 
podemos colocar la hora en la clase DatetimeIndex y aplicarle el atributo month, day o 
year:
pd.DatetimeIndex(df['column']).year 
pd.DatetimeIndex(df['column']).month 
pd.DatetimeIndex(df['column']).day 
pd.DatetimeIndex(df['column']).hour 
pd.DatetimeIndex(df['column']).minute 
pd.DatetimeIndex(df['column']).second
Para las columnas con valores similares a fecha y hora, también puedes acceder a 
estas propiedades a través del accesor .dt.
df['time'].dt.year 
df['time'].dt.month 
df['time'].dt.day 
df['time'].dt.hour 
df['time'].dt.minute 
df['time'].dt.second
La lista completa de componentes se puede encontrar aquí. Por ejemplo, podemos 
encontrar el día de la semana con el método  dt.weekday() .

### 9_Resumen_del_captulo_Transformacin_de_datos.pdf ###
Resumen del capítulo: Transformación de datos
1
Resumen del capítulo: 
Transformación de datos
Agrupación de datos
Hacer una operación  groupby()  cambia el índice de fila de los datos a las claves por las 
que estamos agrupando. Para agrupar por varias columnas, pasamos una lista al 
método  groupby() .
El objeto DataFrameGroubBy  forma parte de un framework de procesamiento de datos 
llamado dividir-aplicar-combinar:
1. dividir los datos en grupos;
2. aplicar una función de agregación estadística a cada grupo;
3. combinar los resultados para cada grupo.
Ejemplo. Dividimos los datos en grupos con  df.groupby(['platform', 'genre']) , aplicamos 
el método  mean()  y combinamos el resultado en un objeto Series 
con  grp['critic_score'].mean() .
import pandas as pd 
 
df = pd.read_csv('/datasets/vg_sales.csv') 
df.dropna(inplace=True) 
 
grp = df.groupby(['platform', 'genre']) 
mean_scores = grp['critic_score'].mean() 
print(mean_scores)
Procesamiento de datos agrupados con 
agg()
El método agg() usa un diccionario como entrada donde las claves son los nombres de 
columnas y los valores correspondientes son las funciones de agregación que quieres 
Resumen del capítulo: Transformación de datos
2
aplicarles. Es útil obtener diferentes estadísticas de resumen para diferentes columnas. 
Incluso podemos aplicar nuestras propias funciones personalizadas con agg().
import pandas as pd 
 
df = pd.read_csv('/stats/vg_sales.csv') 
df.dropna(inplace=True) 
 
agg_dict = {'critic_score': 'mean', 'jp_sales': 'sum'} 
 
grp = df.groupby(['platform', 'genre']) 
print(grp.agg(agg_dict))
Agregación de datos con pivot_table()
pandas  también ofrece tablas dinámicas ("pivot tables") como método alternativo para 
agrupar y analizar datos. Las tablas dinámicas son una gran herramienta para sintetizar 
conjuntos de datos y explorar sus diferentes dimensiones. Son muy populares en las 
aplicaciones de hojas de cálculo como Excel, pero es aún más impresionante crearlas 
mediante programación con  pandas .
Los parámetros comunes del método pivot_table()  son:
index= : la columna cuyos valores se convierten en índices en la tabla dinámica;
columns= : la columna cuyos valores se convierten en columnas en la tabla 
dinámica;
values= : la columna cuyos valores queremos agregar en la tabla dinámica;
aggfunc= : la función de agregación que queremos aplicar a los valores en cada 
grupo de filas y columnas.
Ejemplo. Calcula las ventas totales en Europa para cada combinación de 
género/plataforma.
import pandas as pd 
 
df = pd.read_csv('/datasets/vg_sales.csv') 
df.dropna(inplace=True) 
 
 
pivot_data = df.pivot_table(index='genre', 
Resumen del capítulo: Transformación de datos
3
                            columns='platform', 
                            values='eu_sales', 
                            aggfunc='sum' 
                            )
El uso de una tabla dinámica aquí es conveniente porque fácilmente excluimos todas 
las columnas de df que no nos interesaban para nuestro análisis. Además, puede ser 
más fácil de leer que el código equivalente basado en  groupby() .
Cuando se trabaja en tareas, es importante elegir el método de promedio adecuado, ya 
que eso podría afectar los resultados. En algunos casos, la media aritmética describe 
los datos con mayor precisión, mientras que en otros puede dar un resultado incorrecto, 
lo que lleva a la necesidad de calcular la mediana. El método pivot_table()  acepta 
diferentes funciones de agregación para el parámetro  aggfunc , por ejemplo:
median ;
count  (número de valores);
sum ;
min ;
max ;
first  (el primer valor del grupo);
last  (el último valor del grupo).
Al llamar a  pivot_table() , podemos pasar varias funciones al parámetro  aggfunc  a la 
vez. Por ejemplo,  aggfunc=['median', 'count']  calculará tanto la mediana como el 
número de valores. Se mostrarán en columnas vecinas en la tabla resultante.
Combinar DataFrames 'verticalmente' con 
concat()
El método concat()  se utiliza típicamente para combinar filas de DataFrames 
separados.
import pandas as pd 
 
df = pd.read_csv('/datasets/vg_sales.csv') 
Resumen del capítulo: Transformación de datos
4
 
rpgs = df[df['genre'] == 'Role-Playing'] 
platformers = df[df['genre'] == 'Platform'] 
 
df_all_games = pd.concat([rpgs, platformers])
¡Dos DataFrames se unen en uno! Recuerda que esto funciona aquí porque ambos 
DataFrames más pequeños tienen las mismas columnas.
Combinar DataFrames 'horizontalmente' 
usando  merge()
El método merge()  permite combinar dos DataFrames mediante algunas claves.
df_1.merge(df_2, on='user_id', how='left')
Hay varios tipos de uniones:
'inner' : la conjunción lógica de ambas tablas (se conservan los registros que 
están presentes en ambos DataFrames).
'left' : todos los valores del DataFrame izquierdo están presentes en el 
DataFrame fusionado. Los valores del DataFrame derecho solo se conservan para 
los valores que coinciden con la columna especificada en el DataFrame izquierdo.
'right' : funciona de manera idéntica a una unión izquierda, excepto que el 
DataFrame combinado conserva todos los valores del DataFrame derecho.
'outer' : todos los valores en la columna especificada se conservan de ambos 
DataFrames originales, pero el DataFrame fusionado tiene valores ausentes donde 
no hay ninguna coincidencia.
El modo de unión se establece con el parámetro  how .
pandas  agregará automáticamente sufijos a los nombres de las columnas cuando las 
columnas tengan el mismo nombre en los DataFrames fusionados. Los sufijos por 
defecto son  _x  y _y . Podemos establecer mejores sufijos pasando una lista de 
cadenas de sufijos al parámetro  suffixes=  en merge():
Resumen del capítulo: Transformación de datos
5
both_pupils = first_pupil_df.merge(second_pupil_df, 
                                   on='author', 
                                   suffixes=['_1st_student', '_2nd_student'] 
                                   ) 
Si las columnas utilizadas para fusionarse tienen nombres diferentes.
both_pupils = first_pupil_df.merge(second_pupil_df, 
                                   left_on='author', 
                                   right_on='Author', 
                                   ) 
Si se asigna un nombre a una columna de índice, el nombre también se puede pasar al 
parámetro  on . También es posible combinar varias columnas a la vez; simplemente 
pasa una lista de ellas al argumento  on .
El método  join()  es similar al método  merge() , se puede considerar como una función 
de acceso directo para  merge() .

### DS_14 Sprint_Resumen_del_captulo_Representaciones_del_lenguaje.pdf ###
Resumen del capítulo: Representaciones del lenguaje
1
Resumen del capítulo: 
Representaciones del lenguaje
Insertados de palabras
Las máquinas no pueden trabajar directamente con palabras, imágenes y audio porque 
están diseñadas para trabajar con números. Los modelos de lenguaje transforman el 
texto en vectores y utilizan un conjunto de técnicas que se denominan colectivamente 
insertados de palabras. Esto significa que una palabra está embebida en un espacio 
vectorial que representa un modelo de lenguaje. Las diferentes áreas en ese espacio 
vectorial tienen diferentes significados desde la perspectiva del lenguaje.
Un vector creado a partir de una palabra mediante el insertado de palabras llevará 
información contextual sobre la palabra y sus propiedades semánticas. De esta forma, 
no se pierde la definición convencional de una palabra ni su significado en contexto.
Las propiedades semánticas son los componentes de una palabra que contribuyen a su 
significado. La palabra "marinero" tiene las siguientes propiedades semánticas: 
"masculino", "ocupación", "persona", "naval". Estas propiedades distinguen la palabra 
"marinero" de otras palabras. Pero las palabras, por lo general, no se sostienen solas y 
están rodeadas de otras palabras en una oración. Compara "El gato enredó todos los 
hilos" y "Perdí el hilo varias veces". La palabra "hilo" tiene un significado diferente 
según el contexto.
Este concepto permite trabajar con palabras, aunque estén vectorizadas, sin perder de 
vista el contexto. Esto significa que las palabras con contextos similares tendrán 
vectores similares. La distancia (de coseno, euclidiana, etc.) entre los vectores es una 
medida común de su similitud.
Word2vec
Entonces, ¿cómo funciona word2vec? Recordarás que en la lección anterior, las 
palabras "pinzón" y "frailecillo" se consideraron similares porque las dos se pueden 
usar en contexto con picos rojos, y las palabras "oso hormiguero" y "perezoso" se 
Resumen del capítulo: Representaciones del lenguaje
2
pueden usar en enunciados sobre Perú. Es decir, el significado de las palabras 
depende del contexto.
Otra cosa que puede hacer word2vec es entrenar un modelo para distinguir pares de 
vecinas verdaderas de las aleatorias. Esta tarea es como una tarea de clasificación 
binaria, donde las características son palabras y el objetivo es la respuesta a la 
pregunta de si las palabras son vecinas verdaderas o no.
Insertados para clasificación
Ahora vamos a descubrir cómo la representación vectorial puede ayudar a resolver 
tareas de clasificación y regresión. Digamos que hay un corpus de texto que necesita 
ser clasificado. En ese caso, nuestro modelo constará de dos bloques:
1. Modelos para convertir palabras en vectores: las palabras se convierten en 
vectores numéricos.
2. Modelos de clasificación: los vectores se utilizan como características.
Veamos los detalles:
1. Antes de pasar a la vectorización de las palabras, tenemos que realizar el 
preprocesamiento:
Cada texto está tokenizado (descompuesto en palabras).
Resumen del capítulo: Representaciones del lenguaje
3
Luego se lematizan las palabras (se reducen a su forma raíz). Sin embargo, los 
modelos más complejos, como BERT, no requieren este paso porque entienden las 
formas de las palabras.
El texto se limpia de palabras vacías o caracteres innecesarios.
Para algunos algoritmos (por ejemplo, BERT), se agregan tokens especiales para 
marcar el comienzo y el final de las oraciones.
2. Cada texto adquiere su propia lista de tokens después del preprocesamiento.
3. Luego, los tokens se pasan al modelo, que los vectoriza mediante el uso de un 
vocabulario de tokens precompilado. En la salida obtenemos vectores de longitud 
predeterminada formados para cada texto.
4. El paso final es pasar las características (vectores) al modelo. Luego, el modelo 
predice la tonalidad del texto: "0" — negativo o "1" — positivo.
BERT
BERT (Representaciones de codificador bidireccional de transformadores) es un 
modelo de red neuronal creado para la representación del lenguaje. Fue realizado por 
Google para mejorar la relevancia de los resultados de búsqueda y fue publicado en 
2018 (el artículo original se puede encontrar en: https://arxiv.org/abs/1810.04805). Este 
algoritmo es capaz de "comprender" el contexto de un texto completo, no solo de frases 
cortas. BERT se usa con frecuencia en el machine learning para convertir textos en 
vectores. Los especialistas suelen utilizar modelos BERT existentes que están 
previamente entrenados (por Google o, posiblemente, por otros colaboradores) en 
grandes corpus de texto. Los modelos BERT previamente entrenados funcionan para 
muchos idiomas (104, con exactitud). Puedes entrenar tu propio modelo de 
representación del lenguaje, pero esto requerirá muchos recursos computacionales.
BERT es un paso evolutivo en comparación con word2vec y se convirtió rápidamente 
en la opción popular para los programadores e inspiró a los investigadores a crear otros 
modelos de representación de lenguaje: FastText, GloVe (vectores globales para 
representación de palabras), ELMO (insertados del modelo de lenguaje), GPT 
Resumen del capítulo: Representaciones del lenguaje
4
(transformador generativo de preentrenamiento). Los modelos más precisos en la 
actualidad son BERT y GPT-3 (aunque este último no está disponible para el público en 
este momento).
Al procesar palabras, BERT considera tanto las palabras vecinas inmediatas como las 
palabras más lejanas. Esto permite que BERT produzca vectores precisos con respecto 
al significado natural de las palabras.
Así es como funciona:
Aquí tienes un ejemplo de entrada para el modelo: "The red beak of the puffin 
[MASK] in the blue [MASK] " donde "MASK" representa palabras desconocidas o 
enmascaradas. El modelo tiene que adivinar cuáles son estas palabras 
enmascaradas.
El modelo aprende a averiguar si las palabras del enunciado están relacionadas. 
Teníamos enmascaradas las palabras "flashed" y "sky". El modelo tiene que 
Resumen del capítulo: Representaciones del lenguaje
5
comprender que una palabra sigue a la otra. Entonces, si ocultáramos la palabra 
"crawled" en lugar de "flashed", el modelo no encontraría una conexión.
BERT y preprocesamiento
Vamos a resolver una tarea de clasificación de reseñas de películas usando la 
representación del lenguaje BERT, es decir, usando BERT para crear vectores para 
palabras. Vamos a tomar un modelo previamente entrenado llamado bert-base-
uncased (entrenado en textos en inglés en minúsculas)..
import torch 
import transformers
Antes de convertir textos en vectores, necesitamos preprocesar el texto. BERT tiene su 
propio tokenizador basado en el corpus en el que fue entrenado. Otros tokenizadores 
no funcionan con BERT y no requieren lematización.
Pasos de preprocesamiento para el texto:
1. Inicializa el tokenizador como una instancia de BertTokenizer()  con el nombre del 
modelo previamente entrenado.
tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased') 
2. Convierte el texto en ID de tokens y el tokenizador BERT devolverá los ID de tokens 
en lugar de los tokens:
example = 'Es muy práctico utilizar transformadores' 
ids = tokenizer.encode(example, add_special_tokens=True) 
print(ids)
Obtenemos:
[101, 2009, 2003, 2200, 18801, 2000, 2224, 19081, 102] 
Resumen del capítulo: Representaciones del lenguaje
6
Para operar el modelo correctamente, establecemos el argumento add_special_tokens  
en True . Significa que agregamos el token inicial (101) y el token final (102) a cualquier 
texto que se esté transformando.
3. BERT acepta vectores de una longitud fija, por ejemplo, de 512 tokens. Si no hay 
suficientes palabras en una cadena de entrada para completar todo el vector con 
tokens (o, más bien, sus identificadores), el final del vector se rellena con ceros. Si hay 
demasiadas palabras y la longitud del vector excede 510 (recuerda que se reservan 
dos posiciones para los tokens de inicio y finalización), o bien la cadena de entrada se 
limita al tamaño de 510, o bien se suelen omitir algunos identificadores devueltos por 
tokenizer.encode() , por ejemplo, todos los identificadores después de la posición 512 en 
la lista:
n = 512 
 
padded = np.array(ids[:n] + [0]*(n - len(ids))) 
 
print(padded) 
Obtenemos:
[101 2009 2003 2200 18801 2000 2224 19081 102 0 0 0 0 ... 0 ]
Ahora tenemos que decirle al modelo por qué los ceros no tienen información 
significativa. Esto es necesario para el componente del modelo que se llama attention. 
Vamos a descartar estos tokens y crear una máscara para los tokens importantes, 
indicando valores cero y distintos de cero:
attention_mask = np.where(padded != 0, 1, 0) 
print(attention_mask.shape) 
Obtenemos:
(512, ) 
Resumen del capítulo: Representaciones del lenguaje
7
Insertados de BERT
Inicializa la configuración BertConfig . Pásale un archivo JSON con la descripción de la 
configuración del modelo. JSON (notación de objetos de JavaScript) es un flujo de 
números, letras, dos puntos y corchetes que devuelve un servidor cuando se le llama.
Inicializa el modelo de la clase BertModel . Pasa el archivo con el modelo previamente 
entrenado y la configuración:
config = BertConfig.from_pretrained('bert-base-uncased') 
model = BertModel.from_pretrained('bert-base-uncased') 
Vamos a comenzar por convertir textos en insertados. Esto puede tardar varios 
minutos, así que accede a la librería tqdm (árabe: taqadum, ﺗﻘﺪّم, “progreso”), que 
muestra el progreso de la operación. Luego simplemente envuelve tu bucle en  tqdm() . 
Utiliza  tqdm.auto  para importar la opción correcta para tu plataforma. Mira cómo 
funciona:
from tqdm.auto import tqdm 
 
for i in tqdm(range(int(8e6))): 
    pass 
 
# aparecerá la barra de progreso 
El modelo BERT crea insertados en lotes. Haz pequeño el tamaño del lote para que la 
RAM no se vea abrumada:
batch_size = 100
Haz un bucle para los lotes. La función  tqdm()  indicará el progreso:
# creación de una lista vacía de insertados de reseñas 
embeddings = [] 
 
for i in tqdm(range(len(ids_list) // batch_size)): 
    ...
Resumen del capítulo: Representaciones del lenguaje
8
Transforma los datos en un formato tensor. Tensor es un vector multidimensional en la 
librería Torch. El tipo de datos LongTensor  almacena números en "formato largo", es 
decir, asigna 64 bits para cada número.
# unión de vectores de ids (de tokens) a un tensor 
ids_batch = torch.LongTensor(ids_list[batch_size*i:batch_size*(i+1)]) 
# unión de vectores de máscaras de atención a un tensor 
attention_mask_batch = torch.LongTensor(attention_mask_list[batch_size*i:batch_size*(i+
1)]) 
Pasa los datos y la máscara al modelo para obtener insertados para el lote:
batch_embeddings = model(ids_batch, attention_mask=attention_mask_batch) 
Utiliza la función no_grad()  (sin gradiente) para indicar que no necesitas gradientes en 
la librería Torch (los gradientes son necesarios para el modo de entrenamiento a la 
hora de crear tu propio modelo BERT). Esto hará que los cálculos sean más rápidos:
with torch.no_grad(): 
    batch_embeddings = model(ids_batch, attention_mask=attention_mask_batch) 
Extrae los elementos requeridos del tensor y agrega la lista de todos los insertados:
# conversión de los elementos de tensor a numpy.array con la función numpy() 
embeddings.append(batch_embeddings[0][:,0,:].numpy()) 
Uniendo todo lo anterior, obtenemos este bucle:
batch_size = 100 
 
embeddings = [] 
 
for i in tqdm(range(len(ids_list) // batch_size)): 
     
    ids_batch = torch.LongTensor(ids_list[batch_size*i:batch_size*(i+1)]) 
    attention_mask_batch = torch.LongTensor(attention_mask_list[batch_size*i:batch_size*(i
+1)]) 
 
    with torch.no_grad(): 
Resumen del capítulo: Representaciones del lenguaje
9
        batch_embeddings = model(ids_batch, attention_mask=attention_mask_batch) 
 
    embeddings.append(batch_embeddings[0][:,0,:].numpy()) 
Llama a la función concatenate()  para concatenar todos los insertados en una matriz de 
características:
features = np.concatenate(embeddings) 

### DS_ES_9_Sprint_Hoja_informativa_Recopilacin_de_datos.pdf ###
Hoja informativa: Recopilación de datos
1
Hoja informativa: Recopilación 
de datos
Práctica
# Validación cruzada 
# model: modelo no entrenado para validación cruzada; 
# cv: número de bloques para validación cruzada (son 3, por defecto). 
 
 
from sklearn.model_selection import cross_val_score 
cross_val_score(model, features, target, cv=3)
Teoría
Un rastreador o raspador es un software que se utiliza para extraer datos de sitios 
web.
Datos sin etiquetar son datos que carecen del valor objetivo.
Etiquetado de datos o anotación de datos es el proceso de determinar los valores 
objetivo.
Datos etiquetados son datos con un valor objetivo conocido.
Fuga de información es una situación en la que se filtra ****información sobre el 
objetivo accidentalmente en las funciones.
La validación cruzada es un método de entrenamiento y prueba de modelos cuando 
el conjunto de entrenamiento se divide en K bloques iguales. En cada una de las 
etapas K, el índice de bloque i se utiliza para la validación y el resto para el 
entrenamiento.

### DS_ES_9_Sprint_Resumen_del_captulo_Recopilacin_de_datos.pdf ###
Resumen del capítulo: Recopilación de datos
1
Resumen del capítulo: 
Recopilación de datos
Fuentes de datos
Hay muchas fuentes de datos que puedes usar para entrenar modelos.
Una de las fuentes es el almacén de datos de la empresa. Todo lo que tenemos que 
hacer es extraer ese historial y prepararnos para el análisis. Pero ese es un escenario 
bastante optimista.
La mayoría de las veces, la empresa no puede proporcionar los datos. Si la tarea es 
común, los conjuntos de datos se pueden encontrar en fuentes abiertas:
Plataforma de competencia Kaggle Data Science;
Repositorio de machine learning UC Irvine;
Base de datos abierta del gobierno de los EE. UU.;
FiveThirtyEight: datos abiertos sobre análisis de encuestas de opinión, política, 
economía y más.
Para algunas tareas, los datos se pueden recopilar en Internet. Descarga todas las 
páginas del portal requerido y usa el software crawler o scraper para extraer los datos.
Etiquetado de datos
Quizás la empresa tiene los datos, pero le faltan los valores objetivo. Dichos datos no 
están etiquetados, pero sí puedes usarlos. Para obtener un conjunto de 
entrenamiento, debemos realizar etiquetado de datos o anotación de datos. Es decir, 
establecer la respuesta correcta para cada foto (una persona o algo más). Esto 
etiquetará efectivamente los datos. En ocasiones, el etiquetado se puede realizar sin 
conocimientos especiales (como es el caso de las fotos de perfil), pero cuando los 
datos se relacionan con la salud y el bienestar de las personas, es posible que 
necesites asesoría profesional.
Resumen del capítulo: Recopilación de datos
2
Hay servicios en línea dedicados para el etiquetado. Los datos sin etiquetar se cargan 
en el recurso y se especifica el precio del etiquetado por observación. Cualquiera 
puede ir al servicio, etiquetar los datos y cobrar por ello.
Estos son algunos servicios populares:
Amazon Mechanical Turk
Y.Toloka
Control de calidad de etiquetado
La calidad de los datos después del etiquetado se puede mejorar utilizando los 
métodos para el control de calidad del etiquetado. ¿Cómo funcionan? Todas las 
observaciones, o una parte de ellas, se etiquetan varias veces y luego se forma la 
respuesta final.
Veamos uno de esos métodos, el voto mayoritario. ¿Quién está "votando" y cómo? 
Por ejemplo, cada observación está etiquetada por tres evaluadores. La respuesta final 
es la elegida por la mayoría.
Fuga de información
Ya recopilaste los datos. Ahora podemos verificar si hay fugas de información. Ocurre 
cuando la información sobre el objetivo se filtra accidentalmente en las funciones.
Validación cruzada
Ya sabes cómo formar muestras de entrenamiento, prueba y validación, y que la 
representatividad se logra a través del muestreo aleatorio. Pero ¿cómo podemos 
asegurarnos de que la distribución sea correcta en un conjunto de datos grande? 
¡Toma varias muestras al azar!
La validación cruzada te ayudará a entrenar y probar el modelo utilizando varias 
muestras formadas al azar.
¿Como funciona? Dividimos todos los datos en conjunto de entrenamiento y conjunto 
de prueba. Mantenemos el conjunto de prueba hasta la evaluación final y dividimos 
aleatoriamente el conjunto de entrenamiento en K bloques iguales. El método de 
división en sí se llama K-Fold, donde K es el número de bloques o pliegues (de ahí el 
nombre).
Resumen del capítulo: Recopilación de datos
3
Supongamos que en la primera etapa del procedimiento, el "Bloque 1" es el conjunto 
de validación y el resto de los bloques son para entrenamiento. En la segunda etapa, el 
"Bloque 2" se usa para la validación y el resto de los bloques se usan para el 
entrenamiento. Avanzando de esta manera, cada bloque llega a ser el conjunto de 
validación. Entonces el proceso se repite K veces.
Estamos "cruzando" los datos, tomando cada vez un nuevo bloque para la validación. Y 
la media de todos los valores obtenidos a través de la validación cruzada es el puntaje 
de evaluación de nuestro modelo.
El método de validación cruzada se asemeja a un bootstrap en el que se forman varias 
muestras, pero la diferencia es que la validación cruzada usa subconjuntos con 
contenido fijo que no cambia en cada etapa de entrenamiento y validación. Cada 
observación pasa por el conjunto de entrenamiento y el conjunto de validación.
Resumen del capítulo: Recopilación de datos
4
La validación cruzada es útil cuando necesitamos comparar modelos, seleccionar 
hiperparámetros o evaluar la utilidad de las funciones. Minimiza la aleatoriedad de la 
división de datos y proporciona un resultado más preciso. El único inconveniente de la 
validación cruzada es el tiempo de cálculo, especialmente con muchas observaciones o 
un valor alto de K. Es mucho tiempo.
Validación cruzada en Sklearn
La validación cruzada puede llevar menos tiempo si usamos las herramientas de 
sklearn. Para evaluar el modelo por validación cruzada usaremos la función 
cross_val_score del módulo sklearn.model_selection.
Así se llama a la función:
from sklearn.model_selection import cross_val_score 
cross_val_score(model, features, target, cv=3)
La función toma varios argumentos, como:
model: modelo para validación cruzada. Está entrenado en el proceso de validación 
cruzada, por lo que tenemos que pasarlo sin entrenar. Supongamos que 
necesitamos este modelo para un árbol de decisión:
from sklearn.tree import DecisionTreeClassifier 
model = DecisionTreeClassifier()
features 
target
cv — número de bloques para validación cruzada (son 3, por defecto)
La función no requiere dividir los datos en bloques o muestras para la validación y el 
entrenamiento. Todos estos pasos se realizan de forma automática. La función 
devuelve una lista de valores de evaluación del modelo de cada validación. Cada valor 
es igual a model.score() para la muestra de validación. Por ejemplo, para una tarea de 
clasificación, esto es exactitud.

### Eduardo Ortega Torres_DS.pdf ###
#0001008
17/04/2025
Eduardo Ortega
Torres   

### ESP_Hoja_informativa_Representaciones_del_lenguaje.pdf ###
Hoja informativa: Representaciones del lenguaje
1
Hoja informativa: 
Representaciones del lenguaje
Práctica
# clasificación mediante el uso de insertados 
# df: dataset 
# modelo: modelo de clasificación 
 
import math 
import numpy as np 
import pandas as pd 
 
import torch 
import transformers 
 
from sklearn.linear_model import LogisticRegression 
from sklearn.model_selection import cross_val_score 
from sklearn.model_selection import train_test_split 
 
from tqdm.auto import tqdm 
 
# comprobar la existencia de otros modelos en https://huggingface.co/transformers/pretrain
ed_models.html 
tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased') 
model = transformers.BertModel.from_pretrained('bert-base-uncased') 
 
def BERT_text_to_embeddings(texts, max_length=512, batch_size=100, force_device=None, disa
ble_progress_bar=False): 
     
    ids_list = [] 
    attention_mask_list = [] 
 
    # texto a los ID de relleno de tokens junto con sus máscaras de atención 
     
    for input_text in tqdm(texts, disable=disable_progress_bar): 
        ids = tokenizer.encode(input_text.lower(), add_special_tokens=True, truncation=Tru
e, max_length=max_length) 
        padded = np.array(ids + [0]*(max_length - len(ids))) 
        attention_mask = np.where(padded != 0, 1, 0) 
        ids_list.append(padded) 
        attention_mask_list.append(attention_mask) 
     
    if force_device is not None: 
        device = torch.device(force_device) 
Hoja informativa: Representaciones del lenguaje
2
    else: 
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') 
         
    model.to(device) 
    if not disable_progress_bar: 
        print(f'Uso del dispositivo {device}.') 
     
    # obtener insertados en lotes 
 
    embeddings = [] 
 
    for i in tqdm(range(math.ceil(len(ids_list)/batch_size)), disable=disable_progress_ba
r): 
             
        ids_batch = torch.LongTensor(ids_list[batch_size*i:batch_size*(i+1)]).to(device) 
        attention_mask_batch = torch.LongTensor(attention_mask_list[batch_size*i:batch_siz
e*(i+1)]).to(device) 
             
        with torch.no_grad():             
            model.eval() 
            batch_embeddings = model(input_ids=ids_batch, attention_mask=attention_mask_ba
tch)    
        embeddings.append(batch_embeddings[0][:,0,:].detach().cpu().numpy()) 
         
    return np.concatenate(embeddings) 
 
# ¡Atención! 
# Ejecutar BERT para miles de textos puede llevar mucho tiempo en la CPU, al menos varias
 horas. 
# Intenta encontrar una máquina con GPU que ejecute BERT en varios minutos en lugar de hor
as. 
features = BERT_text_to_embeddings(df['text']) 
target = df['target'] 
 
train_features, test_features, train_target, test_target = train_test_split( 
    features, target, train_size=.8) 
 
model.fit(train_features, train_target) 
 
print(model.score(test_features, test_target))
Teoría
El insertado de palabras es un método de representación de textos con vectores 
respecto a un modelo lingüístico obtenido a partir de un gran corpus de textos.

### ESP_Hoja_informativa_Vectorizacin_de_textos.pdf ###
Hoja informativa: Vectorización de textos
1
Hoja informativa: Vectorización 
de textos
Práctica
# NLTK: obtener la lista de palabras lematizadas de un texto 
 
# es posible que al principio tengas que descargar los archivos de wordnet 
# importar nltk 
# nltk.download('wordnet') # https://wordnet.princeton.edu/ 
 
from nltk.tokenize import word_tokenize 
from nltk.stem import WordNetLemmatizer 
 
lemmatizer  = WordNetLemmatizer() 
 
text = "All models are wrong, but some are useful.". 
 
tokens = word_tokenize(text.lower()) 
lemmas = [lemmatizer.lemmatize(token) for token in tokens] 
 
# mostrar como una sola línea 
print(" ".join(lemmas))
# spaCy: obtener la lista de palabras lematizadas de un texto 
 
# es posible que al principio tengas que descargar un modelo spaCy con 
# python -m spacy descargar en 
 
import spacy 
 
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner']) 
 
doc = nlp(text.lower()) 
 
lemmas = [token.lemma_ for token in doc] 
 
print(" ".join(lemmas))
Hoja informativa: Vectorización de textos
2
# patrón: comprueba la sintaxis en https://docs.python.org/3.7/library/re.html 
# sustitución: con qué debe sustituirse cada coincidencia de patrón 
# texto: el texto que la función escanea en busca de coincidencias con el patrón 
 
import re 
re.sub(pattern, replacement, text)
# obtener palabras vacías para el español 
 
from nltk.corpus import stopwords 
 
stop_words = set(stopwords.words('spanish'))
# construir una bolsa de palabras, un modelo simple para vectorizar textos 
# stopwords: lista de palabras vacías 
 
from nltk.corpus import stopwords 
from sklearn.feature_extraction.text import CountVectorizer 
 
stop_words = set(stopwords.words('spanish')) 
count_vect = CountVectorizer(stop_words=stop_words) 
 
bow = count_vect.fit_transform(corpus) 
 
# diccionario de palabras únicas 
words = count_vect.get_feature_names() 
 
# mostrar una matriz de bolsa de palabras 
print(bow.toarray())
# construir una bolsa de palabras de n-gramas (sin palabras vacías) 
# min_n: valor mínimo de n 
# max_n - valor máximo de n 
 
from sklearn.feature_extraction.text import CountVectorizer 
 
count_vect = CountVectorizer(ngram_range=(min_n, max_n)) 
 
bow = count_vect.fit_transform(corpus) 
 
# diccionario de n-gramas 
words = count_vect.get_feature_names() 
 
# mostrar una matriz de bolsa de palabras 
print(bow.toarray())
Hoja informativa: Vectorización de textos
3
# construir la TF-IDF para un corpus 
 
from nltk.corpus import stopwords 
from sklearn.feature_extraction.text import TfidfVectorizer 
 
stop_words = set(stopwords.words('spanish')) 
 
tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words) 
tfidf = tfidf_vectorizer.fit_transform(corpus)
# construir la TF-IDF para n-gramas de un corpus 
# min_n: valor mínimo de n 
# max_n - valor máximo de n 
 
from nltk.corpus import stopwords 
from sklearn.feature_extraction.text import TfidfVectorizer 
 
stop_words = set(stopwords.words('spanish')) 
 
tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, ngram_range=(min_n, max_n)) 
tfidf = tfidf_vectorizer.fit_transform(corpus)
Teoría
Tokenización: dividir el texto en tokens, o sea, frases, palabras y símbolos separados.
Lematización: reducir una palabra a su forma raíz (lema).
Corpus: conjunto de textos, generalmente asociados temáticamente entre sí.
Expresión regular: una secuencia de caracteres que define un patrón de búsqueda. 
Un patrón suele utilizarse para encontrar diferentes ocurrencias que se ajusten al 
mismo, así por ejemplo, un patrón puede describir diferentes números de teléfono, 
direcciones de correo electrónico, etc.
Bolsa de palabras: un modelo sencillo para convertir textos en vectores sin tener en 
cuenta el orden de las palabras en los textos ni su importancia.
TF-IDF: un modelo sencillo para convertir textos en vectores sin tener en cuenta el 
orden de las palabras, pero sí su importancia.
Hoja informativa: Vectorización de textos
4
N-grama: una secuencia de N tokens.
Análisis de sentimientos: tarea de PNL utilizada para identificar el tono de un texto 
(documento).

### ESP_Resumen_del_captulo_Vectorizacin_de_textos.pdf ###
Resumen del capítulo: Vectorización de textos
1
Resumen del capítulo: 
Vectorización de textos
Lematización
Estos son los pasos para el preprocesamiento de textos:
1. Tokenización: tendrás que dividir el texto en tokens (frases, palabras y símbolos); 
2. Lematización: tendrás que reducir las palabras a sus formas raíz (lema).
Puedes usar estas librerías tanto para la tokenización como para la lematización:
Natural Language Toolkit (NLTK)
spaCy
Importa la función de tokenización y crea un objeto de lematización:
from nltk.tokenize import word_tokenize 
from nltk.stem import WordNetLemmatizer 
 
lemmatizer  = WordNetLemmatizer()
Pasa a la función lemmatize()  el texto "Todos los modelos son incorrectos, pero algunos 
son útiles" como tokens separados:
text = "All models are wrong, but some are useful." 
 
tokens = word_tokenize(text.lower()) 
 
lemmas = [lemmatizer.lemmatize(token) for token in tokens]
La función word_tokenize()  divide un texto en tokens y la función lemmatize()  devuelve 
el lema de un token que se le pasó. Debido a que nos interesa lematizar una oración, el 
resultado generalmente se presenta como una lista de tokens lematizados.
Resumen del capítulo: Vectorización de textos
2
['all', 'model', 'are', 'wrong', ',', 'but', 'some', 'are', 'useful', '.'] 
También convertimos el texto a minúsculas porque así lo exige el lematizador NLTK.
Usa la función join()  para volver a convertir la lista de tokens procesados en una línea 
de texto, separando los elementos con un espacio opcional:
" ".join(lemmas) 
Obtenemos:
'all model be wrong , but some be useful .' 
Un conjunto de textos se denomina colectivamente corpus. Por lo general, es necesario 
definir un corpus para una tarea de análisis de texto (por ejemplo, para construir un 
diccionario o un espacio vectorial basado en él). Un algoritmo de machine learning 
procesa cada registro de texto en función de su "posición" en el corpus. No te 
preocupes, analizaremos todo esto con más detalle en las próximas lecciones.
Crea un corpus a partir de las reseñas. Convierte la columna review  en una lista de 
textos.
corpus = data['review']
Expresiones regulares
Una expresión regular es un instrumento para encontrar patrones complejos en los 
textos. Estos patrones pueden luego ser manipulados como a ti te guste (extraerlos, 
reemplazarlos, etc.). ¡Las expresiones regulares son herramientas poderosas que se 
usan casi en cualquier lugar donde aparezca un texto!
Python tiene un módulo incorporado para trabajar con expresiones regulares: re  (que 
significa "expresiones regulares"):
Resumen del capítulo: Vectorización de textos
3
import re
Echa un vistazo a la función re.sub() . Esta encuentra todas las partes del texto que 
coinciden con el patrón dado y luego las sustituye con el texto elegido.
# patrón 
# sustitución: con qué debe sustituirse cada coincidencia de patrón 
# texto: el texto que la función escanea en busca de coincidencias con el patrón 
re.sub(pattern, substitution, text) 
Solo necesitamos mantener las letras y los espacios en estos textos lematizados de 
reseñas, así que vamos a escribir una expresión regular para encontrarlos.
La expresión comienza por r y va seguida de corchetes entre comillas:
r'[]'
Todas las letras que coinciden con el patrón se enlistan entre corchetes, sin espacios, y 
se pueden colocar en cualquier orden. Vamos a encontrar las letras de la "a" a la "z". Si 
asumimos que pueden estar tanto en minúsculas como en mayúsculas, entonces el 
código debería escribirse de la siguiente manera:
# un rango de letras se indica con un guión: 
# a-z = abcdefghijklmnopqrstuvwxyz 
r'[a-zA-Z]' 
Vamos a tomar una de las reseñas del conjunto de datos. Necesitamos mantener todas 
las letras latinas, los espacios y los apóstrofos, así que nuestro patrón deberá 
identificarlos correctamente. Si llamamos a re.sub() , serán sustituidos por espacios. 
Para indicar los caracteres que no coinciden con el patrón, pon un caret (^) delante de 
la secuencia:
# texto de reseña 
text = """ 
Me gustó este programa desde el primer episodio que vi, que fue el episodio "Rhapsody in B
Resumen del capítulo: Vectorización de textos
4
lue" (para quienes no saben qué es, el Zan se vuelve loco y se convierte en pau episodio n
ivel 10). Los mejores efectos visuales y especiales que había visto en una serie de televi
sión, no hay nada parecido en ninguna parte. 
""" 
re.sub(r'[^a-zA-Z\']', ' ', text) 
" Me gustó este programa desde el primer episodio que vi que fue el  episodio  Rhapsody in 
Blue  para quienes no saben qué es  el Zan se vuelve loco y se convierte en pau episodio n
ivel    10   Los mejores efectos visuales y especiales que había visto en una serie de tel
evisión no hay nada parecido en ninguna parte  " 
Ahora no nos quedan más que las letras latinas y los espacios. En el siguiente paso, 
vamos a deshacernos de los espacios extra, ya que dificultan el análisis. Podemos 
eliminarlos utilizando la combinación de las funciones join()  y split() .
Veamos el ejemplo de un texto con espacios extra en medio, al principio y al final de la 
línea. Utiliza el método split()  para convertirlo en una lista. Si llamamos a split()  sin 
argumentos, dividirá el texto en espacios o grupos de espacios:
text = "           Me   gustó este programa " 
text.split() 
El resultado es una lista sin espacios:
['Me', 'gustó', 'este', 'programa'] 
Combina estos elementos en una línea con espacios usando el método join() :
" ".join(['Me', 'gustó', 'este', 'programa']) 
Entonces obtenemos una línea sin espacios adicionales:
'Me gustó este programa' 
Bolsa de palabras y n-grama
Resumen del capítulo: Vectorización de textos
5
Ahora vamos a aprender cómo podemos convertir datos de texto en datos numéricos, 
los cuales son un tipo de datos mucho más adecuado para las computadoras. La 
conversión generalmente se realiza para que una palabra o una oración se convierta en 
un vector numérico.
Una técnica común para convertir texto se conoce como modelo "bolsa de palabras". 
Lo que hace es transformar textos en vectores sin tomar en cuenta el orden de las 
palabras y, por eso, se llama bolsa.
Tomemos como ejemplo un proverbio famoso:
For want of a nail the shoe was lost. 
For want of a shoe the horse was lost. 
For want of a horse the rider was lost.
Si nos deshacemos de las letras mayúsculas y lematizamos con spaCy, obtenemos 
esto:
for want of a nail the shoe be lose  
for want of a shoe the horse be lose  
for want of a horse the rider be lose
Vamos a contar cuántas veces aparece cada palabra:
"for," "want," "of," "a," "the," "be," "lose" — 3;
"shoe," "horse" — 2;
"nail," "rider" — 1.
"herraje", "caballo" - 2;
"clavo", "jinete" - 1.
Resumen del capítulo: Vectorización de textos
6
Aquí está el vector de este texto:
[2, 2, 2, 1, 1, 1, 1, 1]
Si hay varios textos, la bolsa de palabras los transforma en una matriz. Las filas 
representan los textos y las columnas las palabras únicas de todos los textos del 
corpus. Los números en sus intersecciones representan ocurrencias de cada palabra 
única.
La bolsa de palabras cuenta cada palabra única, pero no considera el orden de las 
palabras ni las conexiones entre ellas. Por ejemplo, observa este texto lematizado:
Peter travel from Tucson to Vegas 
Esta es la lista de palabras: "Peter", "travel", "from", "Tucson, "to", "Vegas" ("Peter," 
"viaja," "de," "Tucson," "a", "Vegas"). Entonces ¿a dónde va Peter? Para responder a la 
pregunta, veamos las frases o n-gramas.
Un n-grama es una secuencia de varias palabras. N indica el número de elementos y 
es arbitrario. Por ejemplo, si N=1, tenemos palabras separadas o unigramas. Si N=2, 
tenemos frases de dos palabras o bigramas. Si N=3, tenemos trigramas. Está claro, 
¿cierto?
Encontremos todos los trigramas para la oración "Sunset raged like a beautiful 
bonfire”(”Atardecer ardió como una hermosa fogata.”)
Resumen del capítulo: Vectorización de textos
7
Tenemos cuatro trigramas: "Sunset raged like", "raged like a", "like a beautiful", "a 
beautiful bonfire" ("Atardecer ardió como", "ardió como una", "como una hermosa", "una 
hermosa fogata"). La palabra "beautiful" no puede iniciar un trigrama porque solo hay 
una palabra después de esta y necesitamos dos.
Volvamos con Peter. Encuentra los bigramas en el texto. "Peter viaja", "viaja de", "de 
Tucson", "Tuscon a", "a Vegas.” Ahora tenemos las partes A y B del viaje. Peter va de 
Tucson a Vegas.
Los n-gramas son similares a las bolsas de palabras porque también se pueden 
convertir en vectores. Este es el vector para el texto de Peter:
Crear una bolsa de palabras
Vamos a aprender a crear una bolsa de palabras y a encontrar palabras vacías.
Para convertir un corpus de texto en una bolsa de palabras, usa la clase 
CountVectorizer()  del módulo sklearn.feature_extraction.text.
Importa la clase:
from sklearn.feature_extraction.text import CountVectorizer 
Crea un contador:
count_vect = CountVectorizer() 
Resumen del capítulo: Vectorización de textos
8
Pasa el corpus de texto al contador. Llama a la función fit_transform() . El contador 
extrae palabras únicas del corpus y cuenta cuántas veces aparecen en cada texto del 
corpus. El contador no cuenta letras separadas.
# bow = bolsa de palabras 
bow = count_vect.fit_transform(corpus) 
Este método devuelve una matriz donde las filas representan textos y las columnas 
muestran palabras únicas del corpus. Los números en sus intersecciones representan 
cuántas veces aparece una determinada palabra en el texto.
Usemos el corpus (ya lematizado) de la lección anterior:
corpus = [ 
    'for want of a nail the shoe be lose', 
    'for want of a shoe the horse be lose', 
    'for want of a horse the rider be lose', 
    'for want of a rider the message be lose', 
    'for want of a message the battle be lose', 
    'for want of a battle the kingdom be lose', 
    'and all for the want of a horseshoe nail' 
]
Vamos a crear una bolsa de palabras para la matriz. Utiliza el atributo shape  para 
descubrir el tamaño de la matriz:
bow.shape 
(7, 16) 
El resultado es 7 textos y 16 palabras únicas.
Aquí está nuestra bolsa de palabras como una matriz:
print(bow.toarray()) 
Resumen del capítulo: Vectorización de textos
9
[[0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1] 
 [0 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1] 
 [0 0 0 1 1 0 0 1 0 0 1 1 0 1 1 1] 
 [0 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1] 
 [0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 1] 
 [0 0 1 1 0 0 1 1 0 0 1 0 0 1 1 1] 
 [1 1 0 1 0 1 0 0 0 1 1 0 0 1 1 0]] 
La lista de palabras únicas en la bolsa se llama vocabulario; se almacena en el 
contador y se puede acceder a ella llamando al método get_feature_names() :
count_vect.get_feature_names()
Aquí está el vocabulario para nuestro ejemplo:
['all', 
 'and', 
 'battle', 
 'for', 
 'horse', 
 'horseshoe', 
 'kingdom', 
 'lost', 
 'message', 
 'nail', 
 'of', 
 'rider', 
 'shoe', 
 'the', 
 'want', 
 'was'] 
CountVectorizer()   también se usa para cálculos de n-gramas. Especifica el tamaño del 
n-grama con el argumento ngram_range  para que cuente las frases. Por ejemplo, si 
necesitamos encontrar frases de dos palabras, debemos especificar el rango de esta 
manera:
count_vect = CountVectorizer(ngram_range=(2, 2))
El contador funciona de la misma forma con frases y con palabras.
Resumen del capítulo: Vectorización de textos
10
Dado que un corpus grande conlleva una bolsa de palabras más grande, algunas de 
las palabras pueden mezclarse y terminar causando más confusión que claridad. Para 
ayudar con esto, por lo general, puedes eliminar las conjunciones y las preposiciones 
sin perder el significado de la oración. Si tienes una bolsa de palabras más pequeña y 
limpia, encontrarás más fácilmente las palabras más importantes para la clasificación 
del texto.
Para asegurarte de obtener una bolsa de palabras más limpia, encuentra las palabras 
vacías (palabras que no significan nada por sí solas). Hay muchas, y difieren según el 
idioma. Echemos un vistazo al paquete stopwords  del módulo nltk.corpus :
from nltk.corpus import stopwords
Deberás descargar el paquete una vez para que funcione:
import nltk 
nltk.download('stopwords')
Llama a la función stopwords.words()  y utiliza 'spanish'   como un argumento para 
obtener un conjunto de palabras vacías para español:
stop_words = set(stopwords.words('spanish'))
Pasa la lista de palabras vacías al CountVectorizer()  al crear el contador:
count_vect = CountVectorizer(stop_words=stop_words)
Ahora el contador sabe qué palabras se deben excluir de la bolsa de palabras.
TF-IDF
La importancia de una palabra dada se determina por el valor de TF-IDF (frecuencia 
de término-frecuencia inversa de documento). La TF representa el número de 
Resumen del capítulo: Vectorización de textos
11
ocurrencias de una palabra en un texto y la IDF mide la frecuencia con la que aparece 
en el corpus.
Esta es la fórmula para TF-IDF:
Así es como se calcula TF:
En la fórmula, t (término) es el número de ocurrencias de la palabra y n es el número 
total de palabras en el texto.
El papel de la IDF en la fórmula es reducir el peso de las palabras más utilizadas en 
cualquier otro texto del corpus dado. La IDF depende del número total de textos en un 
corpus (D) y el número de textos donde aparece la palabra (d).
TF-IDF en sklearn
Puedes calcularla mediante la librería sklearn. La clase TfidfVectorizer()  se encuentra 
en el módulo sklearn.feature_extraction.text . Impórtala como se muestra a 
continuación:
from sklearn.feature_extraction.text import TfidfVectorizer
Resumen del capítulo: Vectorización de textos
12
Crea un contador y define palabras vacías, tal como lo hicimos con CountVectorizer() :
stop_words = set(stopwords.words('spanish')) 
count_tf_idf = TfidfVectorizer(stop_words=stop_words) 
Llama a la función fit_transform()  para calcular la TF-IDF para el corpus de texto:
tf_idf = count_tf_idf.fit_transform(corpus)
Podemos calcular los n-gramas al pasar el argumento ngram_range  a TfidfVectorizer() .
Si los datos se dividen en conjuntos de entrenamiento y prueba, llama a la función 
fit()  solo para el conjunto de entrenamiento. De lo contrario, la prueba estará 
sesgada porque el modelo tomará en cuenta las frecuencias de las palabras del 
conjunto de prueba.
Análisis de sentimiento
Con el fin de determinar el tono del texto, podemos utilizar los valores TF-IDF como 
características.
El análisis de sentimiento identifica textos cargados de emociones. Esta herramienta 
puede ser extremadamente útil en los negocios al evaluar las reacciones de los 
consumidores ante un nuevo producto. Un humano necesitaría varias horas para 
analizar miles de reseñas, mientras que una computadora lo haría en un par de 
minutos.
El análisis de sentimiento funciona etiquetando el texto como positivo o negativo. Al 
texto positivo se le asigna un "1" y al texto negativo se le asigna un "0".

### ES_DS 13 Sprint_Resumen_del_captulo_Anlisis_de_series_temporales.pdf ###
Resumen del capítulo: Análisis de series temporales
1
Resumen del capítulo: Análisis 
de series temporales
Series temporales
Las series temporales son las secuencias de números a lo largo del eje del tiempo. El 
intervalo entre los valores de la serie es constante.
En pandas, el trabajo adecuado con fecha y hora requiere cambiar el tipo de datos en 
la columna de object  a datetime64 . Se puede hacer al leer, usando el argumento 
parse_dates . También tenemos que establecer el índice del DataFrame especificando 
las columnas deseadas en el argumento index_col :
# valores de parse_dates = lista de números de columna o nombres de columna 
data = pd.read_csv('filename.csv', index_col=[0], parse_dates=[0])
Para verificar si las fechas y horas están en orden cronológico, observa el atributo 
is_monotonic  del índice de la tabla. Si el orden es cronológico, el atributo devolverá 
True ; si no, False .
print(data.index.is_monotonic)
Podemos especificar fecha y hora como índice y seleccionar los datos por fecha:
data = data['2016':'2017']  
data = data['2016-01':'2016-06']  
data = data['2016-01-01':'2016-01-10'] 
Resumen del capítulo: Análisis de series temporales
2
Remuestreo
Remuestrear significa cambiar el intervalo con los valores de la serie. Se realiza en 
dos pasos:
1. Elige la nueva duración del intervalo. Considera que los valores del intervalo 
existente están agrupados.
2. En cada grupo se calcula el valor acumulado de la serie. Puede ser mediana, 
media, máximo o mínimo.
Resumen del capítulo: Análisis de series temporales
3
Para cambiar el intervalo y agrupar los valores, llama a la función resample() . 
Especifica el nuevo intervalo en el argumento. Aquí tienes un ejemplo:
# 1H = una hora 
data.resample('1H')  
 
Resumen del capítulo: Análisis de series temporales
4
# 2W = dos semanas 
data.resample('2W')
La función resample()  es similar a la función groupby() . Después de agrupar, llama a las 
funciones mean()  y max()  para agregar los valores:
# media para cada hora 
data.resample('1H').mean() 
 
# máximo por cada dos semanas 
data.resample('2W').max()
Media móvil
La media móvil o promedio móvil es un método para suavizar los datos en una serie 
temporal. El método consiste en encontrar los valores menos susceptibles a 
fluctuaciones, es decir, la media aritmética.
Así es como funciona el método: el intervalo para promediar (tamaño de ventana) se 
selecciona de forma experimental. Cuanto mayor sea el intervalo, más fuerte será el 
suavizado. Luego, la ventana comienza a desplazarse casi desde el principio hasta el 
final de la serie temporal. En cada punto se calcula el valor medio.
En el promedio móvil, las ventanas se superponen y no pueden ir más allá de la serie. 
Esto significa que el número de medias obtenidas será ligeramente menor que el 
número de valores iniciales de la serie.
Resumen del capítulo: Análisis de series temporales
5
En pandas, la media móvil se calcula en dos pasos:
1. Llama a la función rolling()  para crear una ventana móvil. Especifica el tamaño de 
la ventana en el argumento:
# tamaño de la ventana 7 
data.rolling(7) 
2. Llama a la función mean()  para agregar los valores:
# media móvil con tamaño de ventana 7 
data.rolling(7).mean() 
Tendencias y estacionalidad
Una tendencia es un cambio ligero del valor medio de la serie sin repetir patrones. Por 
ejemplo, el incremento anual en la venta de boletos de avión.
Estacionalidad significa patrones que se repiten de forma cíclica en una serie 
temporal. Por ejemplo, el crecimiento de las ventas de boletos de avión cada verano.
Las tendencias y la estacionalidad dependen de la escala de los datos. No puedes ver 
patrones que se repiten todos los veranos si solo hay datos de un año.
Observa el gráfico rolling_mean . El aumento del consumo eléctrico en invierno y en 
verano es una tendencia.
Resumen del capítulo: Análisis de series temporales
6
Si se analizan estos datos en una escala de varios años, el aumento en el invierno y en 
el verano serían cambios estacionales.
El módulo tsa.seasonal (tsa significa análisis de series temporales en inglés) de la 
librería statsmodels contiene la función seasonal_decompose() . Esta función divide la 
serie en tres componentes: tendencia, estacionalidad y residuos. El componente 
residual no puede explicarse por la tendencia y la estacionalidad, y es esencialmente 
solo ruido.
from statsmodels.tsa.seasonal import seasonal_decompose 
 
decomposed = seasonal_decompose(data)
season_decompose()  toma una serie temporal y devuelve una instancia de la clase 
DecomposeResult . Contiene los atributos requeridos:
decomposed.trend : tendencia
decomposed.seasonal : componente estacional
decomposed.resid : residuos
Series estacionarias
Resumen del capítulo: Análisis de series temporales
7
En estadística, la serie temporal se describe como un proceso estocástico. Tiene 
variación aleatoria y su distribución cambia con el tiempo. Tiene una media y una 
varianza y estos valores también cambian.
Un proceso estocástico es estacionario si su distribución no cambia con el tiempo. Un 
ejemplo de proceso estocástico estacionario son las fluctuaciones periódicas de 
valores.
Si la distribución cambia, entonces el proceso estocástico es no estacionario.
No podemos averiguar la distribución de una serie temporal. Entonces, la serie 
temporal estacionaria es una serie en la que la media y la desviación estándar no 
cambian. De las dos series, aquella en la que la media y la desviación estándar 
cambian más lentamente es “más estacionaria”.
Las series de tiempo no estacionarias son más difíciles de pronosticar porque sus 
propiedades cambian demasiado rápido.
Resumen del capítulo: Análisis de series temporales
8
Diferencias de series temporales
Las diferencias de series temporales son una secuencia de diferencias entre 
elementos vecinos de una serie temporal (es decir, el valor anterior se resta del 
siguiente).
El método shift()  se usa para encontrar las diferencias de series temporales. Todos 
los valores se desplazan un paso hacia adelante a lo largo del eje de tiempo:
data = pd.Series([0.5, 0.7, 2.4, 3.2]) 
print(data) 
print(data.shift())
0    0.5 
1    0.7 
2    2.4 
3    3.2 
dtype: float64 
0    NaN 
1    0.5 
2    0.7 
3    2.4 
dtype: float64
El último valor de la serie se pierde porque no hay lugar para cambiarlo. El valor cero 
es NaN , ya que no tiene valor. Agrega un argumento para completar los valores 
ausentes.
import pandas as pd 
 
data = pd.Series([0.5, 0.7, 2.4, 3.2]) 
print(data) 
print(data.shift(fill_value=0))
0    0.5 
1    0.7 
2    2.4 
3    3.2 
dtype: float64 
0    0.0 
1    0.5 
2    0.7 
Resumen del capítulo: Análisis de series temporales
9
3    2.4 
dtype: float64
Las diferencias de las series temporales son más estacionarias que la serie en sí. Por 
ejemplo, una tendencia no lineal se convierte en lineal:

### ES_Resumen_del_captulo_Mtricas_de_negocios.pdf ###
Resumen del capítulo: Métricas de negocios
1
Resumen del capítulo: Métricas 
de negocios
Ingreso, coste de los bienes vendidos y margen
Los ingresos son la cantidad de dinero que los clientes pagaron a la empresa.
El coste de los bienes vendidos es el dinero pagado por la empresa por la compra 
del producto.
Conociendo los ingresos y el coste de los bienes vendidos, se puede calcular el primer 
indicador de negocio: el beneficio bruto. El beneficio bruto es el primer indicador de la 
salud de la empresa. Es bastante fácil de calcular.
La relación entre el beneficio bruto y los ingresos se denomina margen bruto.
Gastos operativos y beneficio operativo
Gastos operativos son todos los gastos relacionados con las operaciones comerciales 
de la empresa.
Si restamos los gastos operativos del beneficio bruto, obtendremos el beneficio 
operativo.
Resumen del capítulo: Métricas de negocios
2
El beneficio operativo ayuda a averiguar cuánto gana una empresa gracias a sus 
actividades comerciales. Salvo contadas excepciones, el beneficio operativo se 
correlaciona con el beneficio neto: cuanto mayor sea el beneficio operativo, mayor será 
el beneficio neto. Además, el beneficio operativo es más rápido y fácil de calcular.
Si el beneficio operativo es negativo, esto se denomina pérdida operativa. Muestra 
que los propietarios de la empresa aún no pueden ganar dinero con su negocio. Pero al 
igual que un beneficio bruto negativo, una pérdida operativa no significa 
necesariamente que todo esté perdido. A menudo la empresa tiene pérdidas 
planificadas porque invierte todos los beneficios en un rápido crecimiento.
Si dividimos el beneficio operativo entre los ingresos, obtenemos el margen de 
beneficio operativo. Se trata de la parte de los ingresos que queda en la empresa tras 
deducir el coste de los bienes vendidos, los salarios, el alquiler, el marketing y otros 
gastos relacionados con las actividades principales. A los inversores les suelen 
interesar los márgenes operativos porque permiten comparar distintas empresas.
Beneficio neto
El indicador de beneficio neto es el que tiene en cuenta el coste de los bienes 
vendidos, los gastos operativos y las obligaciones ante el Estado y los acreedores. Esta 
es la cantidad de dinero que los propietarios pueden tener a su disposición o reinvertir 
en el desarrollo de la empresa.
El beneficio neto solo se puede calcular al final del año cuando se han determinado 
todos los pasivos por impuestos y préstamos. Por lo tanto, los beneficios brutos y 
operativos se utilizan para la gestión diaria de la empresa, mientras que el beneficio 
neto se calcula para la junta anual de accionistas.
Resumen del capítulo: Métricas de negocios
3
Un beneficio neto negativo se denomina pérdida neta. Muestra que la empresa no 
consiguió ganar dinero.
Rentabilidad de la inversión
Los inversionistas quieren saber cuándo las inversiones darán sus frutos. Para ellos, la 
métrica principal es el ROI (rentabilidad de la inversión o return on investment).
Conversión
El proceso de venta puede describirse así: visitante → venta → comprador.
Todo comienza por el hecho de que un usuario se ponga en contacto con la empresa. 
Entonces, hay una venta. Cuanto más eficaz sea la empresa en este sentido, mejor le 
irán las cosas. Por lo tanto, una de las métricas más importantes es la conversión, que 
muestra el porcentaje de usuarios que completaron una acción dirigida.
Embudos
Los embudos representan una forma de mostrar:
1. el camino que sigue el usuario para comprar un producto;
2. la proporción de personas que llegan a la siguiente etapa, es decir, los que "no 
dejan" la compra.
Para construir un embudo, es necesario determinar cuántas personas llegaron a cada 
etapa.
Resumen del capítulo: Métricas de negocios
4
Un gráfico que presenta el número de personas en cada etapa se asemeja a un 
embudo para líquidos:
Al conocer el número de personas en cada etapa, se puede calcular el porcentaje de 
personas que llegaron a un determinado paso, así como el porcentaje de los que dieron 
cada paso posterior:
El análisis de embudo te permite formular hipótesis, ponerlas a prueba y hacer un 
seguimiento de los cambios.
Métricas online y offline
Para responder a esta pregunta, haremos una descomposición de las métricas, o 
sea, dividir las métricas en componentes.
Resumen del capítulo: Métricas de negocios
5
Los ingresos representan la métrica principal. Para calcularla, tenemos que multiplicar 
el número de usuarios por día, su conversión y el recibo promedio, que es la suma de 
los productos añadidos inicialmente por el usuario y de los recomendados por el 
modelo. 
La métrica de evaluación del objetivo es el precio medio de los productos que se 
añaden con base en las recomendaciones. Es la métrica online calculada en un 
sistema en funcionamiento con usuarios reales. No puedes calcularla para otros 
modelos a partir de datos del historial.
Para construir nuevos modelos, también necesitarás métricas offline. Se calculan en 
función de los datos del historial. Ya conoces dos métricas offline, que son las métricas 
exactitud y ECM de machine learning.
El coste de los bienes añadidos con base en las recomendaciones se ve afectado por:
1. ¿Cuántos productos de la lista de recomendaciones son interesantes para el 
usuario? Esto se mide con la métrica precisión.
2. ¿El modelo añade todos los productos interesantes para el usuario a la lista? Se 
mide con la métrica recall.
Resumen del capítulo: Métricas de negocios
6
Recuerda que el valor F1 combina las métricas de precisión y recall.

### Hoja_informativa_Anlisis_de_series_temporales.pdf ###
Hoja informativa: Análisis de series temporales
1
Hoja informativa: Análisis de 
series temporales
Práctica
# reconocimiento de fechas y formación de nuevos índices 
# valores de index_col = la lista de números de columna o nombres de columna 
# valores de parse_dates = la lista de números de columna o nombres de columna 
data = pd.read_csv('filename.csv', index_col=[0], parse_dates=[0])
# comprobar que el índice es monótono 
print(data.index.is_monotonic)
# remuestreo: media para cada hora 
data.resample('1H').mean() 
 
# remuestreo: máximo por cada dos semanas 
data.resample('2W').max()
# media móvil con tamaño de ventana = 7 
data.rolling(7).mean()
# descomposición de la serie temporal en tendencia, estacionalidad y residuos 
from statsmodels.tsa.seasonal import seasonal_decompose 
 
decomposed = seasonal_decompose(data) 
 
decomposed.trend # tendencia 
decomposed.seasonal # estacionalidad 
decomposed.resid # residuos
# cambio de un paso con el llenado del valor cero 
print(data.shift(fill_value=0))
Hoja informativa: Análisis de series temporales
2
Teoría
Las series temporales son las secuencias de números a lo largo del eje del tiempo. El 
intervalo entre los valores de la serie es constante.
Remuestrear significa cambiar el intervalo con los valores de la serie. Se realiza en 
dos pasos:
1. Elige la nueva duración del intervalo. Considera que los valores del intervalo 
existente están agrupados.
2. En cada grupo se calcula el valor acumulado de la serie. Puede ser mediana, 
media, máximo o mínimo.
La media móvil o promedio móvil es un método para suavizar los datos en una serie 
temporal. El método consiste en encontrar los valores menos susceptibles a 
fluctuaciones, es decir, la media aritmética.
Una tendencia es un cambio ligero del valor medio de la serie sin repetir patrones.
Estacionalidad significa patrones que se repiten de forma cíclica en una serie 
temporal.
El proceso estocástico tiene una variación aleatoria y su distribución cambia con el 
tiempo.
Un proceso estocástico es estacionario si su distribución no cambia con el tiempo. Si 
la distribución cambia, entonces el proceso estocástico es no estacionario.
Las diferencias de series temporales son una secuencia de diferencias entre 
elementos vecinos de una serie temporal (es decir, el valor anterior se resta del 
siguiente).

### Hoja_informativa_clasificacin_desequilibrada_ES.pdf ###
Hoja informativa: clasificación desequilibrada
1
Hoja informativa: clasificación 
desequilibrada
Práctica
#entrenamiento de modelo con ajuste de peso de clase 
model = LogisticRegression(class_weight='balanced', random_state=12345)
# concatenación de tablas 
 
pd.concat([table1, table2])
# mezcla de observaciones 
 
features, target = shuffle(features, target, random_state=12345)
# Eliminar observaciones aleatorias de la tabla 
# frac es una fracción de las observaciones de la tabla inicial que devolverá el método 
 
features_sample = features_train.sample(frac=0.1, random_state=12345)
# Calcula las probabilidades de clases 
 
probabilities = model.predict_proba(features)
# itera sobre elementos del rango 
# los pasos primero y último pueden ser números fraccionarios 
 
for value in np.arange(first, last, step):
# trazado de curva ROC 
# Devuelve TFP, TVP y umbrales 
 
from sklearn.metrics import roc_curve 
 
fpr, tpr, thresholds = roc_curve(target, probabilities)
Hoja informativa: clasificación desequilibrada
2
# cálculo de AUR-ROC 
# toma los valores verdaderos de las clases de observación y la predicción de probabilidad de la clase 
 
from sklearn.metrics import roc_auc_score 
auc_roc = roc_auc_score(target_valid, probabilities_one_valid)
Teoría
El sobremuestreo es una técnica de equilibrio de clase que aumenta el número de 
observaciones al duplicar varias veces las observaciones de clase más raras.
El submuestreo es una técnica de equilibrio de clase que disminuye el número de 
observaciones eliminando de forma aleatoria las observaciones de la clase mayoritaria.
Un umbral es el límite de probabilidad que separa a las clases positivas y negativas..
La curva PR es un diagrama que muestra la precisión contra recall.
Tasa de verdaderos positivos, o TVP es el resultado de dividir las respuestas VP entre todas 
las respuestas positivas:
Tasa de falsos positivos, o TFP es el resultado de dividir las respuestas FP entre todas las 
respuestas negativas.
Curva ROC es un gráfico que muestra la tasa de verdaderos positivos frente a la tasa de falsos 
positivos.
AUC-ROC es una métrica de evaluación para tareas de clasificación que equivale al área bajo la 
curva ROC. Los valores de la métrica están en el rango de 0 a 1. El valor AUC-ROC para un 
Hoja informativa: clasificación desequilibrada
3
modelo aleatorio es 0.5.

### Hoja_informativa_Distancia_entre_vectores.pdf ###
Hoja informativa: Distancia entre vectores
1
Hoja informativa: Distancia entre 
vectores
Práctica
# Producto escalar de vectores 
 
import numpy as np 
 
dot_value1 = np.dot(vector1, vector2) 
dot_value2 = vector1 @ vector2
# Distancia euclidiana entre vectores 
 
import numpy as np 
from scipy.spatial import distance 
 
d = distance.euclidean(a, b)
# Distancia Manhattan entre vectores 
 
import numpy as np 
from scipy.spatial import distance 
 
d = distance.cityblock(a, b)
# Índices de elementos mínimos y máximos en la matriz 
 
index = np.array(distances).argmin() # índice de elemento mínimo 
index = np.array(distances).argmax() # índice de elemento máximo
# Creación de clase 
 
class ClassName: 
Hoja informativa: Distancia entre vectores
2
    def fit(self, arg1, arg2, ...): # método de clase 
        # contenido del método
Teoría
Producto escalar es una operación que da como resultado un número (escalar) que 
es igual a la suma de los productos elemento por elemento de los elementos de dos 
vectores.
Distancia euclidiana entre los vectores 𝑎=(𝑥1, 𝑥2, ..., 𝑥𝑛) y 𝑏=(𝑦1, 𝑦2,..., 𝑦𝑛) es la 
suma de los cuadrados de las diferencias de coordenadas:
Distancia Manhattan o distancia entre manzanas es la suma de módulos de 
diferencias de coordenadas de vectores 𝑎=(𝑥1, 𝑥2, ..., 𝑥𝑛) y 𝑏=(𝑦1, 𝑦2,..., 𝑦𝑛):
Clase (class) es un nuevo tipo de datos con sus propios métodos y atributos.

### Hoja_informativa_Pronstico_de_series_temporales_ESP.pdf ###
Hoja informativa: Pronóstico de series temporales
1
Hoja informativa: Pronóstico de 
series temporales
Práctica
# división de datos en conjuntos de entrenamiento y prueba para series temporales (sin barajar)
from sklearn.model_selection import train_test_split 
train, test = train_test_split(data, shuffle=False, test_size=0.2)
# creación de funciones de calendario 
# esta característica contiene años como valores numéricos 
data['year'] = data.index.year 
 
# esta característica contiene días de la semana como valores numéricos 
data['dayofweek'] = data.index.dayofweek
# formación de características de desfase 
 
data['lag_1'] = data['target'].shift(1) 
data['lag_2'] = data['target'].shift(2) 
data['lag_3'] = data['target'].shift(3)
# agregar la característica de media móvil 
 
data['rolling_mean'] = data['target'].rolling(5).mean()
Teoría
El objetivo del pronóstico de series temporales es desarrollar un modelo que prediga 
los valores futuros de una serie temporal con base en datos anteriores.
El periodo en el futuro para el que se prepara el pronóstico se conoce como horizonte de 
pronóstico.

### Hoja_informativa_Recuperacin_de_datos_de_recursos_en_lnea.pdf ###
Hoja informativa: Recuperación de datos de recursos en línea
1
Hoja informativa: Recuperación 
de datos de recursos en línea
Práctica
# Recuperar información de una página web usando una URL 
import requests 
 
req = requests.get(URL) 
print(req.text) # mostrar el contenido de la página 
print(req.status_code) # mostrar código de estado
# Busca una cadena para la primera subcadena que coincida con una expresión regular 
 
import re 
print(re.search(pattern, string).group())
# Divide una cadena en subcadenas por ocurrencias de patrones 
# maxsplit - número máximo de divisiones, maxsplit=0 por defecto 
 
import re 
print(re.split(pattern, string, maxsplit=num_split))
# Encuentra una subcadena y reemplazarla con subcadena repl 
import re 
print(re.sub(pattern, repl, string)) 
# Encuentra todas las subcadenas coincidentes 
 
import re 
print(re.findall(pattern, string))
# Genera una estructura de árbol para una página web 
 
Hoja informativa: Recuperación de datos de recursos en línea
2
from bs4 import BeautifulSoup 
soup = BeautifulSoup(req.text, parser)
# Encuentra la primera etiqueta 'tag' 
# Devuelve una cadena con etiqueta, atributos y contenido 
# attrs - diccionario de atributos tag 
 
tag_content = soup.find(tag, attrs={"attr_name": "attr_value"}) 
print(tag_content.text) # contenido sin etiqueta
# Operaciones con todas las etiquetas tag 
# attrs - diccionario de atributos tag 
 
for tag_content in soup.find_all(tag, attrs={"attr_name": "attr_value"}): 
 # hacer algo
Teoría
Minería web: el proceso de buscar recursos en línea y recuperar datos de ellos
HTML: Lenguaje de marcado de hipertexto, un lenguaje utilizado para crear páginas 
web
HTTP: protocolo de transferencia utilizado para transmitir información en línea
HTTPS: una versión segura de HTTP
Etiqueta HTML: un elemento de lenguaje de marcado de hipertexto
Atributo tag: una función que permite realizar ajustes cuando se trabaja con etiquetas

### Hoja_informativa_Redes_neuronales_convolucionales_esp.pdf ###
Hoja informativa: Redes neuronales convolucionales
1
Hoja informativa: Redes 
neuronales convolucionales
Práctica
# convolución unidimensional 
 
def convolve(sequence, weights): 
    convolution = np.zeros(len(sequence) - len(weights) + 1) 
    for i in range(convolution.shape[0]): 
        convolution[i] = np.sum(weights * sequence[i:i + len(weights)])
# capa convolucional bidimensional en keras 
# filters: el número de filtros que corresponde al tamaño del tensor de output. 
# kernel_size: el tamaño espacial del filtro K. 
# strides: determina cuán lejos se desplaza el filtro sobre la matriz de input (establecid
o en 1 de forma predeterminada.) 
# padding: define el ancho del padding cero. 
# Hay dos tipos de padding: valid (válido) y **same* (igual). 
# El tipo predeterminado de padding es valid y es igual a cero. 
# Same establece el tamaño del padding de forma automática, 
# de modo que el ancho y alto del tensor de output sea igual al ancho y alto del tensor de 
input. 
# activation: esta función se aplica inmediatamente después de la convolución. 
 
keras.layers.Conv2D(filters, kernel_size, strides, padding, activation)
# la capa Flatten hace que el tensor multidimensional sea unidimensional 
 
keras.layers.Flatten()
# Capa Pooling 
# pool_size: tamaño de pooling (agrupación). 
# strides: un paso (o stride) determina cuán lejos se desplaza el filtro sobre la matriz d
e input. Si se especifica *None*, el paso es igual al tamaño de pooling. 
# padding: define el ancho del padding cero. 
 
keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', ...) 
keras.layers.AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid', ...)
Hoja informativa: Redes neuronales convolucionales
2
# generador de datos 
 
from tensorflow.keras.preprocessing.image import ImageDataGenerator 
datagen = ImageDataGenerator() 
 
# extraer datos de una carpeta 
datagen_flow = datagen.flow_from_directory( 
    # carpeta con el conjunto de datos 
    '/dataset/', 
    # tamaño de la imagen objetivo 
    target_size=(150, 150),  
    # tamaño del lote 
    batch_size=16, 
    # modo de clase 
    class_mode='sparse', 
  # indica que este es el generador de datos para el conjunto de entrenamiento (si es ne
cesario) 
    subset='training',  
  # indica que este es el generador de datos para el conjunto de validación (si es neces
ario) 
    subset='validation', 
    # configurar un generador de números aleatorios 
    seed=12345) 
 
# obtener el siguiente lote 
features, target = next(datagen_flow) 
 
# entrenar el modelo mediante generadores de datos 
# utilizando el conjunto de datos completo 
model.fit(datagen_flow, steps_per_epoch=len(datagen_flow)) 
 
# solo con conjuntos de entrenamiento y validación 
model.fit(train_datagen_flow, 
          validation_data=val_datagen_flow, 
          steps_per_epoch=len(train_datagen_flow), 
          validation_steps=len(val_datagen_flow))
# Agregar aumento en generadores de datos 
# horizontal_flip: rotación horizontal 
# vertical_flip: rotación vertical 
 
datagen = ImageDataGenerator(validation_split=0.25, 
                             rescale=1./255, 
                             horizontal_flip=True, 
                             vertical_flip=True)
Hoja informativa: Redes neuronales convolucionales
3
# Implementación de la arquitectura ResNet 
# input_shape: tamaño de la imagen de input 
# classes=1000: el número de neuronas en la última capa totalmente conectada donde sucede
 la clasificación 
# weights='imagenet': la inicialización de los pesos 
# ImageNet: el nombre de una gran base de datos de imágenes que 
# se usaba para entrenar a la red para clasificar imágenes en 1000 clases 
# Escribe weights=None para inicializar los pesos al azar. 
# include_top=True: indica que hay dos capas al final de ResNet: 
# GlobalAveragePooling2D y Dense. Si la configuras en False, estas dos capas no estarán pr
esentes. 
 
from tensorflow.keras.applications.resnet import ResNet50 
 
model = ResNet50(input_shape=None, 
             classes=1000, 
             include_top=True, 
              weights='imagenet')
# Creación de tu propia red basada en ResNet50 
 
backbone = ResNet50(input_shape=(150, 150, 3), 
                    weights='imagenet',  
                    include_top=False) 
 
# congela ResNet50 sin la parte superior (opcional) 
backbone.trainable = False 
 
model = Sequential() 
model.add(backbone) 
model.add(GlobalAveragePooling2D()) 
model.add(Dense(12, activation='softmax'))
Teoría
Convolución: una función que aplica las mismas operaciones a todos los pixeles para 
extraer elementos de imagen que son importantes para la clasificación.
Redes neuronales convolucionales: una clase de redes neuronales que utilizan 
capas convolucionales y hacen la mayor parte del cómputo dentro de la red.
Capa de convolución: una capa donde la operación de convolución se aplica a las 
imágenes de input.
Hoja informativa: Redes neuronales convolucionales
4
Filtro: componente de la capa convolucional, un conjunto de pesos que se aplican a la 
imagen.
Padding: esta configuración coloca ceros en los bordes de la matriz (padding cero) de 
modo que los pixeles más alejados participen en la convolución al menos la misma 
cantidad de veces que los pixeles centrales.
Striding o stride (paso): esta configuración desplaza el filtro en más de un pixel y 
genera una imagen de output más pequeña.
Pooling: compactar un grupo de pixeles en un pixel usando alguna transformación; por 
ejemplo, calculando el máximo o la media aritmética.
Aumento: una técnica que se usa para expandir artificialmente un conjunto de datos al 
transformar las imágenes existentes. Los cambios solo se aplican a los conjuntos de 
entrenamiento, mientras que los de prueba y validación se quedan igual. El aumento 
transforma la imagen original, pero aún preserva sus características principales.

### Hoja_informativa_Redes_totalmente_conectadas_esp.pdf ###
Hoja informativa: Redes totalmente conectadas
1
Hoja informativa: Redes 
totalmente conectadas
Práctica
# Entrenamiento de regresión lineal en Keras 
# importa Keras 
from tensorflow import keras 
 
# crea el modelo 
model = keras.models.Sequential() 
# indica cómo está organizada la red neuronal 
# unidades: el número de neuronas en la capa 
# input_dim: el número de inputs en la capa 
model.add(keras.layers.Dense(units=1, input_dim=features.shape[1])) 
# indica cómo está entrenada la red neuronal 
model.compile(loss='mean_squared_error', optimizer='sgd') 
 
# entrena el modelo 
model.fit(features, target)
# Entrenamiento de regresión logística en Keras 
# importa Keras 
from tensorflow import keras 
 
# crea el modelo 
model = keras.models.Sequential() 
# indica cómo está organizada la red neuronal 
# unidades: el número de neuronas en la capa 
# input_dim: el número de inputs en la capa 
# activación: función de activación 
model.add(keras.layers.Dense(units=1, input_dim=features_train.shape[1],  
              activation='sigmoid')) 
# indica cómo está entrenada la red neuronal 
model.compile(loss='binary_crossentropy', optimizer='sgd') 
 
# entrena el modelo 
model.fit(features, target)
# Entrenamiento de redes neuronales totalmente conectadas en Keras 
# importa Keras 
from tensorflow import keras 
Hoja informativa: Redes totalmente conectadas
2
 
# crea el modelo 
model = keras.models.Sequential() 
# indica cómo está organizada la red neuronal 
# Tenemos dos capas: la primera tiene 10 neuronas, la segunda tiene una 
# unidades: el número de neuronas en la capa 
# input_dim: el número de inputs en la capa 
# activación: función de activación 
model.add(keras.layers.Dense(units=10, input_dim=features_train.shape[1],  
                             activation='sigmoid')) 
model.add(keras.layers.Dense(units=1, activation='sigmoid')) 
 
# indica cómo está entrenada la red neuronal 
model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['acc']) 
 
# entrena el modelo 
model.fit(features, target)
# Trabajar con imágenes en Python 
import numpy as np 
from PIL import Image 
 
# Importar la imagen 
image = Image.open('image.png') 
image_array = np.array(image) 
print(image_array) 
 
# Trazar la imagen 
plt.imshow(image_array) 
 
# Trazar la imagen en blanco y negro 
plt.imshow(image_array, cmap='gray') 
 
# Agregar la barra de color a la imagen 
plt.colorbar()

### Hoja_informativa_SQL_como_herramienta_para_trabajar_con_datos.pdf ###
Hoja informativa: SQL como herramienta para trabajar con datos
1
Hoja informativa: SQL como 
herramienta para trabajar con 
datos
Práctica
-- un comentario de una línea en SQL 
/* un comentario de varias 
líneas 
*/
-- seleccionar columnas de tabla específicas 
SELECT  
 column_1, 
 column_2,  
 column_3 ...   
FROM  
 table_name; 
 
-- seleccionar todas las columnas de la tabla 
 
SELECT  
 *  
FROM  
 table_name;
-- seleccionar datos de tabla con condiciones 
 
SELECT  
 column_1,  
 column_2 -- seleccionar nombres de columna 
FROM  
 table_name -- especificar la tabla 
WHERE  
 condition; -- definir la condición para seleccionar filas
/* 
Selección de filas 
Hoja informativa: SQL como herramienta para trabajar con datos
2
donde el valor field_1 
está entre value_1 y value_2 (incluyente) 
*/ 
 
SELECT  
 *  
FROM  
 table_name 
WHERE  
 field_1 BETWEEN value_1 AND value_2; 
-- Selección de filas basada en valores de campo en una lista 
SELECT  
 *  
FROM  
 table_name 
WHERE  
 column_name IN ('value_1','value_2','value_3');
-- Cálculo del número de filas en la selección 
SELECT  
 COUNT(*) AS cnt, 
 COUNT(column) AS cnt_column, -- devuelve el número de filas en la columna 
 COUNT(DISTINCT column) AS cnt_distinct_column,  
 /* devuelve el número de valores únicos 
 en la columna */ 
 SUM(column) AS sum_column, -- suma de valores de columna 
 AVG(column) AS sum_column, -- valor de columna promedio 
 MIN(column) AS sum_column, -- valor mínimo de columna 
 MAX(column) AS sum_column -- valor máximo de columna 
FROM 
 table;
-- conversión de la columna a otro tipo de datos 
 
CAST (column_name AS data_type)  
column_name :: data_type
Teoría
Una base de datos es un lugar donde se almacenan datos estructurados.
Hoja informativa: SQL como herramienta para trabajar con datos
3
Las entidades son grupos de objetos que comparten características comunes.
Los objetos son instancias individuales de entidades.
Las bases de datos relacionales son bases de datos donde las tablas son entidades y 
las filas de las tablas son sus objetos.
Un DBMS (sistema de gestión de bases de datos) es un conjunto de programas que 
permite crear una base de datos, llenarla con nuevas tablas, mostrar el contenido y 
editar tablas existentes.
Una tabla es un conjunto de filas y columnas.
Un campo es una columna de tabla que contiene las características del objeto. Tiene 
un nombre único y un tipo de datos específico.
Una tupla o registro es una fila de una tabla que contiene información sobre un objeto 
en particular. 
Una celda es una unidad donde se cruzan una fila y una columna.
Una clave principal es un campo o un grupo de campos utilizados para identificar una 
tupla. Todos los valores de clave principal son únicos.
SQL es un lenguaje informático diseñado para gestionar datos en bases de datos 
relacionales.
Una declaración o consulta es una solicitud escrita de acuerdo con la sintaxis SQL. Tu 
instrucción debe especificar qué datos seleccionar y cómo procesarlos.

### moved_DS_11_sprint_Hoja_informativa_Matrices_y_operaciones_matriciales_id.pdf ###
Hoja informativa: Matrices y operaciones matriciales
1
Hoja informativa: Matrices y 
operaciones matriciales
Práctica
# Una matriz a partir de una lista de listas 
 
import numpy as np 
 
matrix = np.array([ 
    [1, 2, 3],  
    [4, 5, 6], 
    [7, 8, 9]]) 
print(matrix)
# Una matriz a partir de una lista de vectores 
 
import numpy as np 
 
string0 = np.array([1,2,3]) 
string1 = np.array([-1,-2,-3]) 
list_of_vectors = [string0, string1] 
matrix_from_vectors = np.array(list_of_vectors) 
 
print(matrix_from_vectors)
# Una matriz a partir de data frames 
 
import pandas as pd 
import numpy as np 
 
matrix = df.values 
print(matrix) 
 
print('Size:', matrix.shape) # dimensiones de la matriz 
print('Row 2:', matrix[2, :]) # el vector es la segunda fila de la matriz 
print('Column 1:', matrix[:, 1]) # el vector es la primera fila de la matriz
# Matriz por multiplicación de vectores 
 
import numpy as np 
     
A = np.array([ 
    [1, 2, 3],  
Hoja informativa: Matrices y operaciones matriciales
2
    [4, 5, 6]]) 
 
b = np.array([7, 8, 9]) 
 
print(np.dot(A, b)) 
print(A.dot(b))
# Matriz por multiplicación de matrices 
 
import numpy as np 
 
print(A.dot(B)) 
print(np.dot(A,B))  
print(A @ B)
# Transposición de matriz 
 
print(matrix.T)
# Creación de una clase 
 
class ClassName: 
    def fit(self, arg1, arg2, ...): # método de clase 
        # contenido del método
Teoría
Una matriz es una tabla numérica rectangular o un arreglo bidimensional que 
consta de m filas y n columnas.
Una transposición de matriz es una operación matricial en la que sus filas se 
convierten en columnas con los mismos números.

### moved_DS_11_sprint_Hoja_informativa_Regresin_lineal_desde_adentro_esp.pdf ###
Hoja informativa: Regresión lineal desde adentro
1
Hoja informativa: Regresión 
lineal desde adentro
Teoría
Un código dummy es un código simple que no significa nada.
Una matriz identidad es una matriz cuadrada con unos en la diagonal principal y 
ceros en el resto.
Una matriz inversa para una matriz cuadrada A es una matriz A con un superíndice 
-1 cuyo producto con A es igual a la matriz identidad.
Las matrices invertibles son matrices que tienen una inversa.
Las no invertibles son matrices que no tienen una inversa.. 

### moved_DS_11_sprint_Hoja_informativa_Vectores_y_operaciones_vectoriales_esp.pdf ###
Hoja informativa: Vectores y operaciones vectoriales
1
Hoja informativa: Vectores y 
operaciones vectoriales
Práctica
# Creación de matriz de NumPy 
 
import numpy as np 
 
numbers1 = [2, 3] # Lista de Python 
vector1 = np.array(numbers1) # Matriz de NumPy 
vector2 = np.array([6, 2])
# Conversión de matriz NumPy en lista 
 
numbers2 = list(vector2) # Lista a partir de vector
# Obtención de matriz NumPy - columna de DataFrame 
 
data[0].values
# Operaciones aritméticas sobre vectores 
 
import numpy as np 
 
sum_of_vectors = vector1 + vector2 # suma de dos vectores 
subtraction_of_vectors = vector2 - vector1 # diferencia de dos vectores 
vector4 = -5 * vector1 # multiplicación de vector por escalar 
array_mult = array1 * array2 # producto elemento por elemento de vectores 
array_div = array1 / array2 # cociente elemento por elemento de vectores 
 
array2_plus_10 = array2 + 10 # agregar un número a cada elemento del vector 
array2_minus_10 = array2 - 10 # restar un número de cada elemento del vector 
array2_div_10 = array2 / 10 # dividir cada elemento del vector entre un número 
 
vector_1_squared = vector_1**2 # exponenciación elemento por elemento
# Vector mínimo y máximo 
 
min(vector) 
max(vector)
Hoja informativa: Vectores y operaciones vectoriales
2
# Exponente vectorial 
 
np.exp(vector)
# Suma y media de elementos vectoriales 
 
vector.sum() 
vector.mean()
Teoría
Vector es un conjunto ordenado de datos numéricos.

### moved_Hoja_informaiva_Mejora_del_modelo.pdf ###
Hoja informaiva: Mejora del modelo
1
Hoja informaiva: Mejora del modelo
Práctica
# División de datos en conjuntos de entrenamiento y validación 
# test_size - la fracción del número total de objetos que tiene que ser dividido como el conjunto de validación (un número de 0 a 1) 
from sklearn.model_selection import train_test_split 
 
df_train, df_valid = train_test_split(df, test_size=0.25, random_state=54321)
# Inicialización del modelo de árbol de decisión 
# n_estimadores - número de árboles 
from sklearn.ensemble import RandomForestClassifier 
 
model = RandomForestClassifier(random_state=54321, n_estimators=3)
# Inicialización del modelo de regresión logística 
from sklearn.linear_model import LogisticRegression 
 
model = LogisticRegression(random_state=54321)
Teoría
Un conjunto de validación es un conjunto de datos que forma parte de un conjunto de datos fuente. Se utiliza para 
comprobar la calidad de un algoritmo durante el entrenamiento de un modelo.
Los parámetros son los ajustes del modelo obtenidos a partir de los datos de entrenamiento que determinan el trabajo del 
modelo.
Los hiperparámetros son los ajustes de los algoritmos de aprendizaje.

### moved_Hoja_informativa_Anlisis_de_algoritmos_DS_Sprint_12_esp.pdf ###
Hoja informativa: Análisis de algoritmos
1
Hoja informativa: Análisis de 
algoritmos
El tiempo de ejecución real es el tiempo de ejecución del algoritmo en una 
computadora determinada
Complejidad computacional es una función que muestra la dependencia de la 
cantidad de trabajo realizado por un algoritmo dado en la cantidad de datos de entrada.
Método iterativo para resolver ecuaciones es un método para calcular una solución 
aproximada realizando iteraciones similares de forma repetida; la solución se vuelve 
más precisa con cada paso.

### moved_Hoja_informativa_Calidad_del_modelo_1.pdf ###
Hoja informativa: Calidad del modelo
1
Hoja informativa: Calidad del 
modelo
Práctica
# Especificar los parámetros del modelo para reproducir un experimento con éxito 
# random_state - número aleatorio 
 
model = DecisionTreeClassifier(random_state=54321)
# Cálculo de la exactitud 
 
from sklearn.metrics import accuracy_score 
 
accuracy = accuracy_score(target, predictions)
# Especificar el parámetro del árbol de decisión para controlar la profundidad máxima del árbol
model = DecisionTreeClassifier(random_state=54321, max_depth=3) 
 
model.fit(features, target)
# guarda el modelo 
# el primer argumento es el modelo 
# el segundo argumento es la ruta al archivo 
 
import joblib 
 
joblib.dump(model, 'model.joblib')
# Sube el modelo entrenado 
 
import joblib 
 
model = joblib.load('model.joblib')
Hoja informativa: Calidad del modelo
2
Teoría
El conjunto de pruebas es un conjunto utilizado para comprobar la calidad de un modelo 
entrenado.
Las métricas de evaluación son las formas de medir la calidad de un modelo de 
machine learning.
La exactitud es la relación entre el número de respuestas correctas y el tamaño del 
conjunto de pruebas.
La precisión es una métrica de evaluación que muestra la relación entre el número de 
observaciones reales con la respuesta "1" y el número de observaciones marcadas como 
"1" por el modelo.
La recall es una métrica de evaluación que refleja qué parte de las observaciones reales 
con respuesta "1" fueron marcadas como tales por el modelo.
La prueba de cordura es el proceso de comparación de nuestro modelo con otro 
aleatorio para evaluar si el modelo tiene sentido.
El sobreajuste es el problema que surge cuando la métrica de evaluación del modelo 
muestra buenos resultados respecto al conjunto de entrenamiento y malos resultados 
respecto al conjunto de prueba.
El subajuste es el problema que ocurre cuando la métrica de evaluación del modelo 
muestra resultados deficientes tanto para el conjunto de entrenamiento como para el 
conjunto de prueba.

### moved_Hoja_informativa_Descenso_de_gradiente_DS_Sprint_12_Esp.pdf ###
Hoja informativa: Descenso de gradiente
1
Hoja informativa: Descenso de 
gradiente
Práctica
import numpy as np 
 
def func(x): 
    # función que se minimizará 
 
def gradient(x): 
    # gradiente de función func 
 
def gradient_descent(initialization, step_size, iterations): 
    x = initialization 
    for i in range(iterations): 
        x = x - step_size * gradient(x) 
    return x
Teoría
El gradiente de una función con valores vectoriales es un vector que consta de 
derivadas de la respuesta para cada argumento que indica la dirección en la que la 
función crece más rápido.
El descenso de gradiente es un algoritmo iterativo para encontrar el mínimo de la 
función de pérdida. Sigue la dirección del gradiente negativo y se aproxima 
gradualmente al mínimo.

### moved_Hoja_informativa_Entrenamiento_del_descenso_de_gradiente_DS_Sprint_12_ESP.pdf ###
Hoja informativa: Entrenamiento del descenso de gradiente
1
Hoja informativa: Entrenamiento 
del descenso de gradiente
Práctica
# Descenso de gradiente estocástico 
 
class SGDLinearRegression: 
    def __init__(self, step_size, epochs, batch_size): 
        self.step_size = step_size 
        self.epochs = epochs 
        self.batch_size = batch_size 
     
    def fit(self, train_features, train_target): 
        X = np.concatenate((np.ones((train_features.shape[0], 1)), train_features), axis=
1)         
        y = train_target 
        w = np.zeros(X.shape[1]) 
         
        for _ in range(self.epochs): 
            batches_count = X.shape[0] // self.batch_size 
            for i in range(batches_count): 
                begin = i * self.batch_size 
                end = (i + 1) * self.batch_size 
                X_batch = X[begin:end, :] 
                y_batch = y[begin:end] 
                 
                gradient = 2 * X_batch.T.dot(X_batch.dot(w) - y_batch) / X_batch.shape[0] 
                 
                w -= self.step_size * gradient 
 
        self.w = w[1:] 
        self.w0 = w[0] 
 
    def predict(self, test_features): 
        return test_features.dot(self.w) + self.w0
# Regresión de cresta 
 
class RidgeRegression: 
    def __init__(self, step_size, epochs, batch_size, reg_weight): 
        self.step_size = step_size 
        self.epochs = epochs 
Hoja informativa: Entrenamiento del descenso de gradiente
2
        self.batch_size = batch_size 
        self.reg_weight = reg_weight 
     
    def fit(self, train_features, train_target): 
        X = np.concatenate((np.ones((train_features.shape[0], 1)), train_features), axis=
1)         
        y = train_target 
        w = np.zeros(X.shape[1]) 
         
        for _ in range(self.epochs): 
            batches_count = X.shape[0] // self.batch_size 
            for i in range(batches_count): 
                begin = i * self.batch_size 
                end = (i + 1) * self.batch_size 
                X_batch = X[begin:end, :] 
                y_batch = y[begin:end] 
                 
                gradient = 2 * X_batch.T.dot(X_batch.dot(w) - y_batch) / X_batch.shape[0] 
                reg = 2 * w.copy() 
                reg[0] = 0 
                gradient += self.reg_weight * reg 
                 
                w -= self.step_size * gradient 
 
        self.w = w[1:] 
        self.w0 = w[0] 
 
    def predict(self, test_features): 
        return test_features.dot(self.w) + self.w0
Teoría
Un lote es una pequeña parte del conjunto de entrenamiento
Se llama al Inicializador de clase cuando se crea una instancia de una clase.
Regularización es un método que agrega limitaciones adicionales a las condiciones 
para reducir el sobreajuste
Una red neuronal es un modelo que consta de muchos modelos simples

### moved_Hoja_informativa_Funciones_avanzadas_de_SQL_para_analistas.pdf ###
Hoja informativa: Funciones avanzadas de SQL para analistas
1
Hoja informativa: Funciones 
avanzadas de SQL para analistas
-- Dividir datos en grupos de acuerdo con los valores de campo 
 
 SELECT  
 field_1,  
 field_2,  
 ...,  
 field_n,  
 AGGREGATE_FUNCTION(field) AS here_you_are 
FROM 
 table_name 
WHERE -- si es necesario 
 condition  
GROUP BY  
 field_1,  
 field_2,  
 ...,  
 field_n
-- Ordenar datos 
 
SELECT  
 field_1,  
 field_2, 
 ...,  
 field_n,  
 AGGREGATE_FUNCTION(field) AS here_you_are 
FROM 
 table_name 
WHERE -- si es necesario 
 condition 
GROUP BY   
 field_1,  
 field_2,  
 ...,  
 field_n, 
ORDER BY -- cuando sea necesario, enlistar solo los campos 
--por los que se ordenarán los datos de la tabla 
 field_1 DESC, -- ordenar los datos en orden descendente 
 field_2 ASC, -- ordenar los datos en orden ascendente 
 ...,  
 field_n, -- ordenar los datos en orden ascendente 
 here_you_are 
Hoja informativa: Funciones avanzadas de SQL para analistas
2
LIMIT -- si es necesario 
 n -- n: el número máximo de filas a devolver
-- Selección por condición con funciones de agregación 
SELECT  
 field_1,  
 field_2,  
 ...,  
 field_n,  
 AGGREGATE_FUNCTION(field) AS here_you_are 
FROM 
 TABLE 
WHERE -- si es necesario 
 condition 
GROUP BY  
 field_1,  
 field_2,  
 ...,  
 field_n 
HAVING 
 AGGREGATE_FUNCTION(field_for_grouping) > n 
ORDER BY -- cuando sea necesario, enlistar solo los campos 
--por los que se ordenarán los datos de la tabla 
 field_1,  
 field_2,  
 ...,  
 field_n, 
 here_you_are 
LIMIT -- si es necesario 
 n 
-- Recuperar fragmentos de fecha 
 
SELECT 
  EXTRACT(date_fragment FROM column) AS new_column_with_date  
FROM  
 Table_with_all_dates
-- Truncar la fecha a un fragmento 
 
SELECT 
  DATE_TRUNC('date_fragment_to_be_truncated_to', column) AS new_column_with_date 
FROM  
 Table_with_all_dates
Hoja informativa: Funciones avanzadas de SQL para analistas
3
-- Subconsultas en bloque FROM 
 
SELECT  
SUBQUERY_1.column_name,  
SUBQUERY_1.column_name_2 
FROM -- para que el código sea legible, inicia las subconsultas en líneas nuevas 
 -- sangra las subconsultas 
 (SELECT  
  column_name, 
  column_name_2  
  FROM  
  table_name 
  WHERE  
  column_name = value) AS SUBQUERY_1;  
-- recuerda nombrar tu subconsulta en el bloque FROM
-- Subconsultas en el bloque WHERE 
 
SELECT  
 column_name,  
 column_name_1 
FROM  
 table_name 
WHERE  
 column_name =  
  (SELECT  
   column_1 
  FROM  
   table_name_2  
  WHERE 
   column_1  = value) 
 
SELECT  
 column_name,  
 column_name_1 
FROM  
 table_name 
WHERE  
 column_name IN   
   (SELECT  
    column_1 
   FROM  
    table_name_2   
   WHERE  
    column_1 = value_1 OR column_1 = value_2)

### moved_Hoja_informativa_Implementacin_de_una_nueva_funcionalidad.pdf ###
Hoja informativa: Implementación de una nueva funcionalidad
1
Hoja informativa: 
Implementación de una nueva 
funcionalidad
Práctica
# Encontrar el intervalo de confianza para la media 
# alpha: nivel de significación; 
# df: número de grados de libertad, = n - 1; 
# loc: distribución media, igual a la estimación media.  
# muestra: sample.mean(); 
# escala: error estándar de distribución, igual a la estimación del error estándar. 
# Cálculo: sample.sem() 
 
from scipy import stats as st 
 
confidence_interval = st.t.interval(alpha = alpha, df=df,  
                  loc=sample.mean(), scale=sample.sem())
# Extracción de submuestra para bootstrap 
 
from numpy.random import RandomState 
state = RandomState(12345) 
 
# sin reemplazo 
print(example_data.sample(frac=1, replace=False, random_state=state)) 
# con reemplazo 
print(example_data.sample(frac=1, replace=True, random_state=state))
# A/B test analysis using bootstrap 
 
import pandas as pd 
import numpy as np 
 
# actual difference between the means in the groups 
AB_difference = samples_B.mean() - samples_A.mean() 
 
alpha = 0.05 
     
state = np.random.RandomState(54321) 
Hoja informativa: Implementación de una nueva funcionalidad
2
 
bootstrap_samples = 1000 
count = 0 
for i in range(bootstrap_samples): 
     # calcula cuántas veces excederá la diferencia entre las medias 
    # el valor actual, siempre que la hipótesis nula sea cierta 
    united_samples = pd.concat([samples_A, samples_B]) 
    subsample = united_samples.sample(frac=1, replace=True, random_state=state) 
     
    subsample_A = subsample[:len(samples_A)] 
    subsample_B = subsample[len(samples_A):] 
    bootstrap_difference = subsample_B.mean() - subsample_A.mean() 
     
    if bootstrap_difference >= AB_difference: 
        count += 1 
 
pvalue = 1. * count / bootstrap_samples 
print('p-value =', pvalue) 
 
if pvalue < alpha: 
    print("La hipótesis nula se rechaza, a saber, es probable que el importe promedio de l
as compras aumente") 
else: 
    print("La hipótesis nula no se rechaza, a saber, es poco probable que el importe medio 
de las compras aumente")
Teoría
Las pruebas A/B o split testing son una técnica de comprobación de hipótesis que 
ayuda a controlar el impacto que provocan los cambios de un servicio o producto sobre 
los usuarios. La técnica implica lo siguiente: la población se divide en el grupo de 
control que usa el servicio regular sin cambios y el grupo experimental que usa la 
nueva versión, la que necesitamos probar. 
Peeking problem: el resultado general se distorsiona cuando se agregan nuevos datos 
al comienzo del experimento.
Error de tipo I: se produce cuando la hipótesis nula es correcta, pero se rechaza 
(resultado falso positivo. En este caso, la nueva funcionalidad se aprueba y, por lo 
tanto, es positiva)
Error de tipo II: se produce cuando la hipótesis nula es incorrecta, pero se acepta 
(resultado falso negativo)
Hoja informativa: Implementación de una nueva funcionalidad
3
Un intervalo de confianza representa un segmento del eje numérico dentro del que 
cae el parámetro poblacional de interés, con una probabilidad predeterminada.

### moved_Hoja_informativa_mtricas_clasificacin.pdf ###
Hoja informativa: métricas de clasificación
1
Hoja informativa: métricas de 
clasificación
Práctica
# Cálculo de la matriz de confusión  
 
from sklearn.metrics import confusion_matrix 
 
print(confusion_matrix(target, predictions))
# Cálculo del valor de recall 
 
from sklearn.metrics import recall_score 
 
print(recall_score(target, predictions))
# Cálculo del valor de precisión 
 
from sklearn.metrics import precision_score 
 
print(precision_score(target, predictions))
# Cálculo del valor F1 
from sklearn.metrics import f1_score 
 
print(f1_score(target, predictions))
Teoría
Respuestas verdaderas positivas (VP): el modelo etiquetó una observación como 
"1", y su valor real también es "1”
Respuestas verdaderas negativas (VN): el modelo etiquetó una observación como 
"0", y su valor real también es "0”
Hoja informativa: métricas de clasificación
2
Respuestas falsas positivas (FP): el modelo etiquetó una observación como "1", pero 
su valor real es "0”
Respuestas falsas negativas (FN): el modelo etiquetó una observación como "0", 
pero su valor real es "1”
Recall es una métrica de evaluación que mide la proporción de respuestas VP entre 
todas las respuestas que realmente tienen la etiqueta "1". Para calcularla, usa esta 
fórmula:
La precisión es una métrica de evaluación que mide la proporción de observaciones 
que se etiquetaron como clase positiva y en realidad eran de la clase positiva. Para 
calcularla, usa esta fórmula:
El valor F1 es una métrica de evaluación que es la media armónica de las métricas de 
recall y precisión. Para calcularla, usa esta fórmula:
Recall = V P + FN
V P
Precisi n =
oˊ
V P + FP
V P
F1 = Precisi n + Recall
oˊ
2 ⋅Precisi n ⋅Recall
oˊ

### moved_Hoja_informativa_mtricas_de_regresin.pdf ###
Hoja informativa: métricas de regresión
1
Hoja informativa: métricas de 
regresión
Práctica
# Para calcular la métrica R2 
 
from sklearn.metrics import r2_score 
 
print("R2 =", r2_score(target, predicted))
# Para calcular la métrica EAM  
 
from sklearn.metrics import mean_absolute_error 
 
mae = mean_absolute_error(target, predicted))
Teoría
El coeficiente de determinación o la métrica R2 (R-al cuadrado) divide el ECM del 
modelo entre el ECM de la media y luego resta el valor obtenido de uno.
La fórmula es
El error absoluto medio (EAM) es una métrica de evaluación de regresión. Para 
calcularlo, usa:
Hoja informativa: métricas de regresión
2
Las notaciones convencionales para Data Science
 es el valor objetivo para la observación con número de serie i en la muestra
 es el valor de predicción para la observación con número de serie i
 es el error de observación
 es el error absoluto de observación
 es el número de observaciones en la muestra
 es la suma de todas las observaciones de la muestra (i varía en el rango de 1 a 
N)
MAE =
∣y −
N
1
i=1
∑
N
i
∣
y^i
yi
y^i
y −
i
y^i
∣y −
i
∣
y^i
N
∑i=1
N

### moved_Hoja_informativa_Pasar_a_la_regresin.pdf ###
Hoja informativa: Pasar a la regresión
1
Hoja informativa: Pasar a la regresión
Práctica
# Calcular el promedio de los errores cuadráticos (ECM) y la raíz cuadrada del promedio de los errores cuadráticos (RECM) 
 
from sklearn.metrics import mean_squared_error  
 
mse = mean_squared_error(answers, predictions) 
rmse = mse ** 0.5
# Inicialización del modelo de árbol de decisión para la regresión 
 
from sklearn.tree import DecisionTreeRegressor 
 
model = DecisionTreeRegressor(random_state=54321)
# Inicialización del modelo de bosque aleatorio para la regresión 
 
from sklearn.ensemble import RandomForestRegressor 
 
model = RandomForestRegressor(random_state=54321, n_estimators=3)
# Inicialización del modelo de regresión lineal 
 
from sklearn.linear_model import LinearRegression 
 
model = LinearRegression()

### moved_Hoja_informativa_Potenciacin_del_gradiente_DS_Sprint12_ESP.pdf ###
Hoja informativa: Potenciación del gradiente
1
Hoja informativa: Potenciación 
del gradiente
Práctica
# Entrenamiento del clasificador CatBoost 
# cat_features - lista de características categóricas 
from catboost import CatBoostClassifier 
 
model = CatBoostClassifier(loss_function="Logloss", iterations=50) 
model.fit(features_train, target_train, cat_features=cat_features, verbose=10)
Teoría
Un ensamble es un conjunto de modelos para resolver el mismo problema. La 
fortaleza de los ensambles es que el error medio de un grupo de modelos es menos 
significativo que sus errores individuales.
La potenciación es un enfoque para la construcción de ensambles donde cada modelo 
posterior considera los errores del anterior y, en la predicción final, los pronósticos de 
los alumnos básicos.

### moved_Hoja_informativa_preparacin_de_caractersticas_Takeaway_sheet_Feature_Preparation_esp.pdf ###
Hoja informativa: preparación de características
1
Hoja informativa: preparación de 
características
Práctica
# Codificación one-hot: obtén características dummy 
# drop_first = True - quitar la primera columna (evitar trampas dummy) 
 
pd.get_dummies(df['column']) 
pd.get_dummies(df['column'], drop_first_True)
# Codificación ordinal 
 
from sklearn.preprocessing import OrdinalEncoder 
 
encoder = OrdinalEncoder() 
encoder.fit(data) 
data_ordinal = encoder.transform(data) 
 
# agregar nombres de columna 
 
data_ordinal = pd.DataFrame(encoder.transform(data), columns=data.columns) 
 
# entrenamiento y transformación automatizados 
from sklearn.preprocessing import OrdinalEncoder 
 
encoder = OrdinalEncoder() 
data_ordinal = pd.DataFrame(encoder.fit_transform(data), columns=data.columns) 
# Estandarización 
 
from sklearn.preprocessing import StandardScaler 
 
scaler = StandardScaler() 
scaler.fit(df) 
df_scaled = scaler.transform(df)
Hoja informativa: preparación de características
2

### moved_Hoja_informativa_Primer_modelo_entrenado.pdf ###
Hoja informativa: Primer modelo entrenado
1
Hoja informativa: Primer modelo 
entrenado
Práctica
# Obtener la clase para la clasificación del árbol de decisión 
 
from sklearn.tree import DecisionTreeClassifier 
 
model = DecisionTreeClassifier()
# Entrenamiento del modelo 
 
model.fit(features, target)
# Hacer de predicciones para nuevos datos 
 
answer = model.predict(new_features)
Teoría
El conjunto de datos de entrenamiento es un conjunto de datos que utilizamos para 
entrenar nuestro algoritmo de machine learning.
Las observaciones son instancias (filas) en un conjunto de datos.
Las características son variables (columnas) en un conjunto de datos.
El objetivo es la característica que queremos predecir.
El aprendizaje supervisado es una tarea que consiste en entrenar un modelo 
mediante un conjunto de datos de entrenamiento (con un valor objetivo conocido) para 
poder predecir el objetivo para datos desconocidos.
La clasificación es un tipo de aprendizaje supervisado con un objetivo categórico.
Hoja informativa: Primer modelo entrenado
2
La clasificación binaria (o binomial) es una clasificación cuyo objetivo puede estar en 
una de dos clases.
La regresión es un tipo de aprendizaje supervisado con un objetivo numérico.

### moved_Hoja_informativa_Relaciones_entre_tablas.pdf ###
Hoja informativa: Relaciones entre tablas
1
Hoja informativa: Relaciones 
entre tablas
Práctica
-- Selección de filas de tabla con valores vacíos 
 
SELECT  
 *  
FROM  
 table_name 
WHERE  
 column_name IS NULL;
-- Exclusión de filas con valores vacíos de la selección 
 
SELECT  
 *  
FROM  
 table_name 
WHERE  
 column_name IS NOT NULL;
-- Acciones determinadas por condiciones 
 
CASE 
    WHEN condition_1 THEN result_1 
    WHEN condition_2 THEN result_2 
    WHEN condition_3 THEN result_3 
    ELSE result_4 
END;
-- Una condición de búsqueda con una expresión regular 
 
column_name LIKE 'regular expression'
-- Inner join 
 
SELECT --enlistar solo los campos necesarios 
 TABLE_1.field_1 AS field_1, 
 TABLE_1.field_2 AS field_2, 
Hoja informativa: Relaciones entre tablas
2
 ... 
 TABLE_2.field_n AS field_n 
FROM 
 TABLE_1 
INNER JOIN TABLE_2 ON TABLE_2.field_1 = TABLE_1.field_2;
-- Left outer join 
 
SELECT  
  TABLE_1.field_1 AS field_1, 
 TABLE_1.field_2 AS field_2, 
 ... 
 TABLE_2.field_n AS field_n 
FROM 
 TABLE_1 
LEFT JOIN TABLE_2 ON TABLE_2.field = TABLE_1.field;
-- Right outer join 
 
SELECT  
 TABLE_1.field_1 AS field_1, 
 TABLE_1.field_2 AS field_2, 
 ... 
 TABLE_2.field_n AS field_n 
FROM 
 TABLE_1 
RIGHT JOIN TABLE_2 ON TABLE_1.field = TABLE_2.field;
-- Unión de varias tablas 
 
SELECT --enlistar solo los campos necesarios 
 TABLE_1.field_1 AS field_1, 
 TABLE_1.field_2 AS field_2, 
 ... 
 TABLE_3.field_n AS field_n 
FROM 
 TABLE_1 
INNER JOIN TABLE_2 ON TABLE_2.field = TABLE_1.field 
INNER JOIN TABLE_3 ON TABLE_3.field = TABLE_1.field
-- Consultas de unión 
 
SELECT  
 column_name_1   
FROM  
 table_1 
UNION --( o UNION ALL) 
SELECT  
Hoja informativa: Relaciones entre tablas
3
 column_name_1   
FROM  
 table_2
Teoría
Una clave foránea es una columna de tabla que contiene valores de campo de otra 
tabla.
En una relación una a una, una fila en una tabla está conectada con una y solo una 
fila en otra tabla.
En una relación de una a muchas, cada una de las filas de una tabla coincide con 
varias (o muchas) filas de otra tabla.
En una relación de muchas a muchas, varias filas de una tabla coinciden con 
varias filas de otra tabla.
Los diagramas ER son diagramas especiales que ilustran la estructura de una base 
de datos, incluidas las tablas y las relaciones entre ellas.

### moved_Resumen_del_captulo_Calidad_del_modelo.pdf ###
Resumen del capítulo: Calidad del modelo
1
Resumen del capítulo: Calidad 
del modelo
Aleatoriedad en algoritmos de aprendizaje
El hecho de aplicar un carácter aleatorio a los algoritmos de aprendizaje refuerza en 
gran medida la capacidad del modelo para descubrir relaciones en los datos. El 
barajado introduce aleatoriedad en el algoritmo de aprendizaje. Una computadora no 
puede generar números verdaderamente aleatorios. Utiliza generadores de números 
pseudoaleatorios que crean secuencias que parecen aleatorias.
Los números aleatorios no son tan simples. Son imprevisibles. Los generadores de 
números pseudoaleatorios pueden configurarse de manera que sus resultados sean 
siempre los mismos. Todo lo que tienes que hacer para añadir pseudoaleatoriedad al 
crear un algoritmo de aprendizaje es especificar el **parámetro random_state :
# especifica un estado aleatorio (número) 
model = DecisionTreeClassifier(random_state=54321) 
 
# entrena al modelo de la misma manera que antes 
model.fit(features, target)
Si estableces random_state  en None  (que es su valor por defecto), la 
pseudoaleatoriedad será siempre diferente.
Dataset de prueba
Para probar si nuestro modelo hace predicciones precisas incluso a la hora de 
enfrentarse a nuevos datos, vamos a utilizar un nuevo conjunto de datos. Ese será el 
conjunto de datos de prueba.
Exactitud
La relación entre el número de respuestas correctas y el número total de preguntas (es 
decir, el tamaño del conjunto de datos de prueba) se denomina exactitud (accuracy).
Resumen del capítulo: Calidad del modelo
2
Para calcular la exactitud, utiliza esta fórmula:
Métricas de evaluación
Las métricas de evaluación se utilizan para medir la calidad de un modelo y se 
pueden expresar numéricamente. Ya has encontrado una de estas métricas: exactitud. 
Hay otras, por ejemplo:
Precisión (precision) toma todos los apartamentos que el modelo consideró caros 
(están marcados como "1") y calcula qué fracción de ellos era realmente costosa. 
Los apartamentos no reconocidos por el modelo se ignoran.
Recall (sensibilidad) toma todos los apartamentos que son realmente caros y 
calcula qué fracción de ellos reconoció el modelo. Los apartamentos que fueron 
reconocidos por el modelo por error se ignoran.
Siempre asegúrate de que tu modelo funcione mejor que la casualidad, es decir, 
realiza una prueba de cordura.
Métricas de evaluación en Scikit-Learn
Puedes encontrar las funciones métricas de la librería scikit-learn en el módulo 
metrics. Utiliza la función accuracy_score() para calcular la exactitud.
from sklearn.metrics import accuracy_score
Resumen del capítulo: Calidad del modelo
3
La función toma dos argumentos (respuestas correctas y predicciones del modelo) y 
devuelve el valor de exactitud.
accuracy = accuracy_score(target, predictions)
Subajuste y sobreajuste
El modelo no tuvo problemas con los ejemplos del conjunto de entrenamiento, pero se 
atascó en el conjunto de pruebas. Esto representa un síntoma de sobreajuste. El 
efecto contrario se llama subajuste. Ocurre cuando la exactitud es baja y 
aproximadamente igual tanto para el conjunto de entrenamiento como para el de 
prueba.
No siempre es posible evitar el sobreajuste o el subajuste. Cuando mejoras en uno de 
estos, aumenta el riesgo del otro.
Revisa este ejemplo de excelente ajuste de un algoritmo de entrenamiento. ¿Cómo 
afecta al equilibrio entre el sobreajuste y el subajuste? La profundidad del árbol 
(altura) es la cantidad máxima de condiciones desde la "parte superior" del árbol hasta 
la respuesta final, según el número de transiciones de nodo a nodo.
La profundidad del árbol en sklearn puede establecerse mediante el parámetro 
max_depth :
Resumen del capítulo: Calidad del modelo
4
# especifica la profundidad (ilimitado por defecto) 
model = DecisionTreeClassifier(random_state=54321, max_depth=3) 
 
model.fit(features, target)
Experimentos con el árbol de decisión
Para guardar el modelo entrenado en el formato correcto, utiliza la función dump de 
esta librería.
# guarda el modelo 
# el primer argumento es el modelo 
# el segundo argumento es la ruta al archivo 
 
from joblib import dump 
 
joblib.dump(model, 'model.joblib')
Puedes abrir y ejecutar el modelo usando la función load.
import joblib 
 
# un argumento es la ruta al archivo 
# un valor de retorno es el modelo 
model = joblib.load('model.joblib')

### moved_Resumen_del_captulo_clasificacin_desequilibrad.pdf ###
Resumen del capítulo: clasificación desequilibrada
1
Resumen del capítulo: 
clasificación desequilibrada
Ajuste de peso de clase
Los algoritmos de aprendizaje automático consideran que todas las observaciones del 
conjunto de entrenamiento tienen la misma ponderación de forma predeterminada. Si 
necesitamos indicar que algunas observaciones son más importantes, asignamos un 
peso a la clase respectiva.
La regresión logística, el árbol de decisión y el bosque aleatorio en la librería sklearn 
tienen un argumento class_weight. De forma predeterminada, es None , es decir, las 
clases son equivalentes:
clase "0" peso = 1.0
clase "1" peso = 1.0
Si especificamos class_weight='balanced , el algoritmo calculará cuántas veces la clase 
"0" ocurre con más frecuencia que la clase "1". Denotaremos este número como N (un 
número desconocido de veces). Los nuevos pesos de clase se ven así:
clase "0" peso = 1.0
clase "1" peso = N
La clase rara tendrá un mayor peso.
Sobremuestreo
Cuando entrenamos modelos, las clases se pueden equilibrar aumentando el tamaño 
de la muestra. La técnica se llama sobremuestreo.
El sobremuestreo se realiza en varios pasos:
Divide la muestra de entrenamiento por clase.
Determina la clase con menos observaciones. Llámala la clase rara.
Duplica las observaciones de clase más raras varias veces.
Crea una nueva muestra de entrenamiento basada en los datos obtenidos.
Mezcla los datos
Resumen del capítulo: clasificación desequilibrada
2
Se pueden copiar observaciones varias veces usando la sintaxis de multiplicación de 
listas de Python. Para repetir los elementos de la lista, la lista se multiplica por un 
número (el número requerido de repeticiones):
answers = [0, 1, 0] 
print(answers) 
answers_x3 = answers * 3 
print(answers_x3)
[0, 1, 0] 
[0, 1, 0, 0, 1, 0, 0, 1, 0]
Utiliza la función pd.concat() para concatenar las tablas. Esta toma una lista de tablas 
como entrada. Los datos de origen se pueden obtener de la siguiente manera:
pd.concat([table1, table2])
Para mezclar las observaciones de forma aleatoria, usa el método shuffle de la librería 
sklearn.utils:
features, target = shuffle(features, target, random_state=54321)
La función upsample():
def upsample(features, target, repeat): 
    features_zeros = features[target == 0] 
    features_ones = features[target == 1] 
    target_zeros = target[target == 0] 
    target_ones = target[target == 1] 
 
    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat) 
    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat) 
     
    features_upsampled, target_upsampled = shuffle( 
        features_upsampled, target_upsampled, random_state=54321) 
     
    return features_upsampled, target_upsampled
Resumen del capítulo: clasificación desequilibrada
3
Submuestreo
En lugar de repetir las preguntas importantes, también podemos eliminar una parte de 
las que no son importantes. Para ello, podemos utilizar la técnica de submuestreo.
El submuestreo se realiza en varios pasos:
Divide la muestra de entrenamiento por clase
Determina la clase con más observaciones. Llámala la clase mayoritaria
Elimina de forma aleatoria una parte de las observaciones de la clase mayoritaria
Crea una nueva muestra de entrenamiento basada en los datos obtenidos
Mezcla los datos
Para descartar de forma aleatoria algunos de los elementos de la tabla, usa la función 
sample(). Toma frac (de fracción) y devuelve elementos aleatorios en cantidades tales 
que su fracción en la tabla inicial sea igual a frac.
features_sample = features_train.sample(frac=0.1, random_state=54321)
La función downsample():
def downsample(features, target, fraction): 
    features_zeros = features[target == 0] 
    features_ones = features[target == 1] 
    target_zeros = target[target == 0] 
    target_ones = target[target == 1] 
 
    features_downsampled = pd.concat( 
        [features_zeros.sample(frac=fraction, random_state=54321)] + [features_ones]) 
    target_downsampled = pd.concat( 
        [target_zeros.sample(frac=fraction, random_state=54321)] + [target_ones]) 
     
    features_downsampled, target_downsampled = shuffle( 
        features_downsampled, target_downsampled, random_state=54321) 
     
    return features_downsampled, target_downsampled
Resumen del capítulo: clasificación desequilibrada
4
Umbral de clasificación
Para determinar la respuesta, la regresión logística calcula la probabilidad de las 
clases. Solo tenemos dos clases (cero y uno). La probabilidad de la clase "1" será 
suficiente para nosotros. El valor está en el rango de cero a uno: si es mayor a 0.5, la 
observación es positiva; de lo contrario, es negativa.
La línea donde termina la clase negativa y comienza la clase positiva se llama umbral. 
Por defecto, es 0.5, pero podemos cambiarlo.
Ajuste de umbral
En sklearn, la probabilidad de clase se puede calcular con la función predict_proba(). 
Lo que hace es tomar características de las observaciones y devolver las 
probabilidades:
probabilities = model.predict_proba(features)
Las cadenas corresponden a las observaciones. La primera columna indica la 
probabilidad de clase negativa y la segunda indica la probabilidad de clase positiva (las 
dos probabilidades suman la unidad).
Para crear un bucle con el rango deseado, usamos la función arange() de la librería 
numpy. Al igual que la función range(), esta itera sobre los elementos especificados del 
rango, pero es diferente porque funciona con números fraccionarios, además de 
enteros:
for value in np.arange(first, last, step):
Curva PR
Tracemos los valores de las métricas y veamos cómo responde la curva al cambio de 
umbral.
En la gráfica, el valor de precisión se traza verticalmente y recall, horizontalmente. Una 
curva trazada a partir de los valores de Precisión y Recall se denomina curva PR. 
Resumen del capítulo: clasificación desequilibrada
5
Cuanto más alta sea la curva, mejor será el modelo.
TVP y TFP
No puedes calcular la precisión cuando no hay observaciones positivas. Consideremos 
las métricas que no implican división entre cero.
Antes de pasar a la nueva curva, definamos algunos términos importantes.
Tasa de verdaderos positivos (True Positive Rate), o TVP, es el resultado de dividir las 
respuestas VP entre todas las respuestas positivas. Aquí está la fórmula, donde P es 
todas las respuestas positivas:
V P
Resumen del capítulo: clasificación desequilibrada
6
La tasa de falsos positivos, o TFP, es el resultado de dividir las respuestas FP entre 
todas las respuestas negativas. Se calcula utilizando una fórmula similar, donde N son 
todas las respuestas negativas:
No habrá división entre cero: los denominadores son valores constantes que no 
dependen de cambios en el modelo.
Curva ROC
Hemos visto un nuevo enfrentamiento: TVP vs. TFP. Tracemos la curva.
Colocamos los valores de la tasa de falsos positivos (TFP) a lo largo del eje horizontal y 
los valores de la tasa de verdaderos positivos (TVP) a lo largo del eje vertical. Luego 
iteramos los valores del umbral de regresión logística y trazamos una curva. Se 
denomina curva ROC (Receiver Operating Characteristic, un término de la teoría del 
procesamiento de señales).
Para un modelo que siempre responde de forma aleatoria, la curva ROC es una línea 
diagonal que va desde la esquina inferior izquierda hasta la esquina superior derecha. 
Cuanto más alta sea la curva, mayor será el valor de TVP y mejor será la calidad del 
modelo.
TV P = P
V P
TFP = N
FP
Resumen del capítulo: clasificación desequilibrada
7
Para encontrar cuánto difiere nuestro modelo del modelo aleatorio, calculemos el valor 
AUC-ROC (Area Under Curve ROC o área bajo la curva ROC). Esta es una nueva 
métrica de evaluación con valores en el rango de 0 a 1. El valor AUC-ROC para un 
modelo aleatorio es 0.5.
Podemos trazar una curva ROC con la variable roc_curve() del módulo sklearn.metrics:
from sklearn.metrics import roc_curve
Esta toma los valores objetivo y las probabilidades de clase positivas, supera diferentes 
umbrales y devuelve tres listas: valores de TFP, valores de TVP, y los umbrales que 
superó.
 fpr, tpr, thresholds = roc_curve(target, probabilities)
Para calcular AUC-ROC, usa la función roc_auc_score() de la librería sklearn:
Resumen del capítulo: clasificación desequilibrada
8
from sklearn.metrics import roc_auc_score
A diferencia de otras métricas, esta toma probabilidades de clase "1" en lugar de 
predicciones:
auc_roc = roc_auc_score(target_valid, probabilities_one_valid)

### moved_Resumen_del_captulo_Implementacin_de_nuevas_funciones.pdf ###
Resumen del capítulo: Implementación de nuevas funciones
1
Resumen del capítulo: Implementación 
de nuevas funciones
Planificación de la implementación
Las pruebas A/B o split testing son una técnica de comprobación de hipótesis que ayuda a controlar el 
impacto que provocan los cambios de un servicio o producto sobre los usuarios. Se realiza de la siguiente 
manera: la población se divide en grupo de control (A) y grupo experimental (B). El grupo A utiliza el servicio 
habitual sin cambios. El grupo B utiliza la nueva versión, que es la que tenemos que probar.
El experimento dura un periodo determinado (por ejemplo, dos semanas). El objetivo es recopilar datos 
sobre el comportamiento de los visitantes en ambos grupos. Si la métrica clave en el grupo experimental 
mejora en comparación con el grupo de control, entonces se implementará la nueva funcionalidad.
Resumen del capítulo: Implementación de nuevas funciones
2
Antes de las pruebas A/B, a menudo se utiliza la prueba A/A, o comprobación de validez, en la que los 
visitantes se dividen en dos grupos de control que se exponen a la misma versión del servicio. La métrica 
clave debe coincidir en ambos grupos, en caso contrario, hay que buscar un error.
Duración de las pruebas A/B
La implementación de una nueva funcionalidad cambia inevitablemente el comportamiento de los usuarios. 
Por lo general, los usuarios necesitan tiempo para acostumbrarse a los cambios. Una vez que se hayan 
acostumbrado por completo, podremos evaluar con certeza si el experimento ha sido un éxito. Cuantos más 
datos tengamos, menor será la probabilidad de error al comprobar las hipótesis estadísticas.
Resumen del capítulo: Implementación de nuevas funciones
3
Las pruebas A/B sufren el llamado peeking problem (es decir, el problema de vislumbrar los resultados), 
que consiste en que el resultado final se distorsiona cuando se añaden nuevos datos al principio del 
experimento. Incluso un pequeño fragmento de datos nuevos es importante en relación con los datos ya 
acumulados y la significación estadística se alcanza en poco tiempo.
Esta es una de las manifestaciones de la ley de los grandes números. La dispersión tiende a ser mayor 
cuando el número de observaciones es escaso. En cambio, cuando tenemos un gran número de 
observaciones, el impacto de los valores atípicos se reduce. Entonces, si la muestra es demasiado pequeña, 
las diferencias son fáciles de ver. Para una prueba estadística, es una disminución del valor p hasta los 
valores lo suficientemente pequeños como para rechazar la hipótesis nula de que no hay diferencia.
Para solucionar el peeking problem, el tamaño de la muestra debe establecerse antes del inicio de la prueba.
Este es el procedimiento correcto de las pruebas A/B:
Y así es como no debes realizar una prueba A/B:
Resumen del capítulo: Implementación de nuevas funciones
4
La forma más fácil de calcular el tamaño de la muestra es utilizar una calculadora en línea como esta: 
https://vwo.com/tools/ab-test-duration-calculator/
Comparación de las medias
Analicemos los resultados de las pruebas A/B: el valor de la métrica describe el comportamiento de todos los 
usuarios.
Los resultados de las mediciones y los valores medios contienen un elemento aleatorio. Por lo tanto, tienen 
un componente de error aleatorio. No podemos predecir el valor exacto de cada observación con exactitud 
absoluta, pero podemos estimarlo utilizando métodos estadísticos.
Supongamos que nuestra hipótesis nula H₀ dice: la nueva funcionalidad no mejora las métricas. Entonces 
nuestra hipótesis correspondiente H₁ será: la nueva funcionalidad mejora las métricas. 
En la fase de comprobación de la hipótesis, son posibles dos tipos de errores:
1. Error de tipo I: se produce cuando la hipótesis nula es correcta, pero se rechaza (resultado falso 
positivo. En este caso, la nueva funcionalidad se aprueba y, por lo tanto, es positiva)
2. Error de tipo II: se produce cuando la hipótesis nula es incorrecta, pero se acepta (resultado falso 
negativo)
Resumen del capítulo: Implementación de nuevas funciones
5
Para aceptar o rechazar la hipótesis nula, calculemos el nivel de significación, también conocido como valor 
p (valor de probabilidad). Muestra la probabilidad del error de tipo I, pero no revela nada sobre el error de 
tipo II.
Ten en cuenta que si el valor p es mayor que el valor de umbral, la hipótesis nula no debería rechazarse. Si 
es menor que el umbral, puede que no valga la pena aceptar la hipótesis nula. Los umbrales generalmente 
aceptados son del 5 % y del 1 %. Pero solo el data scientist toma la decisión final sobre qué umbral podría 
considerarse suficiente.
Los valores medios se comparan utilizando los métodos de prueba de hipótesis unilateral. La hipótesis 
unilateral se acepta si el valor que se está comprobando es mucho mayor o mucho menor que el de la 
hipótesis nula. A nosotros nos interesa la desviación en una sola dirección, que es "mayor que".
Si la distribución de los datos se aproxima a la normalidad (no hay valores atípicos significativos en los 
datos), se utiliza la prueba estándar para comparar las medias. Este método supone una distribución normal 
de las medias de todas las muestras y determina si la diferencia entre los valores comparados es lo 
suficientemente grande como para rechazar la hipótesis nula.
Intervalo de confianza
Un intervalo de confianza representa un segmento del eje numérico dentro del que cae el parámetro 
poblacional de interés, con una probabilidad predeterminada. El parámetro es desconocido, pero podemos 
estimarlo a partir de la muestra. Si el valor cae dentro del rango de 300 a 500 con una probabilidad del 99 %, 
entonces el intervalo de confianza del 99 % para este valor es (300, 500).
Al calcular el intervalo de confianza, normalmente descartamos la misma cantidad de valores de cada uno de 
sus extremos.
El intervalo de confianza no es solo un rango de valores aleatorios. El valor que evaluamos no es aleatorio 
debido a su diseño. La causa de la variabilidad radica en el hecho de que el número es desconocido y que 
se calcula a partir de la muestra. El carácter aleatorio de la muestra introduce aleatoriedad en la estimación. 
El intervalo de confianza mide la confianza en dicha estimación.
Resumen del capítulo: Implementación de nuevas funciones
6
Cálculo del intervalo de confianza
Podemos construir un intervalo de confianza para la media basado en la muestra utilizando el teorema del 
límite central.
Supongamos que tomamos nuestra muestra a partir de una distribución con los siguientes parámetros:
Denota la media de la muestra:
μ = media poblacional
σ =
2
varianza poblacional
Resumen del capítulo: Implementación de nuevas funciones
7
El teorema del límite central dice que todas las medias de todas las muestras posibles con un tamaño n se 
distribuyen normalmente alrededor de la verdadera media poblacional. "Alrededor" significa que la media de 
esta distribución de todas las medias muestrales será igual a la verdadera media poblacional. La varianza 
será igual a la varianza poblacional dividida entre n (el tamaño de la muestra).
La desviación estándar de esta distribución se denomina error estándar (error estándar de la media, o SEM, 
abreviado del "standard error of mean"):
Cuanto mayor sea el tamaño de la muestra, menor será el error estándar, es decir, todas las medias 
muestrales estarán más cerca de la media real. Cuanto mayor sea la muestra, más precisa será la 
estimación.
Vamos a estandarizar esta distribución normal:
A partir de la distribución normal estándar, toma el percentil de 5 % F(0.05) y el percentil de 95 % F(0.95) 
para obtener el intervalo de confianza del 90 %:
Volvamos a escribirlo todo:
¡Aquí lo tenemos! El intervalo de confianza del 90 % para la media real.
Solo nos queda un problema. Para calcular el error estándar, utilizamos la varianza poblacional, pero la 
desconocemos al igual que la media poblacional. La estimamos a partir de la muestra.
Este hecho también afecta a la distribución de las medias muestrales de modo que, si la varianza es 
desconocida, no podemos utilizar la distribución normal y tenemos que describirla con la distribución de 
Student. Al poner en la fórmula el percentil de 5 % t(0.05) y el percentil de 95 % t(0.95), obtenemos:
Es posible simplificar el cálculo utilizando la distribución de Student scipy.stats.t. Tiene una función para el 
intervalo de confianza, interval(), que toma:
=
Xˉ
media de la muestra
∼
Xˉ
N
μ,
(
n
σ2
)
SEM(
) =
Xˉ
n
σ
∼
SEM(
)
Xˉ
−μ
Xˉ
N(0,1 )
2
P
F(0.05) <
< F(0.95)
=
(
SEM(
)
Xˉ
−μ
Xˉ
)
90%
P
−F(0.05) ⋅SEM(
) < μ <
+ F(0.95) ⋅SEM(
) =
(Xˉ
Xˉ
Xˉ
Xˉ )
90%
P
−t(0.05) ⋅SEM(
) < μ <
+ t(0.95) ⋅SEM(
) =
(Xˉ
Xˉ
Xˉ
Xˉ )
90%
Resumen del capítulo: Implementación de nuevas funciones
8
alpha: nivel de significación
df: número de grados de libertad (igual a n - 1)
loc (de localización): la distribución media igual a la estimación media. Para la *muestra*, se calcula del 
modo siguiente: sample.mean() .
scale: el error estándar de la distribución igual a la estimación del error estándar. Se calcula de la 
siguiente manera: sample.sem() .
import pandas as pd 
from scipy import stats as st 
 
confidence_interval = st.t.interval(alpha, len(sample)-1,  
                  loc=sample.mean(), scale=sample.sem())
Bootstrap
Los valores complejos se pueden calcular con la ayuda de la técnica del bootstrapping.
Para conseguir un valor deseado, por ejemplo, la media, podemos obtener las submuestras 
(pseudomuestras) del conjunto fuente de datos. A continuación, calcularemos la media de cada una de ellas. 
En teoría, podemos formar submuestras y calcular el valor deseado a partir de ellas muchas veces. De este 
modo, podemos obtener varios valores para el parámetro de interés y estimar la distribución. 
Resumen del capítulo: Implementación de nuevas funciones
9
El bootstrapping es aplicable a cualquier muestra. Es útil cuando:
Las observaciones no pueden ser descritas mediante una distribución normal;
No hay pruebas estadísticas para el valor objetivo.
De hecho, no siempre se puede confiar en la distribución normal.
Resumen del capítulo: Implementación de nuevas funciones
10
Bootstrap para el intervalo de confianza
Vamos a averiguar cómo formar submuestras para el bootstrapping. Ya conoces la función sample(). Para 
esta tarea necesitamos llamarla en un bucle. Pero aquí nos encontramos con un problema:
for i in range(5): 
    # extrae un elemento aleatorio de la muestra 1 
    # especifica random_state para su reproducción 
    print(data.sample(1, random_state=54321)) 
Como especificamos el random_state , el elemento aleatorio es siempre el mismo. Para solucionarlo, crea una 
instancia RandomState()  del módulo  numpy.random :
from numpy.random import RandomState 
state = RandomState(54321) 
Esta instancia se puede pasar al argumento random_state  de cualquier función. Es importante que, con cada 
nueva llamada, su estado cambie a aleatorio. Así obtendremos diferentes submuestras:
for i in range(5): 
    # extrae un elemento aleatorio de la muestra 1 
    print(data.sample(1, random_state=state)) 
Otro detalle importante a la hora de crear submuestras consiste en que deben proporcionar una selección de 
elementos con reemplazo. Es decir, el mismo elemento puede caer en una submuestra varias veces. Para 
ello, especifica replace=True  para la función sample() . Compara:
example_data = pd.Series([1, 2, 3, 4, 5]) 
print("Sin reemplazo") 
print(example_data.sample(frac=1, replace=False, random_state=state)) 
print("Con reemplazo") 
print(example_data.sample(frac=1, replace=True, random_state=state)) 
Bootstrap para análisis de prueba A/B
El bootstrapping también se utiliza para analizar los resultados de las pruebas A/B.
Mientras se realizaba la prueba, acumulamos datos sobre el parámetro objetivo en el grupo de control y en el 
grupo de tratamiento. Calculamos la diferencia real de los parámetros objetivo entre los grupos. Luego 
formulamos y probamos las hipótesis. La hipótesis nula es que no hay diferencia entre los parámetros 
objetivo de ambos grupos. La hipótesis alternativa es que, en el grupo experimental, el valor del parámetro 
objetivo es mayor. Encontremos el valor p.
Ahora vamos a investigar cuál es la probabilidad de que dicha diferencia se haya obtenido por casualidad 
(este será nuestro valor p). Concatena las muestras y usa bootstrap para obtener la distribución del monto 
promedio de compra.
Resumen del capítulo: Implementación de nuevas funciones
11
Crea muchas submuestras y divide cada submuestra en dos con el índice i:
Encuentra la diferencia del monto promedio de compra entre ellas:
Evaleemos en bootstrap la proporción de diferencias en el monto promedio de compra que resultaron ser no 
menos que las diferencias en el monto promedio de compra entre las muestras originales:
import pandas as pd 
import numpy as np 
 
# diferencia real entre las medias de los grupos 
AB_difference = samples_B.mean() - samples_A.mean() 
 
alpha = 0.05 
     
state = np.random.RandomState(54321) 
 
bootstrap_samples = 1000 
count = 0 
for i in range(bootstrap_samples): 
    # calcula cuántas veces excederá la diferencia entre las medias  
    # el valor actual, siempre que la hipótesis nula sea cierta 
    united_samples = pd.concat([samples_A, samples_B]) 
    subsample = united_samples.sample(frac=1, replace=True, random_state=state) 
     
    subsample_A = subsample[:len(samples_A)] 
    subsample_B = subsample[len(samples_A):] 
    bootstrap_difference = subsample_B.mean() - subsample_A.mean() 
Resumen del capítulo: Implementación de nuevas funciones
12
     
    if bootstrap_difference >= AB_difference: 
        count += 1 
 
pvalue = 1. * count / bootstrap_samples 
print('p-value =', pvalue) 
 
if pvalue < alpha: 
    print("La hipótesis nula se rechaza, a saber, es probable que el importe promedio de las compras aumente") 
else: 
    print("La hipótesis nula no se rechaza, a saber, es poco probable que el importe medio de las compras aumente")
Bootstrap para modelos
Bootstrap se puede utilizar para evaluar los intervalos de confianza en los modelos ML.

### moved_Resumen_del_captulo_Mejora_del_modelo.pdf ###
Resumen del capítulo: Mejora del modelo
1
Resumen del capítulo: Mejora del 
modelo
Datasets de validación
Para que el control de calidad sea fiable, necesitamos un dataset de validación.
El dataset de validación se separa del dataset fuente antes de que se entrene el 
modelo. De otro modo, el modelo sabría todas las respuestas antes de aprender del 
conjunto de entrenamiento. La validación muestra cómo se comporta el modelo en el 
campo y ayuda a revelar si hay sobreajuste.
La parte de los datos que se va a asignar al conjunto de validación depende del 
número de observaciones y características, así como de la variación de los datos. 
Estos son los dos escenarios más comunes:
1) El conjunto de prueba existe (o existirá en el futuro cercano), pero no está disponible 
por el momento. La proporción ideal es de 3:1. Esto significa que un 75 % es para el 
conjunto de entrenamiento y un 25 % es para el conjunto de validación. Este escenario 
se usará en nuestra plataforma de entrenamiento en línea.
2) El conjunto de prueba no existe. En ese caso, los datos fuente deben dividirse en 
tres partes: entrenamiento, validación y prueba. Usualmente, el tamaño del conjunto de 
validación y del conjunto de prueba son iguales. Este escenario nos da una proporción 
de 3:1:1 (un 60 % para el conjunto de entrenamiento y un 20 % para los conjuntos de 
validación y prueba).
División de los datos en dos conjuntos:
Para este propósito, Scikit-learn tiene una función especial llamada train_test_split(). 
Esta función puede dividir cualquier conjunto de datos en dos, y se denomina así 
porque suele utilizarse para dividir los conjuntos en conjuntos de entrenamiento y de 
prueba. Pero en nuestro caso, vamos a recurrir a esta función a fin de obtener un 
conjunto de entrenamiento y otro de validación.
from sklearn.model_selection import train_test_split
Resumen del capítulo: Mejora del modelo
2
Antes de dividir, necesitamos establecer dos parámetros:
Nombre del dataset que vamos a dividir.
Tamaño del conjunto de validación ( test_size ). El tamaño se expresa con un 
decimal entre 0 y 1 que representa una fracción del dataset fuente. En este caso, 
tenemos test_size=0.25  porque queremos trabajar con el 25 % del conjunto fuente.
La función train_test_split()  devuelve dos conjuntos de datos, el de entrenamiento y el 
de validación.
df_train, df_valid = train_test_split(df, test_size=0.25, random_state=54321)
Nota: podemos asignar cualquier valor a random_state  excepto None .
Hiperparámetros
El modelo adquiere todos los parámetros del conjunto de entrenamiento. Además de 
los parámetros de modelo regulares, tenemos hiperparámetros. Estos son 
configuraciones para algoritmos de aprendizaje. Es necesario especificarlos antes del 
entrenamiento.
Por ejemplo, en el árbol de decisión, los hiperparámetros son:
— max_depth: que es la profundidad máxima del árbol.
— criterion: que es el criterio de división.
— min_samples_split: que prohíbe crear nodos que no contengan suficientes 
observaciones del conjunto de entrenamiento.
— min_samples_leaf: las hojas son los nodos más bajos que contienen las 
respuestas y que dejan de dividir los datos. Este hiperparámetro evita que el algoritmo 
añada nodos hoja que no tengan suficientes observaciones del conjunto de 
entrenamiento.
Nuevos modelos: bosque aleatorio
Resumen del capítulo: Mejora del modelo
3
Probemos con un nuevo algoritmo de aprendizaje llamado bosque aleatorio. Este 
algoritmo entrena una gran cantidad de árboles independientes y toma una decisión 
mediante el voto. Un bosque aleatorio ayuda a mejorar los resultados y a evitar el 
sobreajuste.
En la librería scikit-learn, puedes encontrar RandomForestClassifier que es un 
algoritmo de bosque aleatorio. Impórtalo desde el módulo ensemble:
from sklearn.ensemble import RandomForestClassifier
Para determinar el número de árboles en el bosque, usaremos el hiperparámetro 
n_estimators  (número de estimadores). La calidad del resultado final y la duración del 
entrenamiento son estrictamente proporcionales al número de árboles.
model = RandomForestClassifier(random_state=54321, n_estimators=3)
Regresión logística
Si incrementamos el valor del **hiperparámetro n_estimators , el modelo se vuelve 
voluminoso y el proceso de entrenamiento se hace lento, lo que no es bueno. Pero si 
mantenemos el número de árboles bajo, los resultados no mejorarán. Así que eso 
también es malo.
Probemos con la regresión logística. Aunque el nombre sugiere un problema de 
regresión, sigue siendo un algoritmo de clasificación.
Para predecir la clase de un apartamento, la regresión logística hace lo siguiente:
En primer lugar, decide a qué clase se acerca la observación.
En función de la respuesta, elige la clase: si el resultado del cálculo es positivo, 
entonces será "1" (precios altos), mientras que si es negativo, será "0" (precios 
bajos).
Hay solo unos cuantos parámetros en la regresión logística. El modelo no será capaz 
de memorizar ninguna de las características de la fórmula, por lo que la probabilidad de 
sobreajuste será baja.
Resumen del capítulo: Mejora del modelo
4
El modelo LogisticRegression se encuentra en el módulo sklearn.linear_model de la 
librería sklearn. Impórtalo:
from sklearn.linear_model import LogisticRegression 
 
model = LogisticRegression(random_state=54321)

### moved_Resumen_del_captulo_Pasar_a_la_regresin.pdf ###
Resumen del capítulo: Pasar a la regresión
1
Resumen del capítulo: Pasar a la 
regresión
Error cuadrático medio
La métrica de evaluación más comúnmente usada para las tareas de regresión es el 
error cuadrático medio o ECM.
Para encontrar el ECM, primero debes calcular el error de cada observación:
Calcula el ECM usando esta fórmula.
Vamos a analizar estos cálculos:
1. El error de observación muestra el grado de discrepancia entre la respuesta correcta 
y la predicción. Si el error es mucho más grande que cero, el modelo ha sobrevalorado 
el apartamento; si es mucho menor que cero, entonces el modelo lo ha infravalorado.
Resumen del capítulo: Pasar a la regresión
2
2. No tendría sentido sumar los errores tal y como están, ya que los positivos anularían 
los negativos. Para hacer que todos cuenten, necesitamos deshacernos de los signos 
elevando al cuadrado cada uno de ellos.
3. Encontramos el promedio para obtener datos para todas las observaciones.
El ECM debe ser lo más bajo posible.
Cálculo de ECM
Para calcular el error medio cuadrático, importa la función mean_squared_error()  del 
módulo sklearn.metrics .
from sklearn.metrics import mean_squared_error 
 
mse = mean_squared_error(answers, predictions)
Como resultado, obtendrás unidades cuadradas (por ejemplo, "dólares cuadrados"). 
Para conseguir una métrica de evaluación en las unidades regulares, encuentra la raíz 
cuadrada de ECM. Entonces, obtendrás RMSE (raíz del error cuadrático medio):
rmse = mse ** 0.5
Regresión del árbol de decisión
Para tareas de regresión, los árboles de decisión se entrenan de manera similar a la 
clasificación, pero no predicen una clase, sino un número.
El árbol de decisión para tareas de regresión se llama DecisionTreeRegressor  y está en el 
módulo sklearn.tree .
from sklearn.tree import DecisionTreeRegressor 
 
model = DecisionTreeRegressor(random_state=54321)
Regresión lineal
Resumen del capítulo: Pasar a la regresión
3
La regresión lineal es similar a la regresión logística en varios aspectos. Su nombre 
proviene del álgebra lineal. La regresión lineal es menos propensa al sobreajuste 
porque no tiene muchos parámetros.
from sklearn.linear_model import LinearRegression 
 
model = LinearRegression()

### moved_Resumen_del_captulo_preparacin_de_caractersticas.pdf ###
Resumen del capítulo: preparación de características
1
Resumen del capítulo: 
preparación de características
Recapitulemos. La técnica OHE nos permite transformar características categóricas en 
características numéricas en dos pasos:
Codificación One-Hot
Existe una técnica especial para transformar características categóricas en 
características numéricas. Se llama codificación one-hot (OHE).
1. Agrega una columna separada para cada valor de característica.
2. Si la categoría se ajusta a la observación, se asigna 1; de lo contrario, se asigna 0.
Las nuevas funciones ( Gender_F , Gender_M , Gender_None ) se denominan variables 
dummy.
Puedes obtener variables dummy para OHE usando la función get_dummies(). Se 
puede encontrar en la librería pandas.
pd.get_dummies(df['column'])
Trampa dummy
Cuando los datos son abundantes, tenemos la posibilidad de caer en la trampa de las 
características dummy. Hemos agregado tres columnas nuevas a nuestra tabla, pero su 
alta correlación confundirá a nuestro modelo. Para evitar esto, podemos eliminar con 
seguridad cualquier columna, ya que sus valores se pueden deducir fácilmente de las 
otras dos columnas (tiene 1 donde las otras dos columnas tienen ceros y tiene ceros en 
el resto). De esta manera, no caeremos en la trampa dummy.
Para eliminar la columna, llama a la función pd.get_dummies()  junto con el parámetro 
drop_first. Si pasas drop_first=True  entonces se elimina la primera columna. De lo 
contrario, es drop_first=False  por defecto y no se descartan columnas.
Resumen del capítulo: preparación de características
2
pd.get_dummies(df['column'], drop_first=teTrue)
Codificación ordinal
Necesitamos una nueva técnica de codificación que permita codificar categorías 
textuales con números. Vamos a usar codificación ordinal. Funciona de la siguiente 
manera:
1. Codifica cada clase con un número.
2. Los números se ponen en las columnas.
Sklearn proporciona una clase para dicha codificación. Se llama OrdinalEncoder. 
Puedes encontrarla en el módulo sklearn.preprocessing .
Importa OrdinalEncoder  de la librería:
from sklearn.preprocessing import OrdinalEncoder
La transformación se realiza en tres pasos:
1. Crea una instancia de esta clase..
encoder = OrdinalEncoder()
2. Llama al método fit()  para obtener la lista de características categóricas, el mismo 
proceso que hacemos cuando entrenamos un modelo. Pásale los datos.
encoder.fit(data)
3. Utiliza el método transform() . Los datos transformados se almacenarán en la 
variable data_ordinal .
data_ordinal = encoder.transform(data)
Resumen del capítulo: preparación de características
3
Use DataFrame()  para agregar nombres de columna:
data_ordinal = pd.DataFrame(encoder.transform(data),  
               columns=data.columns)
Si necesitamos transformar los datos solo una vez, como en nuestro caso, también 
puedes llamar al método fit_transform() . Este combina fit()  y transform() .
data_ordinal = pd.DataFrame(encoder.fit_transform(data),  
               columns=data.columns)
Escalado de características
Si tenemos datos con características numéricas que tienen diferente dispersión de 
valores, el algoritmo puede encontrar que las características con mayores magnitudes y 
dispersión son más importantes. Este problema se puede solucionar con el escalado de 
características.
Una de las formas de escalar las características es estandarizar los datos.
Supón que todas las características se distribuyen normalmente, la media (M) y la 
varianza (Var) se determinan a partir de la muestra. Los valores de las características 
se convierten mediante esta fórmula:
Para la nueva característica, la media se convierte en 0 y la varianza es igual a 1.
Hay una clase de sklearn dedicada a la estandarización de datos que se llama 
StandardScaler. It's in the sklearn.preprocessing module.
from sklearn.preprocessing import StandardScaler
Resumen del capítulo: preparación de características
4
Crea una instancia de la clase y ajústala usando los datos de entrenamiento. El 
proceso de ajuste implica calcular la media y la varianza:
scaler = StandardScaler() 
scaler.fit(df)
Transforma el conjunto de entrenamiento y el conjunto de validación usando 
transform().
df_scaled = scaler.transform(df)

### moved_Resumen_del_captulo_Primer_modelo_entrenado.pdf ###
Resumen del capítulo: Primer modelo entrenado
1
Resumen del capítulo: Primer 
modelo entrenado
Dataset para el entrenamiento
En realidad, programas y especialistas se capacitan de manera similar: recopilan y 
clasifican conocimientos, descubren dependencias y adquieren experiencia. Tanto el 
proceso de aprendizaje de un humano como el machine learning incluyen cierto 
material de estudio. En el caso del machine learning, los modelos aprenden de 
conjuntos de datos de entrenamiento.
En el análisis de datos, las filas se llaman instancias, mientras que las columnas son 
las variables. En el machine learning, las filas y columnas representan observaciones 
y características, respectivamente. La característica que necesitamos predecir se 
llama objetivo.
Aprendizaje supervisado
Tienes un conjunto de datos de entrenamiento y una característica objetivo (precio de 
venta de la propiedad) que necesitas predecir usando el resto de las características. 
Esta es una tarea de aprendizaje supervisado. El "maestro" plantea preguntas 
(características) y da respuestas (el objetivo). No se da ninguna explicación sobre 
cómo las características conducen a la respuesta exactamente; la máquina tiene que 
resolverlo por sí misma. El aprendizaje supervisado resulta conveniente para resolver 
múltiples tareas comerciales.
También hay otras clases:
aprendizaje no supervisado: sin objetivo
aprendizaje semisupervisado: solo una parte de los datos de entrenamiento 
conoce el objetivo
Recomendación: los usuarios y los elementos reemplazan las funciones y las 
observaciones (algo que puedas recomendar, por ejemplo, películas o vecindarios).
Resumen del capítulo: Primer modelo entrenado
2
Veamos los tipos de aprendizaje supervisado.
Todas las variables y características son categóricas o numéricas, y el objetivo no es 
una excepción.
Las tareas de clasificación se ocupan de objetivos categóricos (por ejemplo, determinar 
especies de animales en una imagen). Cuando solo tenemos dos categorías (por 
ejemplo, si un cliente volverá a visitar el sitio web o no), se le denomina clasificación 
binaria.
Si el objetivo es numérico, entonces es una tarea de regresión. Los datos se utilizan 
para encontrar relaciones entre las variables y hacer predicciones basadas en la 
información, como el pronóstico del tiempo o la predicción de los precios del mercado 
de valores para los próximos días.
Modelos y algoritmos
Supongamos que existe algún tipo de relación entre las características y el objetivo. 
Para hacer predicciones, la máquina tiene que entender cuál es esa relación. Pero no 
es posible tener en cuenta todas las posibles razones por las que el objetivo haya 
salido tal y como es. Por ello, tenemos que simplificar esta compleja relación y recurrir 
a los modelos de machine learning.
Existen muchos modelos diferentes que pueden utilizarse para reflejar cómo las 
características se transforman en el objetivo, y cada uno de ellos conlleva sus propias 
suposiciones sobre cómo se estructura dicha relación. Quien trabaja en la ciencia de 
datos acepta estas suposiciones eligiendo un modelo, para luego utilizarlo con el fin de 
hacer predicciones. Si estas predicciones coinciden con la realidad, significa que las 
suposiciones eran lo suficientemente precisas y que el modelo elegido era el correcto. 
Este enfoque se denomina modelado.
Un modelo popular se llama árbol de decisiones. Este puede describir el proceso de 
toma de decisiones en casi cualquier situación. Así es como hacemos un árbol de 
decisiones con respuestas sí/no y diferentes escenarios.
Resumen del capítulo: Primer modelo entrenado
3
Cada árbol sale diferente. Vamos a entrenar el modelo para construir el más 
adecuado. Además del conjunto de datos, necesitaremos un algoritmo de 
aprendizaje. El conjunto de datos se procesa a través de nuestro algoritmo de 
aprendizaje, produciendo un modelo entrenado.
Una vez entrenado, el modelo está listo para hacer predicciones, es decir, para tomar 
nuevas características de entrada y generar respuestas (el objetivo), sin necesidad de 
Resumen del capítulo: Primer modelo entrenado
4
recurrir a algoritmos ni a conjuntos de datos de entrenamiento.
Es importante recordar que el proceso de machine learning consta de dos pasos: 
entrenamiento del modelo y aplicación del mismo.
Librería Scikit-learn
Resumen del capítulo: Primer modelo entrenado
5
Los algoritmos de aprendizaje suelen ser más complejos que los modelos. Así que, por 
ahora, considéralos como cajas negras y no pienses demasiado en lo que está 
pasando dentro. Céntrate mejor en lo que hay que poner ahí y en lo que hay que hacer 
con el resultado.
Las librerías de Python ofrecen muchos algoritmos. En esta lección trabajaremos con la 
librería popular scikit-learn, o sklearn (scientific kit for learning). Sklearn es una gran 
fuente de herramientas para trabajar con datos y modelos. Para mayor comodidad, la 
librería está dividida en módulos. Los árboles de decisión se almacenan en el módulo 
tree.
Cada modelo corresponde a una clase separada en sklearn. DecisionTreeClassifier 
es una clase para clasificaciones de árboles de decisión. Vamos a importarla de la 
librería:
from sklearn.tree import DecisionTreeClassifier
Luego creamos una instancia de la clase:
model = DecisionTreeClassifier()
Ahora la variable model  almacena nuestro modelo, y tenemos que ejecutar un algoritmo 
de aprendizaje para entrenar al modelo para que haga predicciones.
Para iniciar el entrenamiento, llama al método fit() y pásale nuestras variables como 
argumento.
model.fit(features, target)
Ahora tenemos un modelo entrenado en la variable model . Para predecir respuestas, 
llama al método predict() y pásale la tabla con las características de las nuevas 
observaciones.
answer = model.predict(new_features)

### Resumen_del_capitulo_metricas_de_regresion.pdf ###
Resumen del capítulo: métricas de regresión
1
Resumen del capítulo: métricas 
de regresión
Coeficiente de determinación
El coeficiente de determinación o la métrica R2 (R-al cuadrado) divide el ECM del 
modelo entre el ECM de la media y luego resta el valor obtenido de uno. Si la métrica 
aumenta, la calidad del modelo también mejora.
R2 se calcula de la siguiente manera:
R2 es igual a uno solo si el valor ECM es cero. Dicho modelo predeciría 
perfectamente todas las respuestas.
R2 es cero: el modelo funciona tan bien como la media.
Cuando R2 es negativo, la calidad del modelo es muy baja.
R2 no puede tener valores mayores a uno.
Para calcular esta métrica, puedes usar la función r2_score() de la librería 
sklearn.metrics:
from sklearn.metrics import r2_score 
 
print("R2 =", r2_score(target, predicted))
Error absoluto medio
Necesitamos establecer la notación convencional para Data Science:
Resumen del capítulo: métricas de regresión
2
El valor objetivo para la observación con número de serie i en la muestra utilizada 
para medir la calidad. El subíndice indica el número de serie de la observación.
El valor de predicción para la observación con número de serie i (en la muestra de 
prueba, por ejemplo).
EAM (error absoluto medio) es otra métrica de evaluación. Es algo similar al ECM, pero 
no está al cuadrado. Vamos a escribirlo usando las notaciones convencionales para 
Data Science.
El error de una observación se escribe de la siguiente manera:
Para cancelar la diferencia entre subajuste y sobreajuste, se calcula el error absoluto.
Para recopilar los errores a lo largo de la muestra, agreguemos la siguiente notación:
Resumen del capítulo: métricas de regresión
3
El número de observaciones en la muestra.
La suma de todas las observaciones de la muestra (i varía en el rango de 1 a N).
La fórmula para EAM, o error absoluto medio, es:
Para calcular esta métrica, puedes usar la función mean_absolute_error() de la librería 
sklearn.metrics:
from sklearn.metrics import mean_absolute_error 
 
mae = mean_absolute_error(target, predicted))
Para calcular el valor ECM, usamos el valor de la media como una constante.
El modelo constante debe seleccionarse de manera que te permita obtener el valor de 
métrica EAM más bajo posible. Necesitamos encontrar el valor a, en el que se alcanza 
el mínimo:
Resumen del capítulo: métricas de regresión
4
El mínimo se alcanza cuando a es igual a la mediana del objetivo.
A diferencia del EAM, la métrica RECM es más sensible a valores grandes: los errores 
significativos afectan fuertemente el valor final de RECM. De esa forma, puedes 
cambiar el valor de una métrica sin cambiar otra.

### Resumen_del_capítulo_Entrenamiento_del_descenso.pdf ###
Resumen del capítulo: Entrenamiento del descenso de gradiente
1
Resumen del capítulo: 
Entrenamiento del descenso de 
gradiente
Descenso de gradiente para regresión lineal
Recuerda la tarea de entrenamiento de regresión lineal:
Vamos a entrenar un modelo usando descenso de gradiente. Pero primero, 
escribe la función de pérdida en forma de vector para encontrar su gradiente. 
Expresa el ECM como un producto escalar de la diferencia de vectores:
donde ﻿ es el vector de respuesta correcta y ﻿ es el vector de predicción.
Convierte el producto escalar en un producto matricial:
Después de transponer el vector, es decir, convertirlo de una columna a una fila, 
podemos multiplicarlo por otro vector.
Combina las fórmulas de ECM y regresión lineal:
w =
MSE(Xw,y)
w
arg min
MSE(y,a) =
​(a
​ −
i=1
∑
n
i
y
​) =
i
2
​(a −
n
1
y,a −y)
y
a
(a −y,a −y) = (a −y) (a −
⊤
y)
MSE(Xw,y) =
​(Xw −
n
1
y) (Xw −
⊤
y)
Resumen del capítulo: Entrenamiento del descenso de gradiente
2
Encuentra la función gradiente para el vector de parámetros 
﻿. Los gradientes de 
las funciones con valores vectoriales se calculan de manera similar a las 
derivadas. Por ejemplo, cuando se trabaja con números, la derivada parcial de 
﻿ para el vector de parámetros 
﻿ es igual a 
﻿. Cuando se 
trabaja con vectores, solo permanece el factor de 
﻿ del primer paréntesis 
:
Descenso de gradiente estocástico
Podemos calcular el gradiente usando pequeñas partes del conjunto de 
entrenamiento. Estas piezas se conocen como minilotes o lotes. Para que el 
algoritmo "vea" todo el conjunto de entrenamiento, deben cambiarse sus lotes en 
cada iteración de forma aleatoria. Aquí necesitamos el descenso de gradiente 
estocástico en minilotes o descenso de gradiente estocástico, DGE.
Para obtener lotes, necesitamos barajar todos los datos del conjunto de 
entrenamiento y dividirlo en partes. Un lote debe contener un promedio de 100-
200 observaciones (el tamaño del lote). Cuando el algoritmo DGE ha pasado por 
todos los lotes una vez, significa que una época ha terminado. El número de 
épocas depende del tamaño del conjunto de entrenamiento. Pueden ser una o dos 
si el conjunto es pequeño o varias docenas si el conjunto es grande. El número de 
lotes es igual al número de iteraciones para completar una época.
Así es como funciona el algoritmo DGE:
1. Hiperparámetros de entrada: tamaño del lote, número de épocas y tamaño del 
paso.
2. Define los valores iniciales de los pesos del modelo.
3. Divide el conjunto de entrenamiento en lotes para cada época.
4. Para cada lote:
4.1. Calcula el gradiente de la función de pérdida
w
(xw −y)2
w
2x(xw −y)
w
(X )
T
∇MSE(Xw,y) =
​X (Xw −
n
2
⊤
y)
Resumen del capítulo: Entrenamiento del descenso de gradiente
3
4.2. Actualiza los pesos del modelo (agrega el gradiente negativo multiplicado 
por el tamaño del paso a los pesos actuales)
5. El algoritmo devuelve los pesos finales del modelo.
Encontremos la complejidad computacional del DGE con las siguientes 
definiciones:
﻿: el número de observaciones en todo el conjunto de entrenamiento
﻿: el tamaño del lote
﻿: el número de funciones
DGE en Python
Vamos a aprender a pasar hiperparámetros a un modelo. Necesitamos declarar la 
clase del modelo y crear el método "inicializador de clase" ( __init__ ):
class SGDLinearRegression:
    def __init__(self):
        ...
Agrega un hiperparámetro step_size  al inicializador de clase:
class SGDLinearRegression:
    def __init__(self, step_size):
        ...
Ahora podemos pasar el tamaño del paso al modelo al crear una clase:
# puedes elegir el tamaño del paso de forma arbitraria
model = SGDLinearRegression(0.01)
Guarda el tamaño del paso como un atributo:
n
b
p
Resumen del capítulo: Entrenamiento del descenso de gradiente
4
class SGDLinearRegression:
    def __init__(self, step_size):
        self.step_size = step_size
Aquí está la implementación completa del algoritmo DGE:
class SGDLinearRegression:
    def __init__(self, step_size, epochs, batch_size):
        self.step_size = step_size
        self.epochs = epochs
        self.batch_size = batch_size
    
    def fit(self, train_features, train_target):
        X = np.concatenate((np.ones((train_features.shape[0], 
1)), train_features), axis=1)        
        y = train_target
        w = np.zeros(X.shape[1])
        
        for _ in range(self.epochs):
            batches_count = X.shape[0] // self.batch_size
            for i in range(batches_count):
                begin = i * self.batch_size
                end = (i + 1) * self.batch_size
                X_batch = X[begin:end, :]
                y_batch = y[begin:end]
                
                gradient = 2 * X_batch.T.dot(X_batch.dot(w) - 
y_batch) / X_batch.shape[0]
                
                w -= self.step_size * gradient
        self.w = w[1:]
        self.w0 = w[0]
Resumen del capítulo: Entrenamiento del descenso de gradiente
5
    def predict(self, test_features):
        return test_features.dot(self.w) + self.w0
Regularización de regresión lineal
Vamos a modificar la función de pérdida para eliminar el sobreajuste. Para reducir 
el sobreajuste, podemos usar la regularización*,* que "refina" el modelo si los 
valores de los parámetros complican la operación del algoritmo. Para un modelo 
de regresión lineal, la regularización implica la limitación de pesos. Cuanto más 
bajos sean los valores de peso, más fácil será entrenar el algoritmo. Para 
averiguar qué tan grandes son los pesos, calculamos la distancia entre el vector 
de peso y el vector que consta de ceros. Por ejemplo, la distancia euclidiana 
﻿ es igual al producto escalar de los pesos solos: 
﻿.
Para limitar los valores de peso, incluye el producto escalar de los pesos en la 
fórmula de la función de pérdida:
La derivada 
﻿ es igual a 
﻿. Calcula el gradiente de la función de pérdida:
Para controlar la magnitud de la regularización, agrega el peso de regularización 
a la fórmula de la función de pérdida. Se denota como ﻿.
El peso de regularización también se agrega a la fórmula de cálculo de gradiente:
Cuando usamos la distancia euclidiana para la regularización del peso, dicha 
regresión lineal se denomina regresión de cresta.
d
​(w,0)
2
(w,w)
L(w) = MSE(Xw,y) + (w,w)
(w,w)
2w
∇L(w) =
​X (Xw −
n
2
⊤
y) + 2λw
λ
L(w) = MSE(Xw,y) + λ(w,w)
∇L(w) =
​X (Xw −
n
2
⊤
y) + 2λw
Resumen del capítulo: Entrenamiento del descenso de gradiente
6
Así es como se verá el DGE si consideramos la regularización:
class SGDLinearRegression:
    def __init__(self, step_size, epochs, batch_size, reg_wei
ght):
        self.step_size = step_size
        self.epochs = epochs
        self.batch_size = batch_size
        self.reg_weight = reg_weight
    
    def fit(self, train_features, train_target):
        X = np.concatenate((np.ones((train_features.shape[0], 
1)), train_features), axis=1)        
        y = train_target
        w = np.zeros(X.shape[1])
        
        for _ in range(self.epochs):
            batches_count = X.shape[0] // self.batch_size
            for i in range(batches_count):
                begin = i * self.batch_size
                end = (i + 1) * self.batch_size
                X_batch = X[begin:end, :]
                y_batch = y[begin:end]
                
                gradient = 2 * X_batch.T.dot(X_batch.dot(w) - 
y_batch) / X_batch.shape[0]
                reg = 2 * w.copy()
                reg[0] = 0
                gradient += self.reg_weight * reg
                
                w -= self.step_size * gradient
        self.w = w[1:]
        self.w0 = w[0]
Resumen del capítulo: Entrenamiento del descenso de gradiente
7
    def predict(self, test_features):
        return test_features.dot(self.w) + self.w0
Fundamentos de redes neuronales
Una red neuronal es un modelo que consta de muchos modelos simples (por 
ejemplo, modelos de regresión lineal). El nombre proviene de la biología: una red 
neuronal artificial utiliza el principio de una operación de red de células 
neuronales donde las neuronas construyen relaciones complejas entre los datos 
de entrada y salida.
Este es un ejemplo de una red neuronal con tres entradas 
﻿ y dos salidas 
﻿:
x
​,x
​,x
​
1
2
3
а
​,а
​
1
2
Resumen del capítulo: Entrenamiento del descenso de gradiente
8
El valor de cada salida, o neurona, se calcula de la misma manera que una 
predicción de regresión lineal.
Cada valor de salida tiene sus propios pesos y 
﻿).
Aquí hay otro ejemplo: La red tiene tres entradas 
﻿, dos variables ocultas 
﻿ y 
﻿, y una salida ﻿.
Los valores h₁ y h₂ se pasan a la función logística σ(x):
a
​ =
1
xw
​1
a
​ =
2
xw
​2
w
​2
x
​,x
​,x
​
1
2
3
h
​1
h
​2
а
Resumen del capítulo: Entrenamiento del descenso de gradiente
9
Los valores 
﻿ y 
﻿ se pasan a la función logística 
﻿:
donde ﻿ es el número de Euler (aproximadamente 2.718281828).
La función logística en una red neuronal se denomina función de activación. Se 
incluye en la neurona después de multiplicar los valores de entrada por los pesos, 
cuando las salidas de la neurona se convierten en entradas para otras neuronas. 
De esta manera, podemos describir dependencias más complejas.
Cada variable oculta (
﻿) es igual al valor de entrada multiplicado por un peso:
Por comodidad, expresamos las variables ocultas 
﻿ y 
﻿ como vector ﻿. Aquí 
está la fórmula para calcular la predicción de la red neuronal:
Por comodidad, expresamos las variables ocultas 
﻿ y 
﻿ como vector ﻿. Aquí 
está la fórmula para calcular la predicción de la red neuronal:
Si ponemos los pesos de varias neuronas en matrices, podemos obtener una red 
aún más compleja, por ejemplo:
h
​1
h
​2
σ(x)
σ(x) =
​
1 + e−x
1
e
h
​,h
​
1
2
h
​ =
1
xw
​1
h
​ =
2
xw
​2
h
​1
h
​2
h
a = σ(h)w
​3
h
​1
h
​2
h
a = σ(xW)w
​3
Resumen del capítulo: Entrenamiento del descenso de gradiente
10
donde:
﻿: vector de entrada con dimensión ﻿ (número de características)
﻿: matriz con dimensión 
﻿
﻿: matriz con dimensión 
﻿
﻿: matriz con dimensión 
﻿
﻿: predicción del modelo (número único)
Cuando una red neuronal de este tipo calcula una predicción, realiza todas las 
operaciones de forma secuencial:
Entrenamiento de redes neuronales
x
p
W
​1
p x m
W
​2
m x k
W
​3
k x 1
a
a = σ(σ(xW
​)W
​)W
​
1
2
3
Resumen del capítulo: Entrenamiento del descenso de gradiente
11
Para entrenar una red neuronal, necesitamos establecer el objetivo de 
entrenamiento. Cualquier red neuronal se puede escribir como una función a 
partir de su vector de entrada y sus parámetros. Definamos lo siguiente:
﻿: características del conjunto de entrenamiento
﻿: conjunto de todos los parámetros de la red neuronal
﻿: función de red neuronal
Tomemos esta red neuronal:
Los parámetros de la red neuronal son pesos en las neuronas:
Aquí está la función de red neuronal:
X
P
N(X,P)
Resumen del capítulo: Entrenamiento del descenso de gradiente
12
Los parámetros de la red neuronal son pesos en las neuronas:
Aquí está la función de red neuronal:
También vamos a definir:
﻿: respuestas del conjunto de entrenamiento
﻿: función de pérdida (por ejemplo, ECM)
Entonces podemos declarar el objetivo del entrenamiento de redes neuronales de 
la siguiente manera:
El mínimo de esta función también se puede encontrar usando el algoritmo DGE.
El algoritmo de aprendizaje de la red neuronal es el mismo que el algoritmo DGE 
para la regresión lineal. Solo calculamos el gradiente de la red neuronal, en lugar 
del gradiente para la regresión lineal.
P = W
​W
​W
​
1
2
3
N(X,P) = σ(σ(XW
​)W
​)W
​
1
2
3
y
L(a,y)
​L(N(X,P),y)
P
min
∇L(N(X,P),y)

### Resumen_del_captulo_Anlisis_de_algoritmos.pdf ###
Resumen del capítulo: Análisis de algoritmos
1
Resumen del capítulo: Análisis 
de algoritmos
Complejidad computacional
El tiempo de ejecución del algoritmo no se mide en segundos. Está determinado 
por el número de operaciones elementales que realiza el algoritmo: sumas, 
multiplicaciones y comparaciones. El tiempo de ejecución en cualquier 
computadora generalmente se denomina tiempo de ejecución real. El tiempo de 
ejecución de un algoritmo también está determinado por sus argumentos.
Es imposible calcular el tiempo de ejecución de los algoritmos complejos. Es por 
eso que determinamos su complejidad computacional, o tiempo de ejecución 
asintótico. El término se basa en la palabra asíntota, que define una línea recta a 
la que se acerca una curva, pero no la cruza.
Vamos a expresar la longitud de la lista como ﻿. El tiempo de trabajo es una 
función de ﻿, escrita como 
﻿. El tiempo de ejecución asintótico de un 
algoritmo muestra cómo 
﻿ crece a medida que ﻿ se incrementa.
Cuando 
﻿ es un polinomio, el tiempo de ejecución asintótico es igual al 
término de la potencia más alta sin coeficiente (por ejemplo, 
﻿ en lugar de 
﻿). 
A medida que ﻿ alcanza valores mayores, los demás términos pierden 
importancia.
Si 
﻿, el tiempo de ejecución asintótico 
﻿. El algoritmo 
tiene una complejidad lineal. El símbolo de la tilde (
﻿) significa que el tiempo 
de ejecución asintótico es ﻿.
Si 
, el tiempo de ejecución asintótico 
﻿. El 
algoritmo tiene una complejidad cuadrática.
Si 
﻿, entonces 
﻿. El algoritmo tiene 
complejidad cúbica.
n
n
T(n)
T(n)
n
T(n)
n2
5n2
n
T(n) = 4n + 3
T(n) ∽n
∽
n
T(n) = 5n +
2
3n −1
T(n) ∽n2
T(n) = 10n −
3
2n +
2
5
T(n) ∽n3
Resumen del capítulo: Análisis de algoritmos
2
Si 
﻿, entonces 
﻿. El algoritmo tiene complejidad constante, 
es decir, no depende de ﻿.
Tiempo de entrenamiento de un modelo de regresión 
lineal
El objetivo del entrenamiento de una regresión lineal se representa de la siguiente 
manera:
Los pesos se calculan mediante esta fórmula:
Determinaremos la complejidad computacional del cálculo de pesos, pero primero 
repasemos algunos aspectos:
Vamos a expresar el número de observaciones en el conjunto de 
entrenamiento como ﻿ y el número de características como ﻿
El tamaño de la matriz 
﻿ será 
﻿ y el tamaño del vector ﻿ será ﻿
La complejidad computacional se expresará como 
﻿ porque depende de 
dos parámetros, ﻿ y ﻿
Para calcular la complejidad del entrenamiento del algoritmo, hay que sumar las 
respuestas:
﻿
Normalmente hay menos características que observaciones, lo que significa que 
﻿. Si multiplicamos ambas partes por 
﻿, obtenemos 
﻿. Tomando 
solo el término con la mayor potencia, obtenemos: 
﻿. Si hay muchas 
características, el entrenamiento llevará más tiempo.
T(n) = 10
T(n) ∽1
n
w =
MSE(Xw,y)
w
arg min
w = (X X)
X y
⊤
−1
⊤
n
p
X
n x p
y
n
T(n,p)
n
p
T(n,p) ∽np +
2
p +
3
np +
2
np
p < n
p2
p <
3
np2
T(n,p) ∽np2
Resumen del capítulo: Análisis de algoritmos
3
Métodos iterativos
La siguiente fórmula se emplea como método directo para entrenar modelos de 
regresión lineal:
Los métodos directos ayudan a encontrar una solución precisa utilizando una 
fórmula o un algoritmo determinado. Su complejidad computacional es 
independiente de los datos.
Otro enfoque para entrenar modelos de regresión lineal es el uso de métodos 
iterativos o algoritmos iterativos. Sin embargo, estos no te darán una solución 
precisa, sino solo una aproximada. El algoritmo realiza iteraciones similares 
repetidamente y la solución se vuelve más precisa con cada paso. En el caso de 
que no se necesite una gran precisión, bastará con unas pocas iteraciones.
La complejidad computacional de los métodos iterativos depende del número de 
pasos realizados, que puede verse afectado por la cantidad de datos.
Método de bisección
Vamos a encontrar la solución de la ecuación 
﻿ mediante un método 
iterativo. Vamos a definir 
﻿ como una función continua. Esto significa que su 
gráfico se puede trazar sin levantar el lápiz del papel.
El método de bisección nos ayudará a resolver nuestra ecuación. Este toma una 
función continua y el segmento 
﻿ como entrada. Los valores 
﻿ y 
﻿ 
tienen signos diferentes.
Cuando se cumplen estas dos condiciones:
1. la función es continua
2. los valores de los extremos del segmento tienen signos diferentes
entonces la raíz de la ecuación se encuentra en algún punto del segmento dado.
En cada iteración, el método de bisección:
w = (X X)
X y
⊤
−1
⊤
f(x) = 0
f(x)
[a,b]
f(a)
f(b)
Resumen del capítulo: Análisis de algoritmos
4
comprueba si algún valor 
﻿ o 
﻿ es igual a cero. Si lo es, ya tenemos la 
solución;
encuentra el centro del segmento 
﻿;
compara el signo de 
﻿ con los signos de 
﻿ and 
﻿:
Si 
﻿ y 
﻿ tienen signos diferentes, la raíz se encuentra en el 
segmento 
﻿. El algoritmo analizará este segmento en su siguiente 
iteración.
Si 
﻿ y 
﻿ tienen signos diferentes, la raíz se encuentra en el segmento 
﻿. El algoritmo analizará este segmento en su siguiente iteración.
Los signos de 
﻿ y 
﻿ son diferentes, por lo que no hay más opciones.
La exactitud de la solución se suele elegir de antemano, por ejemplo, e (margen 
de error) = 0.000001. En cada iteración, el segmento con la raíz se divide entre 2. 
Una vez que se alcance una longitud de segmento inferior a e, el algoritmo podrá 
detenerse. Esta condición se denomina criterio de parada.
Comparación de métodos
Una gran parte de este curso se centra en el descenso de gradiente, que es el 
método iterativo clave para el machine learning. Muchos algoritmos de 
entrenamiento se basan en él porque tiene muchas ventajas en comparación con 
los métodos directos, por ejemplo:
Funciona más rápido con grandes conjuntos de datos en regresiones lineales 
con la función de pérdida ECM.
También es apropiado para regresiones lineales con otras funciones de 
pérdida (no todas tienen fórmulas).
Puede utilizarse para entrenar redes neuronales que también carecen de 
fórmulas directas.
f(a)
f(b)
c = (a + b)/2
f(c)
f(a)
f(b)
f(c)
f(a)
[a,c]
f(c)
f(b)
[b,c]
f(a)
f(b)

### Resumen_del_captulo_Descenso_de_gradiente.pdf ###
Resumen del capítulo: Descenso de gradiente
1
Resumen del capítulo: 
Descenso de gradiente
Minimización de la función de pérdida
Antes de plantear una tarea de descenso de gradiente, veamos el problema de 
entrenamiento desde un nuevo ángulo. Ya conoces la función de pérdida. Esta 
cuantifica las respuestas incorrectas del modelo y suele utilizarse para su 
entrenamiento.
En el curso anterior, equiparamos la función de pérdida con el ECM. En este 
nuevo ejercicio, vamos a expresar la función de pérdida como 
﻿, donde el 
vector ﻿ representa las respuestas correctas y el vector ﻿ representa las 
predicciones.
Escribe la tarea de entrenamiento del modelo mediante la siguiente función de 
pérdida:
Para calcular el ECM, o la función de pérdida cuadrática, eleva al cuadrado la 
diferencia entre las respuestas correctas y las predicciones:
¿Qué otras funciones de pérdida existen para una tarea de regresión? Si 
queremos que la función de la tarea sea menos sensible a los valores atípicos, 
entonces en lugar del ECM, utilizaremos la función de pérdida absoluta (EAM), 
que también conoces ya. Al igual que con el ECM, la utilizaremos como una 
función de pérdida, no como una métrica de evaluación.
L(y,a)
y
a
w =
​L(y,a)
w
arg min
L = (y,a) =
​(a
​ −
i=1
∑
n
i
y
​)
i 2
Resumen del capítulo: Descenso de gradiente
2
Cuando se trata de problemas de clasificación, a menudo se utiliza la métrica de 
exactitud. Pero rara vez se puede utilizar como una función de pérdida. Esto se 
debe a que la función de cálculo de la exactitud no tiene ninguna derivada que 
muestre el cambio en la función ante pequeños cambios en el argumento.
Sustituyamos la exactitud por la verosimilitud logarítmica negativa o la función 
de pérdida logística. La función de pérdida suma los logaritmos de las 
probabilidades en función de la observación. Si la respuesta correcta es 0, se 
suma 
﻿. Si es cero, se suma 
﻿. El logaritmo es la potencia a la 
que se eleva la base (en nuestro caso, 10) para extraer el argumento.
Aquí tienes la fórmula:
donde 
﻿ es la probabilidad de la clase 1 para la observación con índice ﻿. Es 
decir, el valor de 
﻿ debe ser lo más alto posible para una observación de clase 
positiva y lo más bajo posible para una observación de clase negativa.
El nombre de la verosimilitud logarítmica negativa proviene de la función de 
verosimilitud, que calcula la probabilidad de que el modelo dé respuestas 
correctas para todas las observaciones si los valores 
﻿ se toman como 
respuesta:
Al tomar el logaritmo, los valores "se expanden" a un rango más amplio. Por 
ejemplo, el rango de 0 a 1 se convierte en el rango de -∞ a 0. Para este rango, los 
errores de cálculo no son tan importantes. Multiplicamos por -1 porque la función 
de pérdida final debe minimizarse.
A diferencia de la exactitud, la función de pérdida logarítmica tiene una derivada.
L = (y,a) =
​∣a
​ −
i=1
∑
n
i
y
​∣
i
log
​a
​
10
i
log
​(1 −
10
a
​)
i
L(y,a) = −
​
​
​
i=1
∑
n
{ log
​ a
​
10
i
log
​(1 −a
​)
10
i
if y
​ = 1
i
if y
​ = 0
i
a
​i
i
a
​i
a
​i
Probabilidad(y,a) =
​
​
​
i=1
∏
n
{ a
​i
1 −a
​i
if y
​ = 1
i
if y
​ = 0
i
Resumen del capítulo: Descenso de gradiente
3
Gradiente de una función
No siempre podemos encontrar manualmente el mínimo de la función de pérdida. 
El gradiente de una función puede ayudar a encontrar la dirección.
La función de pérdida depende de los parámetros del algoritmo. Esta función es 
una función de valor vectorial, es decir, toma un vector y devuelve un escalar.
El gradiente de una función de valor vectorial es un vector que está formado por 
las derivadas de la respuesta para cada argumento. Se expresa como 
﻿ (nabla - 
un arpa hebrea de figura similar a la del símbolo). El gradiente de la función ﻿ de 
un vector ﻿-dimensional ﻿ se calcula de la siguiente manera:
donde 
﻿ es la derivada parcial de la función ﻿ para el argumento 
﻿. La 
función gradiente para un argumento es la derivada.
El gradiente indica la dirección en la que la función crece más rápido. Sin 
embargo, no sirve para resolver el problema de minimización. Necesitamos el 
gradiente negativo, que es un vector opuesto que muestra el decrecimiento más 
rápido. Lo podemos determinar de la siguiente manera:
Descenso de gradiente
El descenso de gradiente es un algoritmo iterativo para encontrar el mínimo de la 
función de pérdida. Sigue la dirección del gradiente negativo y se aproxima 
gradualmente al mínimo.
Es difícil llegar al mínimo en una sola iteración, porque el vector de gradiente 
negativo no indica el punto mínimo de la función de pérdida, sino la dirección del 
decrecimiento.
Para comenzar el descenso, vamos a elegir el valor inicial del argumento (vector 
). Se expresa como 
﻿. A partir de ahí, se realizará el primer paso de descenso 
∇
f
n
x
∇f(x) =
​,
​,…,
​
(∂x
​1
∂f
∂x
​2
∂f
∂x
​n
∂f )
∂f/∂x
​i
f
x
​i
−∇f(x)
x
x0
Resumen del capítulo: Descenso de gradiente
4
de gradiente. El siguiente punto, 
﻿, se calcula de la siguiente manera: al punto 
﻿ 
se le suma el gradiente negativo multiplicado por el tamaño del paso de descenso 
del gradiente ( ﻿).
El valor ﻿ determina el tamaño del paso de descenso de gradiente. Si el paso es 
pequeño, el descenso pasará por muchas iteraciones. No obstante, cada una de 
ellas nos acercará al mínimo de la función de pérdida. Si el paso es demasiado 
grande, podemos pasar por alto el mínimo (chocaremos contra la roca como un 
buceador).
Repite la operación para obtener los valores de los argumentos en las siguientes 
iteraciones. El número de iteraciones se expresa como ﻿. Para obtener los nuevos 
valores de 
﻿, hay que multiplicar el gradiente negativo por el tamaño del paso y 
sumar este producto al valor anterior:
El descenso de gradiente está completado cuando:
el algoritmo complete el número necesario de iteraciones
o
el valor de  está por debajo de un valor umbral (elegido por el analista según 
la exactitud requerida).
Descenso de gradiente en Python
Primero, vamos a hacer un resumen de los pasos necesarios para ejecutar un 
algoritmo de descenso de gradiente:
1. En los argumentos del algoritmo, establece el valor inicial, 
﻿.
2. Calcula el gradiente de la función de pérdida (es el vector de derivadas 
parciales respecto a cada argumento que toma el vector ﻿ como entrada).
3. Encuentra un nuevo valor mediante la fórmula: 
x1
x0
μ
x =
1
x −
0
μ × ∇f(x)
μ
t
xt
x =
t
x
−
t−1
μ × ∇f(x
)
t−1
x
x0
x
x =
t
x
−
t−1
μ × ∇f(x
)
t−1
Resumen del capítulo: Descenso de gradiente
5
donde ﻿ es el tamaño de paso que se establece en el argumento del 
algoritmo.
4. Realiza el número de iteraciones especificado en los argumentos.
import numpy as np
def func(x):
    # función que se minimizará
def gradient(x):
    # gradiente de función func
def gradient_descent(initialization, step_size, iterations):
    x = initialization
    for i in range(iterations):
        x = x - step_size * gradient(x)
    return x
μ

### Resumen_del_captulo_Distancia_entre_vectores.pdf ###
Resumen del capítulo: Distancia entre vectores
1
Resumen del capítulo: Distancia 
entre vectores
Producto escalar
Si multiplicamos todas las componentes y luego sumamos los valores obtenidos, 
tendremos el producto escalar o producto punto. Como resultado de esta operación 
con dos vectores del mismo tamaño obtenemos un nuevo número. Ese número se 
llama escalar.
Aquí está la fórmula para un producto escalar de dos vectores 𝑎=[𝑥1, 𝑥2… 𝑥𝑛] y 𝑏=[𝑦1, 
𝑦2… 𝑦𝑛]:
El producto escalar de los vectores a y b generalmente se denota entre paréntesis (𝑎, 
𝑏) o a un punto 𝑎⋅𝑏.
En NumPy, podemos encontrar el producto escalar usando la función numpy.dot() :
import numpy as np 
 
dot_value = np.dot(vector1, vector2)
El operador de multiplicación de matrices nos permite calcular el producto escalar 
aún más fácilmente. El operador se marca con @ :
import numpy as np 
 
volume = np.array([0.1, 0.3, 0.1]) 
content = np.array([0.4, 0.0, 0.1]) 
 
dot_value = vector1 @ vector2
Resumen del capítulo: Distancia entre vectores
2
También existe la multiplicación elemento por elemento. A diferencia del producto 
escalar, el resultado será un vector:
import numpy as np 
 
vector3 = vector1 * vector2
Distancia planar
La longitud del vector o módulo es igual a la raíz cuadrada del producto escalar del 
vector y este mismo. Por ejemplo, para el vector 𝑎=(𝑥, 𝑦), se calcula así:
Para medir la distancia entre dos puntos, es decir, para obtener la raíz cuadrada de las 
diferencias de los vectores, vamos a buscar la distancia euclidiana. Esta calcula la 
distancia más corta usando el teorema de Pitágoras: el cuadrado de la hipotenusa es 
igual a la suma de los cuadrados de los otros lados.
La distancia euclidiana se puede escribir así: d₂(a, b). La d lleva el subíndice 2 para 
indicar que las coordenadas del vector están elevadas a la segunda potencia.
La distancia entre los puntos a(𝑥1, 𝑦1) y b(𝑥2, 𝑦2) se calcula mediante la fórmula:
Resumen del capítulo: Distancia entre vectores
3
Encontremos la distancia euclidiana entre los puntos a=(5, 6) y b=(1, 3):
import numpy as np 
 
a = np.array([5, 6]) 
b = np.array([1, 3]) 
d = np.dot(a-b, a-b)**0.5 
print('La distancia entre a y b es igual a', d)
SciPy tiene una librería dedicada al cálculo de distancias, que se llama distance. 
Llama a la función distance.euclidean()  para encontrar la distancia euclidiana:
import numpy as np 
from scipy.spatial import distance 
 
a = np.array([5, 6]) 
b = np.array([1, 3]) 
d = distance.euclidean(a, b) 
print('La distancia entre a y b es igual a', d)
Los resultados del cálculo son los mismos.
Distancia Manhattan
Distancia Manhattan o distancia entre manzanas es la suma de módulos de 
diferencias de coordenadas. El nombre se debe a que el trazado de las calles de 
Manhattan hace imposible utilizar la distancia euclidiana (que se calcula mediante la 
línea recta).
Vamos a calcular la distancia Manhattan entre los puntos a=(𝑥1, 𝑦1) y b=(𝑥2, 𝑦2) con la 
siguiente fórmula:
Resumen del capítulo: Distancia entre vectores
4
La distancia Manhattan se formula como d₁(a, b). La d lleva el subíndice 1 para indicar 
que las coordenadas del vector están elevadas a la primera potencia (el número no 
cambia).
La función para calcular la distancia Manhattan en SciPy se llama distance.cityblock() :
import numpy as np 
from scipy.spatial import distance 
 
a = np.array([5, 6]) 
b = np.array([1, 3]) 
d = distance.cityblock(a, b) 
print('La distancia entre a y b es igual a', d)
Para encontrar el índice mínimo en la matriz NumPy, llama a la función argmin() .
index = np.array(distances).argmin() # índice de elemento mínimo
Distancias en el espacio multidimensional
En machine learning, los vectores son características de las observaciones. Por lo 
general, los vectores son multidimensionales en lugar de bidimensionales.
La distancia euclidiana entre los vectores 𝑎=(𝑥1, 𝑥2… 𝑥𝑛) y 𝑏=(𝑦1, 𝑦2… 𝑦𝑛) es la 
suma de los cuadrados de las diferencias de coordenadas:
La distancia Manhattan es la suma de módulos de diferencias de coordenadas:
Resumen del capítulo: Distancia entre vectores
5
Incluso cuando el número de coordenadas es superior a dos, utilizamos las conocidas 
funciones distance.euclidean()  y distance.cityblock()  para calcular distancias en el 
espacio multidimensional.
Algoritmo de vecinos más cercanos
Observa la imagen de abajo. ¿Cómo podemos predecir la clase de observación? 
Podemos encontrar el objeto más cercano en la muestra y obtener la respuesta de él. 
Así es como funciona el algoritmo de vecinos más cercanos. Por lo general, 
buscamos la observación más cercana en el conjunto de entrenamiento.
El algoritmo funciona tanto en el plano como en el espacio multidimensional, en cuyo 
caso las distancias se calculan mediante fórmulas multidimensionales.
Creación de clase modelo
Resumen del capítulo: Distancia entre vectores
6
Clase (class) es un nuevo tipo de datos con sus propios métodos y atributos. Echemos 
un vistazo al modelo constante para una tarea de regresión. Este predice respuestas 
basadas en el valor medio del objetivo en el conjunto de entrenamiento.
Para crear una nueva clase, especifica la palabra clave class  seguida del nombre de la 
clase.
 
class ConstantRegression: 
    # contenido de clase con un offset (desplazamiento) de cuatro espacios 
    # ...
Para entrenar el modelo, vamos a utilizar el método fit() . Es una función dentro de la 
clase y el primer parámetro siempre es self . self  es una variable que almacena el 
modelo. Es necesaria para trabajar con los atributos. Otros dos parámetros son las 
características y el objetivo del conjunto de entrenamiento, al igual que en Sklearn.
class ConstantRegression: 
    def fit(self, features_train, target_train): 
        # contenido de la función con offset 4+4 
        # ...
Al realizar el entrenamiento, debemos guardar el valor medio del objetivo. Para crear el 
nuevo atributo value , agrega self  con un punto al principio del nombre de la variable. 
De esta forma, indicamos que la variable está dentro de la clase:
class ConstantRegression: 
    def fit(self, features_train, target_train): 
        self.value = target_train.mean()
Vamos a usar el método predict()  para predecir la respuesta, que es la media 
guardada:
class ConstantRegression: 
    def fit(self, features_train, target_train): 
        self.value = target_train.mean() 
 
Resumen del capítulo: Distancia entre vectores
7
    def predict(self, new_features): 
        answer = pd.Series(self.value, index=new_features.index) 
        return answer

### Resumen_del_captulo_Funciones_avanzadas_de_SQL_para_analistas.pdf ###
Resumen del capítulo: Funciones avanzadas de SQL para analistas
1
Resumen del capítulo: 
Funciones avanzadas de SQL 
para analistas
Agrupación de datos
Cuando los datos se van a dividir en grupos por valores de campo, se utiliza el 
comando GROUP BY:
 SELECT  
 field_1,  
 field_2,  
 ...,  
 field_n,  
 AGGREGATE_FUNCTION(field) AS here_you_are 
FROM 
 table_name 
WHERE -- si es necesario 
 condition  
GROUP BY  
 field_1,  
 field_2,  
 ...,  
 field_n
Una vez que sabes por qué campos agruparás, asegúrate de que todos esos campos 
estén enumerados tanto en el bloque SELECT como en el bloque GROUP BY. La 
función de agregación en sí misma no debe incluirse en el bloque GROUP BY; de lo 
contrario, la consulta no cumplirá. GROUP BY de SQL funciona de manera muy similar 
al método groupby()  en pandas.
GROUP BY se puede usar con cualquier función de agregación: COUNT, AVG, SUM, 
MAX, MIN. Puedes llamar a varias funciones a la vez.
Ordenar datos
Resumen del capítulo: Funciones avanzadas de SQL para analistas
2
Los resultados del análisis normalmente se presentan en un cierto orden. Para ordenar 
los datos por un campo, utiliza el comando ORDER BY.
SELECT  
 field_1,  
 field_2, 
 ...,  
 field_n,  
 AGGREGATE_FUNCTION(field) AS here_you_are 
FROM 
 table_name 
WHERE -- si es necesario 
 condition 
GROUP BY   
 field_1,  
 field_2,  
 ...,  
 field_n, 
ORDER BY -- si es necesario. Enumera solo los campos 
--por los que tenemos que ordenar los datos de la tabla 
 field_1,  
 field_2,  
 ...,  
 field_n,  
 here_you_are;
A diferencia de GROUP BY, con ORDER BY solo los campos por los que queremos 
ordenar los datos deben aparecer en el bloque de comandos.
Se pueden utilizar dos modificadores con el comando ORDER BY para ordenar los 
datos en columnas:
ASC (el valor predeterminado) ordena los datos en orden ascendente.
DESC ordena los datos en orden descendente.
Los modificadores ORDER BY se escriben justo después del campo por el que se 
ordenan los datos:
ORDER BY  
 field_name DESC 
-- ordenar los datos en orden descendente 
 
ORDER BY  
 field_name ASC;  
-- ordenar los datos en orden ascendente
Resumen del capítulo: Funciones avanzadas de SQL para analistas
3
El comando LIMIT establece un límite para el número de filas en el resultado. Siempre 
viene al final de una instrucción, seguido del número de filas en las que se establecerá 
el límite (n):
SELECT  
 field_1,  
 field_2,  
 ...,  
 field_n,  
 AGGREGATE_FUNCTION(field) AS here_you_are 
FROM 
 table_name 
WHERE -- si es necesario 
 condition 
GROUP BY   
 field_1,  
 field_2,  
 ...,  
 field_n, 
ORDER BY -- si es necesario. Enumera solo los campos 
--por los que tenemos que ordenar los datos de la tabla 
 field_1,  
 field_2,  
 ...,  
 field_n,  
 here_you_are 
LIMIT -- si es necesario 
 n; 
-- n: el número máximo de filas a devolver
Procesamiento de datos dentro de una agrupación
La construcción WHERE se usa para ordenar datos por filas. Sus parámetros son, de 
hecho, filas de tabla. Cuando necesitamos ordenar datos por resultados de funciones 
de agregación, usamos la construcción HAVING, que tiene mucho en común con 
WHERE:
 SELECT  
 field_1,  
 field_2,  
 ...,  
 field_n,  
 AGGREGATE_FUNCTION(field) AS here_you_are 
Resumen del capítulo: Funciones avanzadas de SQL para analistas
4
FROM 
 TABLE 
WHERE -- si es necesario 
 condition 
GROUP BY  
 field_1,  
 field_2,  
 ...,  
 field_n 
HAVING 
 AGGREGATE_FUNCTION(field_for_grouping) > n 
ORDER BY -- si es necesario. Enumera solo los campos 
--por los que tenemos que ordenar los datos 
 field_1,  
 field_2,  
 ...,  
 field_n, 
 here_you_are 
LIMIT -- si es necesario 
 n; 
 
La selección resultante incluirá solo aquellas filas para las que la función de agregación 
produzca resultados que cumplan la condición indicada en los bloques HAVING y 
WHERE.
HAVING y WHERE tienen mucho en común. Entonces ¿por qué no podemos pasar 
todas nuestras condiciones a una de ellas? El caso es que el comando WHERE se 
compila antes de realizar las operaciones de agrupación y aritméticas. Es por eso que 
es imposible establecer parámetros de clasificación para los resultados de una función 
de agregación con WHERE. De allí viene la necesidad de usar HAVING.
Presta atención especial al orden en que aparecen los comandos:
1) GROUP BY
2) HAVING
3) ORDER BY
Este orden es obligatorio. De lo contrario, el código no funcionará.
Operadores y funciones para trabajar con fechas
Resumen del capítulo: Funciones avanzadas de SQL para analistas
5
Tenemos dos funciones principales para trabajar con valores de fecha y hora: 
EXTRACT y DATE_TRUNC. Ambas funciones se llaman en el bloque SELECT.
Así es como se ve la función EXTRACT:
SELECT 
  EXTRACT(date_fragment FROM column_name) AS new_column_with_date  
FROM  
 Table_with_all_dates;
EXTRACT, como es lógico, extrae la información que necesitas de la marca temporal. 
Puedes recuperar:
century  — siglo
day  — día
doy  — día del año, del 1 al 365/366
isodow  — (día de la semana según la ISO 8601, el formato internacional de fecha y 
hora); el lunes es 1, el domingo es 7
hour  — hora
milliseconds  — milisegundos
minute  — minuto
second  — segundo
month  — mes
quarter  — trimestre
week  — semana del año
year  — año
DATE_TRUNC trunca la fecha cuando solo necesitas un cierto nivel de precisión. (Por 
ejemplo, si necesitas saber qué día se realizó un pedido, pero la hora no importa, 
puedes usar DATE_TRUNC con el argumento "day"). A diferencia de EXTRACT, la 
fecha truncada resultante se proporciona como una cadena. La columna de la que se 
toma la fecha completa viene después de una coma:
Resumen del capítulo: Funciones avanzadas de SQL para analistas
6
SELECT 
  DATE_TRUNC('date_fragment_to_be_truncated_to', column_name) AS new_column_with_date  
FROM  
 Table_with_all_dates;
Puedes usar los siguientes argumentos con la función DATE_TRUNC:
'microseconds'
'milliseconds'
'second'
'minute'
'hour'
'day'
'week'
'month'
'quarter'
'year'
'decade'
'century'
Subconsultas
Una subconsulta, o consulta interna, es una consulta dentro de una consulta. 
Recupera información que luego se utilizará en la consulta externa.
Se puede utilizar las subconsultas en varias ubicaciones dentro de una consulta. Si una 
subconsulta está dentro del bloque FROM, SELECT seleccionará datos de la tabla que 
genera la subconsulta. El nombre de la tabla se indica dentro de la consulta interna y la 
consulta externa se refiere a las columnas de la tabla. Las subconsultas siempre se 
escriben entre paréntesis:
SELECT  
SUBQUERY_1.column_name,  
SUBQUERY_1.column_name_2 
FROM -- para que el código sea legible, coloca subconsultas en nuevas líneas 
 -- sangra las subconsultas 
 (SELECT  
  column_name, 
  column_name_2 
Resumen del capítulo: Funciones avanzadas de SQL para analistas
7
  FROM  
  table_name 
  WHERE  
  column_name = value) AS SUBQUERY_1;  
-- recuerda nombrar tu subconsulta en el bloque FROM
Es posible que necesites subconsultas en varios lugares dentro de tu consulta. Vamos 
a añadir una en el bloque WHERE. La consulta principal comparará los resultados de la 
subconsulta con los valores de la tabla en el bloque externo FROM. Cuando haya una 
coincidencia, se seleccionarán los datos:
SELECT  
 column_name,  
 column_name_1 
FROM  
 table_name 
WHERE  
 column_name =  
  (SELECT  
   column_1 
  FROM  
   table_name_2  
  WHERE 
   column_1  = value);
Ahora agreguemos la construcción IN a nuestra muestra y recopilemos datos de varias 
columnas:
SELECT  
 column_name,  
 column_name_1 
FROM  
 table_name 
WHERE  
 column_name IN   
   (SELECT  
    column_1 
   FROM  
    table_name_2   
   WHERE  
    column_1 = value_1 OR column_1 = value_2);
Resumen del capítulo: Funciones avanzadas de SQL para analistas
8
Funciones de ventana
En SQL, una ventana es una secuencia de filas con las que se realizan los cálculos. 
Puede ser la tabla completa o, por ejemplo, las seis filas por encima de la actual. 
Trabajar con estas ventanas es diferente a trabajar con solicitudes normales.
SELECT 
 author_id, 
 name, 
 price/SUM(price) AS ratio OVER () 
FROM  
 books_price;
La función que precede a la palabra clave OVER se ejecutará con los datos dentro de 
la ventana. Si no indicamos ningún parámetro (como aquí), se utilizará todo el resultado 
de la consulta.
Si queremos agrupar los datos, usamos PARTITION BY:
SELECT 
 author_id, 
 name, 
 price/SUM(price) AS ratio OVER (PARTITION BY  
                  author_id) 
FROM  
 books_price;
Una mirada más detallada a las funciones de la ventana
Palabras clave más importantes al usar funciones de ventana:
Resumen del capítulo: Funciones avanzadas de SQL para analistas
9
ORDER BY: nos permite definir el orden de clasificación de las filas a través de las 
cuales se ejecutará la ventana
ROWS: donde indicamos los marcos de ventana sobre los cuales se calculará una 
función de agregación
SELECT 
 author_id, 
 name, 
 SUM(price) OVER (ORDER BY  
           author_id  
         ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) 
FROM  
 books_price;
Marcos indicadores:
UNBOUNDED PRECEDING: todas las filas que están por encima de la actual
N PRECEDING: las n filas por encima de la actual
CURRENT ROW: la fila actual
N FOLLOWING: las n filas debajo de la actual
UNBOUNDED FOLLOWING: todas las filas debajo de la actual

### Resumen_del_captulo_Ingeniera_de_caractersticas.pdf ###
Resumen del capítulo: Ingeniería de características
1
Resumen del capítulo: Ingeniería 
de características
Conocer los datos
Tanto las conversiones implícitas (de un tipo de datos derivado al tipo de datos padre: 
por ejemplo, de int a float, o 5 → 5.0) como las conversiones explícitas (lo contrario) 
son importantes. Es esencial que los datos se presenten en un formato conveniente y 
legible.
Una parte de este proceso es nombrar columnas. Para esto sirve el método set_axis() .
Clasificar por tipo
Las categorías en un conjunto de datos podrían estar almacenadas como strings de 
diversas longitudes.
¿Cuáles son las implicaciones de almacenarlas de este modo?
La tabla es difícil de procesar visualmente.
El tamaño del archivo y el tiempo de procesamiento de datos es mayor de lo 
necesario.
Para filtrar datos por tipo de ticket, necesitamos ingresar el nombre completo (¡sin 
ningún error de dedo!).
Crear nuevas categorías y modificar las existentes puede tomar demasiado tiempo.
Para almacenar información sobre categorías de la mejor manera posible, utiliza un 
diccionario que mapee cada nombre de categoría como un número. Este número se 
utilizará en la tabla en lugar del nombre de la categoría.
Resumen del capítulo: Ingeniería de características
2
Clasificar por grupo de edad
Suele haber solamente una entrada con un valor de índice específico. Es imposible 
trabajar con bits de datos como este y llegar a conclusiones estadísticas. Es por ello 
que estos datos deben categorizarse, es decir, organizarse en categorías.
Una manera de categorizar los datos es filtrarlos por grupo de edad. Por ejemplo, 18 o 
menos, 19-65, o más de 65.
Reglas de clasificación como estas pueden representarse de manera práctica en 
Python como funciones que toman parámetros y devuelven un valor de categoría.
La función group  que escribimos y el método apply()  pueden usarse para devolver una 
columna con un grupo basado en una columna con un índice distinto.
data['column_group'] = data['column'].apply(group)
Funciones de una fila
Cuando el valor de una columna individual no es suficiente para categorizar, la función 
puede pasar los contenidos de una fila completa como un objeto Series. Una función a 
la que se le da una fila completa también puede devolver un valor de una columna en 
específico.
Cuando se procesan filas en lugar de valores individuales, el método apply()  se 
diferencia en dos aspectos:
1. Se llama al método apply()  para el DataFrame data , no solo para la columna 
['age'] .
2. Por defecto, pandas pasa las columnas a la función group() . Para pasar filas a 
una función, debemos usar el método apply()  con el parámetro axis = 1 .

### Resumen_del_captulo_Matrices_y_operaciones_matriciales.pdf ###
Resumen del capítulo: Matrices y operaciones matriciales
1
Resumen del capítulo: Matrices y 
operaciones matriciales
Creación de matrices
Una matriz (del latín "madre") es una tabla numérica rectangular o una matriz 
bidimensional. Consta de m filas y n columnas (el tamaño se escribe como 𝑚×𝑛). Las 
matrices generalmente se denotan con letras latinas mayúsculas, por ejemplo, A. Sus 
elementos están en minúsculas con doble índice 
, donde i es el número de fila y j es 
el número de columna.
Llama a np.array()  para crear una matriz NumPy a partir de una lista de listas. Todas 
las listas anidadas tienen la misma longitud.
import numpy as np 
 
matrix = np.array([ 
    [1, 2, 3],  
    [4, 5, 6], 
    [7, 8, 9]]) 
print(matrix)
Tomemos una lista de vectores en lugar de una lista de listas:
import numpy as np 
 
string0 = np.array([1,2,3]) 
string1 = np.array([-1,-2,-3]) 
list_of_vectors = [string0, string1] 
matrix_from_vectors = np.array(list_of_vectors) 
 
print(matrix_from_vectors)
aij
Resumen del capítulo: Matrices y operaciones matriciales
2
Crea una matriz a partir de la tabla pandas: su atributo values  es una matriz.
import pandas as pd 
import numpy as np 
 
matrix = df.values 
print(matrix)
El atributo shape  define el tamaño de la matriz . Su 
 elemento se establece en 
NumPy como A[I,j]: las filas y columnas se enumeran desde cero, al igual que los 
índices de matriz.
import numpy as np 
 
A = np.array([ 
    [1, 2, 3],  
    [2, 3, 4]]) 
 
print('Tamaño:', A.shape) 
print('A[1, 2]:', A[1, 2])
Selecciona filas y columnas individuales de la matriz:
import numpy as np 
 
matrix = np.array([ 
    [1, 2, 3],  
    [4, 5, 6], 
    [7, 8, 9], 
    [10,11,12]]) 
 
print('Row 0:', matrix[0, :]) 
print('Column 2:', matrix[:, 2])
Operaciones con elementos de matriz
Puedes realizar las mismas operaciones con elementos de matriz que con elementos 
vectoriales. Dos matrices se pueden sumar, restar, multiplicar o dividir. La parte más 
aij
Resumen del capítulo: Matrices y operaciones matriciales
3
importante es que las operaciones se realizan elemento por elemento y las matrices 
son del mismo tamaño. El resultado de una operación es una matriz del mismo tamaño.
import numpy as np 
 
matrix1 = np.array([ 
    [1, 2],  
    [3, 4]]) 
 
matrix2 = np.array([ 
    [5, 6],  
    [7, 8]]) 
 
print(matrix1 + matrix2)
Puedes multiplicar una matriz por un número, sumar un número o restarlo: la operación 
se aplica a cada elemento.
import numpy as np 
 
matrix = np.array([ 
    [1, 2],  
    [3, 4]]) 
 
print(matrix * 2) 
print(matrix - 2)
Multiplicar una matriz por un vector
Para entender cómo se multiplica una matriz por un vector, vamos a tomar una lista de 
filas. Cada fila de esta lista (matriz) es un vector multiplicado por un escalar y los 
números resultantes forman un nuevo vector.
Por ejemplo, una matriz 𝐴 de tamaño 𝑚×𝑛 se multiplica por un vector  (n-
dimensional). El producto será un nuevo vector 
. Este es un vector 𝑚 
dimensional cuya 𝑖 coordenada es igual al producto escalar de la fila 𝑖 de la matriz y el 
vector .
b
=
c
Ab
b
Resumen del capítulo: Matrices y operaciones matriciales
4
Vamos a realizar esta operación en NumPy y llamar a la función np.dot() , que ya 
conocemos.
import numpy as np 
     
A = np.array([ 
    [1, 2, 3],  
    [4, 5, 6]]) 
 
b = np.array([7, 8, 9]) 
 
print(np.dot(A, b)) 
print(A.dot(b))
Para que la multiplicación sea correcta, el tamaño del vector debe ser igual al ancho de 
la matriz.
Transposición de matriz
La transposición de una matriz es su "giro" sobre la diagonal principal de la matriz, 
que va desde la esquina superior izquierda hasta la esquina inferior derecha. Con esta 
inversión, la matriz A de tamaño 𝑚×𝑛 se transforma en una matriz de tamaño 𝑛×𝑚. En 
otras palabras, las filas de la matriz se convierten en sus columnas y las columnas se 
convierten en sus filas. La matriz transpuesta se indica con el índice superior T:
Resumen del capítulo: Matrices y operaciones matriciales
5
En NumPy, esta operación la realiza el atributo T . Si necesitas construir una matriz, 
comienza desde la lista de columnas para crear una matriz y aplica la transposición:
import numpy as np 
 
matrix = np.array([ 
    [1, 2],  
    [4, -4],  
    [0, 17]]) 
 
print("Matriz transpuesta") 
print(matrix.T)
Multiplica la matriz original por un vector de longitud igual a 3:
vector = [2, 1, -1] 
print(np.dot(matrix, vector))
Tenemos un error: las dimensiones de la matriz (3, 2) y el vector (3,) no están 
alineados. La segunda dimensión de la matriz no es igual a la longitud del vector. Para 
hacer que la multiplicación sea correcta, podemos transponer la matriz:
print(np.dot(matrix.T, vector))
Multiplicación de matrices
Resumen del capítulo: Matrices y operaciones matriciales
6
Durante la multiplicación de matrices, se construye una tercera matriz usando dos 
matrices. Consiste en productos escalares de las filas de la primera matriz por las 
columnas de la segunda. El producto de la fila i de la matriz A (
) y la columna j de la 
matriz B (
) es igual a la matriz 
:
La multiplicación de matrices es posible si el ancho de la primera matriz A (𝑚×𝑛) es 
igual a la altura de la segunda matriz B (𝑛×r). Entonces las dimensiones de su producto 
serán m×r. Así, decimos que la dimensión n "colapsa".
En NumPy, las matrices A y B se multiplican llamando a las funciones np.dot(A, B)  o 
A.dot(B) . También puedes reemplazar esta llamada con un signo de multiplicación de 
matriz @:
import numpy as np 
 
print(A.dot(B)) 
print(np.dot(A,B))  
print(A @ B)
Ai
Bj
Cij
Resumen del capítulo: Matrices y operaciones matriciales
7
El resultado de la multiplicación de matrices depende del orden de los multiplicadores.
matrix = np.array([ 
    [1, 2, 3],  
    [-1, -2, -3]]) 
 
print(matrix @ matrix)
Ocurrió un error: las dimensiones de las matrices no coinciden.
Multiplicar la matriz por sí misma solo es posible si es cuadrada:
square_matrix = np.array([ 
    [1, 2, 3],  
    [-1, -2, -3], 
    [0, 0, 0]]) 
 
print(square_matrix @ square_matrix)

### Resumen_del_captulo_metricas_de_clasificacion.pdf ###
Resumen del capítulo: métricas de clasificación
1
Resumen del capítulo: métricas 
de clasificación
Equilibrio y desequilibrio de las clases
Hay un fuerte desequilibrio de clases en algunos problemas, lo que afecta 
negativamente la forma en que se entrena el modelo. Las clases están desequilibradas 
cuando su proporción está lejos de 1:1. Se observa equilibrio de clase si su número es 
aproximadamente igual.
La exactitud (accuracy) no elimina este desequilibrio de clases. ¡Necesitamos una 
nueva métrica!
Ya sabes que una clase con la etiqueta "1" se llama positiva y una clase con la etiqueta 
"0" se llama negativa.
Si combinamos estas respuestas con predicciones, obtendremos la siguiente división:
Respuestas verdaderas positivas (VP): el modelo etiquetó un objeto como "1", y su 
valor real también es "1”
Respuestas verdaderas negativas (VN): el modelo etiquetó un objeto como "0", y su 
valor real también es "0”
Respuestas falsas positivas (FP): el modelo etiquetó un objeto como "1", pero su 
valor real es "0”
Respuestas falsas negativas (FN): el modelo etiquetó un objeto como "0", pero su 
valor real es "1”
Matriz de confusión
Cuando VP, FP, VN y FN se recopilan en una tabla, se denomina matriz de confusión. 
La matriz se forma de la siguiente manera:
Las etiquetas del algoritmo (0 y 1) se colocan en el eje horizontal ("Predicciones").
Las etiquetas verdaderas de la clase (0 y 1) se colocan en el eje vertical 
("Respuestas").
Lo que obtienes:
Resumen del capítulo: métricas de clasificación
2
1. Las predicciones correctas están en la diagonal principal (desde la esquina superior 
izquierda):
VN en la esquina superior izquierda
VP en la esquina inferior derecha
2. Las predicciones incorrectas están fuera de la diagonal principal:
FP en la esquina superior derecha
FN en la esquina inferior izquierda
La matriz de confusión se encuentra en el módulo sklearn.metrics, que ya conoces. La 
función confusion_matrix() toma respuestas y predicciones correctas y devuelve una 
matriz de confusión.
from sklearn.metrics import confusion_matrix 
 
print(confusion_matrix(target, predictions))
Recall
Resumen del capítulo: métricas de clasificación
3
La matriz de confusión te ayudará a crear nuevas métricas. Comencemos con recall.
Recall (sensibilidad) revela qué porción de respuestas positivas ha identificado el 
modelo entre todas las respuestas. Recall se calcula usando esta fórmula:
Recall es una métrica de evaluación que mide la proporción de respuestas VP entre 
todas las respuestas que realmente tienen una etiqueta 1. Queremos que el valor de 
recall esté cerca de 1. Esto significaría que el modelo es bueno para identificar 
verdaderos positivos. Si está más cerca de cero, es necesario revisar y reparar el 
modelo.
La función recall_score() se encuentra en el módulo sklearn.metrics. La función toma 
respuestas y predicciones correctas y devuelve el valor de recuperación.
from sklearn.metrics import recall_score 
 
print(recall_score(target, predictions))
Precisión
Otra métrica para evaluar la calidad de una predicción de clase objetivo es la 
precision.
La precisión mide cuántas respuestas negativas encontró el modelo mientras buscaba 
respuestas positivas. Cuantas más respuestas negativas se encuentren, menor será la 
precisión.
La precisión se calcula usando esta fórmula:
Queremos que el valor de precisión esté cerca de 1. La función precision_score() se 
encuentra en el módulo sklearn.metrics. La función toma respuestas y predicciones 
correctas y devuelve el valor de precisión.
Recall = V P + FN
V P
Precisi n =
oˊ
V P + FP
V P
Resumen del capítulo: métricas de clasificación
4
from sklearn.metrics import precision_score 
 
print(precision_score(target, predictions))
Valor F1
Por separado, recall y precisión no son muy informativas. Debes aumentar 
simultáneamente los valores de ambas métricas… ¡O podemos recurrir a una nueva 
métrica que las combine!
Recall y precisión evalúan la calidad de las predicciones de la clase positiva desde 
diferentes ángulos. Recall describe qué tan bien entendió el modelo las propiedades de 
esta clase y qué tan bien reconoció la clase. Precisión detecta si el modelo se está 
excediendo al asignar demasiadas etiquetas positivas.
Ambas métricas son importantes. Las métricas de agregación, una de las cuales es el 
valor F1, ayudan a controlarlas de forma simultánea. Esta es la media armónica de 
recall y precisión. En F1, 1 significa que la relación de recall a precisión es 1:1.
Es importante comprender que cuando recall o precisión están cerca de cero, la media 
armónica misma se acerca a 0.
La gráfica muestra los valores del valor F1 con diferentes valores de recall y precisión. 
El color azul corresponde a cero y el color amarillo corresponde a uno.
F1 = Precisi n + Recall
oˊ
2 ⋅Precisi n ⋅Recall
oˊ
Resumen del capítulo: métricas de clasificación
5
Si una clase positiva se pronostica de manera deficiente en una de las escalas (recall o 
precisión), un valor F1 cercano a cero indicará que la predicción de la clase 1 ha 
fallado.
La función f1_score() se encuentra en el módulo sklearn.metrics. La función toma 
respuestas y predicciones correctas y devuelve la media armónica de recall y precisión.
from sklearn.metrics import f1_score 
 
print(f1_score(target, predictions))

### Resumen_del_captulo_Potenciacin_del_gradiente.pdf ###
Resumen del capítulo: Potenciación del gradiente
1
Resumen del capítulo: 
Potenciación del gradiente
Ensambles y potenciación
Un ensamble es un conjunto de modelos para resolver el mismo problema. La 
fortaleza de los ensambles es que el error medio de un grupo de modelos es 
menos significativo que sus errores individuales.
Ya conoces uno de los tipos de modelos de ensamble: el bosque aleatorio. En un 
bosque aleatorio se promedian los resultados de los alumnos base o débiles 
(modelos que componen el ensamble). Los clasificadores base para un bosque 
aleatorio son árboles de decisión.
Otro enfoque para la construcción de ensambles es la potenciación, donde cada 
modelo posterior considera los errores del anterior y, en la predicción final, los 
pronósticos de los alumnos básicos. Echa un vistazo:
donde 
﻿ es la predicción del ensamble, 
﻿ es el número de alumnos base, 
﻿ es la predicción del alumno base, y 
﻿ es la ponderación del modelo.
Por ejemplo, estamos tratando con una tarea de regresión. Tenemos ﻿ 
observaciones con características ﻿ y respuestas correctas ﻿. Nuestra tarea es 
minimizar la función de pérdida ECM:
Por conveniencia, iguala los pesos del modelo a la unidad:
a
​(x) =
N
​γ
​b
​(x)
k=1
∑
N
k k
a
​(x)
N
N
b
​(x)
k
γ
​n
n
x
y
ECM(y,a) =
​
​(a(x
​) −
n
1
i=1
∑
n
i
y
​) →
i
2
​
a(x)
min
γ
​ =
k
1, for all k = 1,…,N
Resumen del capítulo: Potenciación del gradiente
2
Obtenemos:
Ahora creamos un conjunto de modelos secuenciales.
Primero, construye al alumno base 
﻿ resolviendo la tarea de minimización:
El resultado es este ensamble:
Indica el residuo. Es la diferencia entre la predicción en el primer paso y las 
respuestas correctas:
En el segundo paso, construimos el modelo de esta manera:
El ensamble tomará la siguiente forma:
El ensamble tomará la siguiente forma:
a
​(x) =
N
​b
​(x)
k=1
∑
N
k
b
​1
b
​ =
1
​
​
​(b(x
​) −
b
arg min n
1
i=1
∑
n
i
y
​)
i
2
a
​(x) =
1
b
​(x)
1
e
​ =
1,i
y
​ −
i
b
​(x
​)
1
i
b
​ =
2
arg
​
​
​(b(x
​) −
b
min n
1
i=1
∑
n
i
e
​)
1,i
2
a
​(x) =
2
​b
​(x) =
k=1
∑
2
k
b
​(x) +
1
b
​(x)
2
e
​ =
2,i
y
​ −
i
a
​(x
​) =
2
i
y
​ −
i
​b
​(x
​)
k=1
∑
2
k
i
Resumen del capítulo: Potenciación del gradiente
3
En cada paso siguiente, el algoritmo minimiza el error de ensamble del paso 
anterior.
Vamos a resumir las fórmulas. En el paso 
﻿, el residuo se calcula de la 
siguiente manera:
El ensamble en sí se representa como la suma de las predicciones de todos los 
alumnos base combinados hasta este paso:
Entonces, en el paso 
﻿, el algoritmo elegirá el modelo con el error de ensamble 
en el paso 
﻿:
Potenciación del gradiente
En la potenciación, cada alumno base intenta "empujar" las predicciones del paso 
anterior hacia las respuestas correctas. Así es como se minimiza la función de 
pérdida. La potenciación del gradiente hace que el proceso sea aún más eficiente.
Como antes, nuestro objetivo es minimizar la función de pérdida, ¡pero ahora las 
respuestas del modelo son el argumento a lo largo del cual se realizará el 
descenso! Y así obtenemos la potenciación del gradiente.
Por ejemplo, nuestra función de pérdida es 
﻿ y tiene una derivada. 
Recordemos la fórmula del ensamble:
En cada paso, selecciona las respuestas que minimizarán la función:
N −1
e
​ =
N−1,i
y
​ −
i
a
​(x
​)
N−1
i
a
​(x) =
N−1
​b
​(x)
k=1
∑
N−1
k
N
N −1
b
​(x) =
n
arg
​
​
​(b(x
​) −
b
min n
1
i=1
∑
n
i
e
​)
N−1,i
2
L(y,a)
a
​(x) =
N
a
​(x) +
N−1
γ
​b
​(x)
N N
L(y,a(x)) →
​
a
min
Resumen del capítulo: Potenciación del gradiente
4
Minimiza la función con descenso de gradiente. Para ello, en cada paso, calcula el 
gradiente negativo de la función de pérdida para la predicción 
﻿:
Para impulsar las predicciones hacia las respuestas correctas, el alumno base 
aprende a predecir 
﻿: 
Obtén el peso para 
﻿ de la tarea de minimización, iterando varios números:
Es el coeficiente para el alumno base lo que ayuda a ajustar el ensamble para 
hacer predicciones lo más precisas posible.
La potenciación de gradiente es adecuada para diferentes funciones de pérdida 
que tienen derivadas, por ejemplo, el cuadrado medio en una tarea de regresión o 
logarítmica en una tarea de clasificación binaria.
Regularización de potenciación del gradiente
La regularización se puede utilizar para reducir el sobreajuste durante la 
potenciación del gradiente. Si se han reducido los pesos en una regresión lineal, 
entonces la regularización de potenciación del gradiente es:
1. reducción del tamaño del paso
2. ajuste de los parámetros del árbol
3. aleatorización de submuestras para estudiantes base 
﻿
Reduce el tamaño del paso. Revisa la fórmula para calcular predicciones en el 
paso 
﻿:
g
​
N
g
​(x) =
N
∇L(y,a
​(x) +
N−1
a)
g
​
N
b
​(x) =
N
arg
​
​
​(b(x
​) −
b
min n
1
i=1
∑
N
i
g
​(x
​))
N
i
2
b
​
N
yN = arg
​L(y,a
​(x) −
y
min
N−1
yb
​(x))
N
b
​i
N
a
​(x) =
N
a
​(x) +
N−1
γ
​b
​(x)
N N
Resumen del capítulo: Potenciación del gradiente
5
Cuando un algoritmo toma pasos que son demasiado grandes, este recuerda 
rápidamente el conjunto de entrenamiento. Esto da como resultado un sobreajuste 
del modelo.
Introduce el coeficiente ﻿, que controla la tasa de aprendizaje y puede usarse 
para reducir el tamaño del paso:
El valor de este coeficiente se elige iterando diferentes valores en el rango de 0 a 
1. Un valor más pequeño significa un paso más pequeño hacia el gradiente 
negativo y una mayor exactitud del ensamble. Pero si la tasa de aprendizaje es 
demasiado baja, el proceso de entrenamiento llevará demasiado tiempo.
La segunda forma de regularizar la potenciación del gradiente es ajustar los 
parámetros del árbol. Podemos limitar la profundidad del árbol o la cantidad de 
elementos en cada nodo, probar diferentes valores y ver cómo afecta el resultado. 
Por ejemplo, limitemos la profundidad de cada árbol a 2. En cada paso de la 
potenciación, no tendremos que hacer un árbol complejo que se ajuste a todos los 
residuos y el modelo no estará demasiado sobreajustado.
El tercer método de regularización es trabajar con submuestras. El algoritmo 
funciona con submuestras en lugar del conjunto completo. Esta versión del 
algoritmo es similar al DGE y se llama potenciación del gradiente estocástico.
Librerías para potenciación del gradiente
1. XGBoost (potenciación del gradiente extrema) es una librería popular de 
potenciación del gradiente en Kaggle. Código abierto. Lanzada en 2014.
2. LightGBM (máquina ligera de potenciación del gradiente). Desarrollada por 
Microsoft. Entrenamiento de potenciación del gradiente rápido y preciso. 
Funciona directamente con características categóricas. Lanzada en 2017. 
Comparación con XGBoost: 
https://lightgbm.readthedocs.io/en/latest/Experiments.html
3. CatBoost (potenciación categórica). Desarrollada por Yandex. Superior a otros 
algoritmos en términos de métricas de evaluación. Aplica varias técnicas de 
η
a
​(x) =
N
a
​(x) +
N−1
η × γ
​b
​(x)
N N
Resumen del capítulo: Potenciación del gradiente
6
codificación para características categóricas (LabelEncoding, One-Hot 
Encoding). Lanzada en 2017. Comparación con XGBoost and LightGBM: 
https://catboost.ai/#benchmark
Echemos un vistazo a la librería CatBoost.
Importa CatBoostClassifier  de la librería y crea un modelo.Ya que tenemos un 
problema de clasificación, especifica la función de pérdida logística. Haz 10 
iteraciones para que no tengamos que esperar demasiado.
from catboost import CatBoostClassifier
model = CatBoostClassifier(loss_function="Logloss", iteration
s=10)
Entrena el modelo con el método fit() . Además del objetivo y las características, 
pasa las características categóricas al modelo:
# cat_features - categorical features
model.fit(features_train, target_train, cat_features=cat_feat
ures)
Cuando tienes muchas iteraciones y no quieres generar información para cada 
una, usa el argumento verbose :
model = CatBoostClassifier(loss_function="Logloss", iterations=5
model.fit(features_train, target_train, cat_features=cat_feature
Calcula la predicción con predict() :
pred_valid = model.predict(features_valid)

### Resumen_del_captulo_Pronstico_de_series_temporales.pdf ###
Resumen del capítulo: Pronóstico de series temporales
1
Resumen del capítulo: 
Pronóstico de series temporales
Pronóstico de series temporales
El objetivo del pronóstico de series temporales es desarrollar un modelo que prediga 
los valores futuros de una serie temporal con base en datos anteriores. El periodo en el 
futuro para el que se prepara el pronóstico se conoce como horizonte de pronóstico. 
Para los ejercicios de este capítulo usaremos un horizonte de un paso.
Si los valores de una serie temporal o de la función x(t)  (donde t = tiempo) son 
números, entonces te enfrentas a una tarea de regresión para la serie temporal. Si son 
categorías, será una tarea de clasificación.
Crearemos un conjunto de entrenamiento y un conjunto de prueba usando los datos 
iniciales. No puedes mezclar los conjuntos en el pronóstico de series temporales, y los 
datos del conjunto de entrenamiento deben preceder a los datos del conjunto de 
prueba. De lo contrario, las pruebas del modelo serán defectuosas: después de todo, el 
modelo no debe entrenarse con datos futuros. La función train_test_split()  del módulo 
sklearn.model_selection  mezcla datos de forma predeterminada, así que vamos a 
establecer el argumento shuffle  en False  para que los datos se puedan separar 
correctamente en conjuntos de entrenamiento y de prueba:
import pandas as pd 
from sklearn.model_selection import train_test_split 
 
train, test = train_test_split(data, shuffle=False, test_size=0.2)
Exactitud del pronóstico
Resumen del capítulo: Pronóstico de series temporales
2
Usaremos la métrica EAM para evaluar los modelos en nuestras tareas, ya que esta 
métrica es fácil de interpretar.
Hay dos formas de pronosticar series temporales sin entrenamiento:
1. Todos los valores de la muestra de prueba se pronostican con el mismo número 
(una constante). Para la métrica EAM, este número es la mediana.
2. El nuevo valor x(t) se predice mediante el valor anterior de la serie, definido como 
x(t-1). Este método es independiente de la métrica.
Creación de características
1. Características del calendario
Por lo general, las tendencias y la estacionalidad se vinculan con una fecha específica. 
El tipo datetime64  en pandas ya contiene la información necesaria y todo lo que resta 
es presentarlo como columnas separadas. Veamos este ejemplo:
# esta característica contiene años como valores numéricos 
data['year'] = data.index.year 
 
# esta característica contiene días de la semana como valores numéricos 
data['dayofweek'] = data.index.dayofweek
2. Características de desfase
Los valores anteriores en la serie temporal te dirán si la función x(t)  aumentará o 
disminuirá. Vamos a usar la función shift()  para obtener los valores de desfase:
data['lag_1'] = data['target'].shift(1) 
data['lag_2'] = data['target'].shift(2) 
data['lag_3'] = data['target'].shift(3)
No todos los valores de desfase están disponibles para las primeras fechas, por lo que 
estas líneas contienen NaN .
Resumen del capítulo: Pronóstico de series temporales
3
3. Media móvil
La característica de media móvil establece la tendencia general de la serie temporal. 
Aquí podrás recordar cómo calcularla:
data['rolling_mean'] = data['target'].rolling(5).mean()
La media móvil en t considera el valor actual de la serie x(t). Esto es incorrecto: el 
objetivo se "deslizó" en las características. El cálculo de la media móvil no debe incluir 
el valor actual de la serie.

### Resumen_del_captulo_Recuperacin_de_datos_de_recursos_en_lnea.pdf ###
Resumen del capítulo: Recuperación de datos de recursos en línea
1
Resumen del capítulo: 
Recuperación de datos de 
recursos en línea
¿Qué es la minería web?
A veces, es posible que no recibas suficientes datos para realizar un análisis 
exhaustivo. En tales casos, es necesario hacer más investigaciones. Esto te permite 
tener en cuenta más factores, identificar más patrones y llegar a conclusiones 
inesperadas. Los analistas pueden enriquecer sus datos complementándolos con datos 
de Internet. Primero rastrean los recursos que pueden ser relevantes, luego recuperan 
todos los datos necesarios. Este proceso se llama minería web, o análisis sintáctico.
Cosas que un analista necesita saber sobre Internet: 
navegadores, HTML y HTTP
Internet es una red de computadoras que intercambian datos. Tiene reglas para 
intercambiar y representar información en Internet (considerando la forma en que las 
computadoras la muestran). Para que esto suceda, se inventaron varias cosas:
1. Un lenguaje para crear documentos: HTML
2. Aplicaciones de software para ver dichos documentos: navegadores
3. Reglas generales para transferir documentos: HTTP
Protocolos de transferencia
Para que el proceso de intercambio de datos funcione, es necesario que existan ciertas 
reglas que rijan cómo una computadora envía datos a otra. El intercambio de datos en 
Internet se basa en el principio de "solicitud-respuesta": un navegador genera una 
solicitud, luego el servidor la analiza y envía una respuesta. Las reglas para formular 
solicitudes y respuestas están determinadas por lo que se conoce como protocolo de 
transferencia, en este caso, HTTP.
Resumen del capítulo: Recuperación de datos de recursos en línea
2
La mayoría de los sitios web actuales utilizan un protocolo de transferencia de datos 
con seguridad mejorada llamado HTTPS. Este protocolo garantiza que toda la 
comunicación entre tu navegador y un sitio web esté encriptada.
Cuando accedes a un sitio web, tu navegador envía una solicitud HTTP al servidor. El 
servidor, a su vez, formula una respuesta: el código HTML de la página 
correspondiente. Una solicitud formada por un navegador puede contener lo siguiente:
Un método HTTP: determina la operación que se debe realizar. Existen varios 
métodos y los más populares son GET y POST. El primero solicita datos del 
servidor, mientras que el segundo los envía.
Ruta: el segmento de la dirección que sigue al nombre del sitio (en 
example.com/hello, la ruta es /hello).
La versión del protocolo HTTP utilizado para enviar la solicitud (por ejemplo, 
HTTP/1.1).
Encabezados de solicitud, que se utilizan para enviar información adicional al 
servidor.
Solicitar cuerpo. Por ejemplo, el cuerpo de una solicitud POST son los datos que se 
envían. No todas las solicitudes tienen cuerpo.
La respuesta puede contener:
La versión HTTP.
El código y el mensaje de la respuesta (por ejemplo, "200 OK" si todo va bien, o 
"404 Not Found" si no se encuentra la ruta solicitada).
Encabezados que contienen información adicional para el navegador.
Cuerpo de la respuesta (por ejemplo, cuando abres el sitio web, verás el código 
HTML de esta página en el cuerpo de la respuesta).
Introducción a HTML
Para exportar una lista de productos del sitio web de una tienda virtual, primero 
deberás obtener el código de la página y su contenido.
Con HTML, cada objeto de la página debe marcarse para que se muestre 
correctamente. Este marcado implica colocar bloques de información dentro de 
Resumen del capítulo: Recuperación de datos de recursos en línea
3
comandos llamados "etiquetas". Estas etiquetas le dicen a los navegadores cómo 
mostrar la información que rodean.
Un elemento HTML se compone de etiquetas y el contenido que está dentro de ellas. 
Una etiqueta HTML consta de un nombre rodeado de corchetes angulares. Un 
elemento comienza con una etiqueta de apertura con el nombre de la etiqueta y 
termina con una etiqueta de cierre con una barra y el nombre de la etiqueta. Se hace 
referencia al elemento por el nombre de la etiqueta.
Esta es la estructura típica de una página HTML:
1. <html> ... </html>
La etiqueta <html> presenta cada documento HTML e indica su comienzo, mientras 
que </html> marca su final. El <head> y el <body> del documento HTML se 
encuentran entre estas etiquetas.
2. <head> ... </head>
Estas etiquetas marcan el encabezado del documento. Las etiquetas que introducen el 
título del documento (<title>) y la metainformación (adicional) (<meta>) se colocan 
entre estas etiquetas.
3. <body> ... </body>
La etiqueta <body> marca el comienzo del cuerpo de una página HTML. 
Todo el contenido de la página (títulos, párrafos de texto, tablas, imágenes) se coloca 
dentro del cuerpo.
Para que el marcado sea más claro, los desarrolladores dejan comentarios dentro de 
etiquetas especiales <!-- -->  en el código de la página. 
Esto es muy útil para los analistas, así que dejaremos comentarios en código en 
nuestros ejemplos.
Los analistas a menudo analizan las tablas. Por lo general, se colocan en los 
elementos de la tabla, entre las etiquetas <table>  y </table> . La etiqueta de apertura 
<table>  marca el comienzo de una tabla, y la etiqueta de cierre </table>  marca su final. 
Dentro de este elemento, el contenido de la tabla se divide en filas mediante etiquetas 
<tr>  (fila de tabla), y las filas, a su vez, se dividen en celdas mediante etiquetas <td>  
(datos de tabla). La primera fila generalmente contiene encabezados de columna en 
Resumen del capítulo: Recuperación de datos de recursos en línea
4
lugar de celdas normales. Se colocan entre las etiquetas <th>  (encabezado de la 
tabla).
El texto a menudo se coloca dentro de un elemento p (párrafo). El comienzo del párrafo 
se marca con la etiqueta <p>  y el final con la etiqueta </p> .
La etiqueta de bloque <div>  (división), que puede envolver varios elementos, es 
bastante común. div es útil, ya que puede incorporar cualquier cantidad de elementos, 
incluso de diferentes tipos (por ejemplo, un encabezado con una imagen más un par de 
párrafos de texto), y asignarles características o comportamientos comunes.
También puedes colocar atributos entre corchetes para brindar más información sobre 
cómo debe comportarse el elemento. Diferentes tipos de información requieren 
diferentes atributos.
El nombre del atributo le dice al navegador a qué función se refiere el atributo, mientras 
que el valor especifica lo que debería suceder con la función. La mayoría de las veces, 
deberás trabajar con los atributos id  y class . El atributo id proporciona un identificador 
único para un elemento. El valor del atributo class es un nombre que pueden compartir 
varios elementos, al igual que varios miembros de la familia comparten un apellido.
Herramientas para desarrolladores
Todos los navegadores modernos tienen una barra de herramientas para 
desarrolladores web, una navaja suiza para cualquier desarrollador. Aquí puedes ver el 
código de una página completa o de un elemento en particular, ver el estilo de cada 
elemento en la página e incluso cambiar la forma en que se muestran en tu PC. 
Puedes acceder a él presionando Control+Shift+i.
Tu primera solicitud GET
Para obtener datos del servidor, usaremos el método get(), y para enviar solicitudes 
HTTP, necesitamos la librería de solicitudes. Importamos la librería:
import requests
El método get() actúa como un navegador. Le pasaremos el enlace como argumento. 
El método enviará al servidor una solicitud GET, luego procesará la respuesta que 
Resumen del capítulo: Recuperación de datos de recursos en línea
5
reciba y devolverá una respuesta, un objeto que contiene la respuesta del servidor a la 
solicitud.
req = requests.get(URL) # guardando el objeto de respuesta como variable requerida
Un objeto de respuesta contiene la respuesta del servidor: el código de estado, el 
contenido de la solicitud y el código de la propia página HTML. Los atributos de los 
objetos de respuesta hacen posible obtener solo los datos relevantes del servidor. Por 
ejemplo, un objeto de respuesta con el atributo de texto solo devolverá el contenido de 
texto de la solicitud:
print(req.text) # el nombre del atributo se coloca después del objeto de respuesta y se di
vide por un punto
El atributo status_code te dice si el servidor respondió o si ocurrió un error.
print(req.status_code)
Desafortunadamente, no todas las solicitudes regresan con datos. A veces, las 
solicitudes devuelven errores; cada uno tiene un código especial dependiendo de su 
tipo. Estos son los errores más comunes:
Códigos de error
Error code
Name
Implication
200
OK
Todo está bien
302
Found
La página ha sido movida
400
Bad Request
Error en la sintaxis de la solicitud
404
Not Found
No se puede encontrar la página
500
Internal Server Error
Error por parte del servidor
502
Bad Gateway
Error en el intercambio de datos entre servidores
503
Server Unvailable
El servidor no puede procesar solicitudes temporalmente
Resumen del capítulo: Recuperación de datos de recursos en línea
6
Expresiones regulares
Para buscar cadenas en textos grandes, necesitarás una herramienta poderosa: las 
expresiones regulares. Una expresión regular es una regla para buscar subcadenas 
(fragmentos de texto dentro de cadenas). Es posible crear reglas complejas para que 
una expresión regular devuelva varias subcadenas.
Para comenzar a trabajar con expresiones regulares en Python, necesitamos importar 
el módulo re (es decir, expresiones regulares). Siguen dos etapas.
En la primera etapa, creamos el patrón de la expresión regular. Este es un algoritmo 
que describe lo que debe buscarse dentro del texto (por ejemplo, todas las letras 
mayúsculas).
Luego, este patrón se pasa a los métodos específicos de re . Estos métodos buscan, 
reemplazan y eliminan símbolos. En otras palabras, el patrón identifica qué buscar y 
cómo, mientras que el método define qué hacer con las coincidencias que se 
encuentran.
La siguiente tabla enumera los patrones de expresiones regulares más simples. 
Puedes crear expresiones regulares más complejas combinándolos.
Sintaxis de expresiones regulares
Regular
expression
Description
Example
Explanation
[]
Caracter único contenido entre
paréntesis
[a-]
a o -
[^…]
Negación
[^a]
cualquier caracter
excepto «a»
-
Rango
[0-9]
rango: cualquier dígito
del 0 al 9
.
Cualquier caracter único excepto una
nueva línea
a.
as, a1, a_
\d (see [0-9])
Cualquier dígito
a\d | a[0-
9]
a1, a2, a3
\w
Cualquier letra, dígito o _
a\w
a_, a1, ab
[A-z]
Cualquier letra latina
a[A-z]
ab
Resumen del capítulo: Recuperación de datos de recursos en línea
7
Regular
expression
Description
Example
Explanation
[А-я]
Cualquier letra cirílica
a[А-я]
aя
?
0 o 1 entrada
a?
a o nada
+
1 o más entradas
a+
a o aa, o aaa
*
0 o más entradas
a*
Nada o - a, o aa
^
Comienzo de cadena
^a
a1234, abcd
$
Fin de cadena
a$
1a, ba
Estas son algunas de las tareas más comunes para los analistas:
encontrar una subcadena dentro de una cadena
dividir cadenas en subcadenas
reemplazar partes de una cadena con otras cadenas
Para completar estas tareas, necesitarás los siguientes métodos re :
1. search(pattern, string) busca un pattern  en una string . Aunque search()  recorre 
toda la cadena para encontrar el patrón, solo devuelve la primera subcadena que 
encuentra:
import re 
print(re.search(pattern, string))
El método search()  devuelve un objeto de tipo de coincidencia. El parámetro span  
define un rango de índices que coinciden con el patrón. El parámetro de coincidencia 
indica el valor de la propia subcadena.
Si no necesitamos información sobre el span, podemos devolver solo la subcadena 
usando el método group() :
import re 
print(re.search(pattern, string).group())
2. split(pattern, string) rompe una string  en los puntos donde aparece el patrón. 
Resumen del capítulo: Recuperación de datos de recursos en línea
8
import re 
print(re.split(pattern, string))
La cadena se divide donde se encuentra el patrón. El número de divisiones se puede 
controlar mediante el parámetro maxsplit del método split() .
import re 
print(re.split(pattern, string, maxsplit = num_split))
3. sub(pattern, repl, string) busca el pattern  subcadena dentro de una string  y lo 
reemplaza con la subcadena repl (es decir, reemplazar).
import re 
print(re.sub(pattern, repl, string)) 
4. findall(pattern, string) devuelve una lista de todas las subcadenas de una string  
que coinciden con el pattern . Compáralo con el método search()  que solo devuelve la 
primera subcadena coincidente.
import re 
print(re.findall(pattern, string))
El método findall() es particularmente útil porque permite determinar el número de 
subcadenas recurrentes en una cadena con la función len() :
import re 
print(len(re.findall(pattern, string)))
Análisis sintáctico de HTML
La extracción manual de valores de datos puros de una cadena que contiene el código 
de una página puede ser difícil. Para resolver esto necesitamos la librería 
BeautifulSoup. Los métodos de la librería BeautifulSoup convierten un archivo HTML en 
Resumen del capítulo: Recuperación de datos de recursos en línea
9
una estructura de árbol. Luego, el contenido necesario se puede encontrar mediante 
etiquetas y atributos.
from bs4 import BeautifulSoup 
soup = BeautifulSoup(req.text, 'lxml')
El primer argumento presenta datos que formarán una estructura de árbol. El segundo 
argumento es un analizador sintáctico. Este define la forma en que una página web 
entra en el árbol. Existen numerosos analizadores, y todos crean diferentes estructuras 
a partir de un mismo documento HTML. Hemos elegido el analizador lxml por su 
rendimiento de alta velocidad. Pero, por supuesto, existen otros analizadores, como 
html.parser, xml y html5lib.
Una vez que el código se convierte en una estructura de árbol, los datos se pueden 
buscar utilizando varios métodos. El primer método de búsqueda se llama find(). Se 
ejecuta a través de un documento HTML y encuentra el primer elemento cuyo nombre 
se pasó como argumento y lo devuelve junto con las etiquetas y su contenido.
tag_content = soup.find(tag)
Para mostrar el contenido sin etiquetas, necesitarás el método text. Este devolverá el 
resultado en forma de cadena:
tag_content.text
Todavía hay otro método de búsqueda, que es find_all. A diferencia del método 
anterior, find_all()  encuentra todas las instancias de un elemento dado en un 
documento HTML y devuelve una lista:
tag_content = soup.find_all(tag)
Extraigamos solo el contenido de los párrafos con la ayuda del método de text:
Resumen del capítulo: Recuperación de datos de recursos en línea
10
for tag_content in soup.find_all(tag): 
    print(tag_content.text)
Los métodos find()  y find_all()  tienen un filtro extra para buscar elementos de página: 
el parámetro attrs (atributos). Se utiliza para buscar por clases e identificadores. Sus 
nombres se especifican en el panel de herramientas para desarrolladores web.
Debes pasar a attrs  un diccionario con nombres y valores de atributos:
soup.find(tag, attrs={"attr_name": "attr_value"})
API
A veces es necesario solicitar información de fuentes externas cuya estructura es 
mucho más complicada que la de una página HTML ordinaria. Para evitar tener que 
estudiar su estructura para obtener datos más rápido, los analistas envían solicitudes 
GET a aplicaciones de terceros a través de una interfaz de transferencia de datos 
especial llamada API (interfaz de programación de aplicaciones).
La librería requests permite pasar parámetros a una URL. Cuando buscas cierto 
contenido en un sitio web de varias páginas, debes pasar el diccionario PARAM a la 
palabra clave params (parámetros). Por ejemplo:
URL = 'https://yandex.com/' 
PARAM={"page": "4"} 
req = requests.get(url = URL, params = PARAM)
Esta solicitud debe devolver la cuarta página (según el catálogo) del sitio web 
https://yandex.com/. 
JSON
Al responder a tu solicitud, el servidor devuelve datos estructurados en uno de varios 
formatos especiales, de los cuales el más común es JSON (Notación de objetos de 
Resumen del capítulo: Recuperación de datos de recursos en línea
11
JavaScript). Parece un revoltijo de dígitos, letras, dos puntos y llaves.
Así es como se ven los datos en este formato:
[ 
  { 
    "name": "General Slocum", 
    "date": "Junio 15, 1904"  
  }, 
  { 
    "name": "Camorta", 
    "date": "Mayo 6, 1902" 
  }, 
  { 
    "name": "Norge", 
    "date": "Junio 28, 1904" 
  } 
]
Si JSON incluye varios elementos, estos se escriben entre corchetes [ ... ] , al igual 
que en las listas. Un objeto JSON individual parece un diccionario: está delimitado por 
corchetes y tiene pares clave : valor .
JSON permite recopilar datos dentro de un objeto (una lista de pares clave : valor ) y 
luego hacer una cadena para pasar en una solicitud. El receptor vuelve a convertir esta 
cadena en un objeto.
Python tiene un módulo incorporado para trabajar con datos en formato JSON:
import json
Su método json.loads() convierte cadenas que están en formato JSON:
x = '{"Nombre": "General Slocum", "fecha": "Junio 15, 1904"}' 
y = json.loads(x) 
 
print('Nombre : {0}, fecha : {1}'.format(y['Nombre'], y['fecha']))
# Respuesta 
Nombre : General Slocum, fecha : Junio 15, 1904

### Resumen_del_captulo_Redes_neuronales_convolucionales_esp.pdf ###
Resumen del capítulo: Redes neuronales convolucionales
1
Resumen del capítulo: Redes 
neuronales convolucionales
Convolución
Las redes neuronales completamente conectadas no pueden trabajar con imágenes 
grandes. Si no hay suficientes neuronas, la red no podrá encontrar todas las 
conexiones, y si hay demasiadas, la red estará sobreajustada. La convolución es una 
solución simple para esto.
Para encontrar los elementos más importantes para la clasificación, la convolución 
aplica las mismas operaciones a todos los pixeles.
Comencemos con una convolución unidimensional. Ya que no hay imágenes 
unidimensionales, observemos esta operación en una secuencia.
Vamos a definir w como pesos y s como secuencia.
La convolución (c) se hace así: los pesos (w) se mueven a lo largo de la secuencia (s) y 
se calcula el producto escalar para cada posición en la secuencia. La longitud n del 
vector de los pesos nunca es mayor que la longitud m del vector de la secuencia; de 
otro modo, no habría una posición a la que se pudiera aplicar la convolución.
Resumen del capítulo: Redes neuronales convolucionales
2
Vamos a expresar una operación de convolución unidimensional con una fórmula:
Aquí, t es el índice para calcular el producto escalar y k es un valor entre 0 y (m - n + 
1).
Se escoge el número (m - n + 1) para que los pesos no excedan a la secuencia.
Vamos a expresar una convolución unidimensional en Python:
def convolve(sequence, weights): 
    convolution = np.zeros(len(sequence) - len(weights) + 1) 
    for i in range(convolution.shape[0]): 
        convolution[i] = np.sum(weights * sequence[i:i + len(weights)])
Resumen del capítulo: Redes neuronales convolucionales
3
Ahora vamos a ver cómo funciona la convolución bidimensional (2D). Tomemos una 
imagen bidimensional s con un tamaño de m×m pixeles y una matriz de peso de n×n 
pixeles. La matriz es el kernel (núcleo) de la convolución.
El kernel se mueve en la imagen de izquierda a derecha y de arriba a abajo. Sus pesos 
se multiplican por cada pixel en todas las posiciones. Los productos se suman y se 
registran como los pixeles resultantes.
Para ver esto en acción, vamos a convolucionar las siguientes matrices:
s = [[1, 1, 1, 0, 0], 
     [0, 1, 1, 1, 0], 
     [0, 0, 1, 1, 1], 
     [0, 0, 1, 1, 0], 
     [0, 1, 1, 0, 0]] 
 
w = [[1, 0, 1], 
     [0, 1, 0], 
     [1, 0, 1]]
La convolución bidimensional se puede expresar mediante esta fórmula:
Puedes encontrar los contornos de esta imagen usando la convolución. Los contornos 
horizontales se pueden encontrar mediante la convolución con el siguiente kernel:
np.array([[-1, -2, -1], 
          [ 0,  0,  0], 
          [ 1,  2,  1]])
Este kernel fue descubierto por los científicos estadounidenses Irwin Sobel y Gary 
Feldman, y su propósito específico es encontrar los contornos de una imagen. Sirve 
para resaltar los contornos mejor que el kernel anterior de esta lección.
Resumen del capítulo: Redes neuronales convolucionales
4
Usa el siguiente kernel para encontrar contornos verticales:
np.array([[-1, 0, 1], 
          [-2, 0, 2], 
          [-1, 0, 1]])
Capas convolucionales
Hablemos de las capas convolucionales y el papel que desempeñan en las redes 
neuronales convolucionales (CNN). Las capas convolucionales aplican una operación 
de convolución a las imágenes de input y hacen la mayor parte del cómputo dentro de 
la red.
Una capa convolucional consiste en filtros (conjuntos de pesos) que se pueden 
personalizar y entrenar, los cuales se aplican a la imagen. En esencia, un filtro es una 
matriz cuadrada con tamaño de K×K pixeles.
Si el input es una imagen de color, se agrega al filtro una tercera dimensión, la 
profundidad. En este caso, el filtro ya no es una matriz, sino un tensor, o una matriz 
multidimensional.
Veamos un ejemplo. A continuación puedes ver tres canales de color: rojo, azul y 
verde. Un filtro de 3x3x3 (tres pixeles de ancho, alto y profundidad) se mueve a través 
de la imagen input en cada canal, realizando una operación de convolución. No se 
mueve a través de la tercera dimensión y cada color tiene pesos diferentes. En 
consecuencia, las imágenes resultantes se doblan hacia el resultado final de la 
convolución.
Resumen del capítulo: Redes neuronales convolucionales
5
Una capa convolucional puede tener varios filtros, cada uno de los cuales devuelve una 
imagen bidimensional que se puede reconvertir a una tridimensional. En la siguiente 
capa convolucional, la profundidad de los filtros será igual al número de filtros de la 
capa anterior.
El asterisco (*) indica una operación de convolución.
Resumen del capítulo: Redes neuronales convolucionales
6
Las capas convolucionales contienen menos parámetros que las capas completamente 
conectadas, lo que hace que sea más fácil entrenarlas.
Echemos un vistazo a las configuraciones de la capa convolucional:
1. Padding o relleno. Esta configuración coloca ceros en los bordes de la matriz 
(padding cero) de modo que los pixeles más alejados participen en la convolución 
al menos la misma cantidad de veces que los pixeles centrales. Esto evita que se 
pierda información importante. Los ceros agregados también participan en la 
convolución y el tamaño del padding determina el ancho del padding cero.
Resumen del capítulo: Redes neuronales convolucionales
7
2. Striding o stride (paso). Esta configuración desplaza el filtro en más de un pixel y 
genera una imagen de output más pequeña.
Vamos a calcular el tamaño del tensor de output para la capa convolucional. Si la 
imagen inicial tiene un tamaño de W×W×D, un filtro K×K×D, padding (P) y un paso (S), 
entonces la nueva imagen de tamaño W' se puede determinar de la siguiente manera:
Usa este visualizador en GitHub para ver cómo se comportan las convoluciones con 
diferentes parámetros.
Capas convolucionales en Keras
Vamos a crear una capa convolucional llamada Conv2D.
keras.layers.Conv2D(filters, kernel_size, strides, padding, activation)
Resumen del capítulo: Redes neuronales convolucionales
8
Esto es lo que significan todos los parámetros:
filters: el número de filtros, que corresponde al tamaño del tensor de output.
Kernel_size: las dimensiones espaciales del filtro K. Todo filtro es un tensor de 
tamaño K×K×D, donde D es igual a la profundidad de la imagen de input.
strides: un paso (o stride) determina cuán lejos se desplaza el filtro sobre la matriz 
de input. De forma predeterminada, está configurado en 1.
padding: este parámetro define el ancho del padding cero. Hay dos tipos de 
padding: valid (válido) y same (igual). El tipo por defecto es valid y es igual a cero. 
Same establece el tamaño del padding automáticamente de modo que el ancho y 
la altura del tensor de output sea igual al ancho y la altura del tensor de input.
activation: esta función se aplica inmediatamente después de la convolución. 
Puedes usar las funciones de activación que ya conoces: 'relu'  y 'sigmoid' . De 
manera predeterminada, este parámetro es None (esto significa que la activación 
está deshabilitada).
Para que los resultados de la capa convolucional sean compatibles con la capa 
completamente conectada, debes conectar una nueva capa llamada Flatten 
("aplanar"), que convierte al tensor multidimensional en unidimensional.
Por ejemplo, una imagen de input con el tamaño 32x32x3 pasa por una capa 
convolucional, luego por una totalmente conectada, y entre ellas se debe colocar una 
capa Flatten:
from tensorflow.keras import Sequential 
from tensorflow.keras.layers import Conv2D, Flatten, Dense 
 
 
model = Sequential() 
 
# este tensor mide (None, 32, 32, 3) 
# la primera dimensión define diferentes objetos 
# está configurada en None porque desconocemos el tamaño del lote 
 
model.add(Conv2D(filters=4, kernel_size=(3, 3), input_shape=(32, 32, 3))) 
 
# este tensor mide (None, 30, 30, 4) 
 
model.add(Flatten()) 
 
# este tensor mide (None, 3600) 
Resumen del capítulo: Redes neuronales convolucionales
9
# donde 3600 = 30*30*4 
 
model.add(Dense(...))
Arquitectura LeNet
Con las técnicas de pooling (agrupación), puedes reducir el número de los parámetros 
del modelo. Un ejemplo de esto es la operación *Max Pooling, que se hace así:
1. Se determina el tamaño del kernel (por ejemplo, 2x2).
2. El kernel comienza a moverse de izquierda a derecha y de arriba a abajo; en cada 
cuadro de cuatro pixeles hay un pixel con el valor máximo.
3. El pixel con el máximo valor se mantiene, mientras que los que le rodean 
desaparecen.
4. El resultado es una matriz formada solo por los pixeles con los valores máximos.
En Keras también puedes usar la operación AveragePooling. Estas son las principales 
diferencias entre dichas técnicas:
MaxPooling devuelve el valor máximo de pixel del grupo de pixel dentro de un 
canal. Si la imagen de input tiene un tamaño de W×W, entonces el tamaño de la 
imagen de output es W/K, donde K es el tamaño del kernel.
Resumen del capítulo: Redes neuronales convolucionales
10
AveragePooling devuelve el valor promedio de un grupo de pixeles dentro de un 
canal.
En Keras, la operación AveragePooling se escribe así:
keras.layers.AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid', ...)
Veamos cada parámetro:
Pool_size: tamaño de pooling (agrupación). Cuanto más grande sea, más pixeles 
vecinos se involucran.
strides: un paso (o stride) determina cuán lejos se desplaza el filtro sobre la matriz 
de input. Si se especifica None, el paso es igual al tamaño de pooling.
padding: este parámetro define el ancho del padding cero. El tipo predeterminado 
del padding es valid, que es igual a cero. Same establece el tamaño del padding 
automáticamente.
Los parámetros de MaxPooling2D son similares a estos.
Ahora tenemos todas las herramientas para crear una arquitectura popular para 
clasificar imágenes con tamaño de 20-30 pixeles: LeNet. La red toma su nombre de su 
creador, Yann André LeCun, desarrollador de la tecnología de compresión de imágenes 
DjVu y el encargado del Laboratorio de inteligencia artificial de Facebook.
LeNet se estructura de la siguiente manera:
1. La red comienza con dos o tres capas de 5x5 alternando con AveragePooling con 
un tamaño de 2x2. Estas reducen gradualmente la resolución espacial y recopilan 
toda la información de la imagen en una pequeña matriz de unos 5 píxeles.
2. El número de filtros aumenta de capa a capa para evitar la pérdida de información 
importante.
3. Hay una o dos capas totalmente conectadas al final de la red. Estas recopilan y 
clasifican todas las características.
Así es como creamos LeNet en Keras:
model = Sequential() 
 
Resumen del capítulo: Redes neuronales convolucionales
11
model.add(Conv2D(6, (5, 5), padding='same', activation='tanh', 
                 input_shape=(28, 28, 1))) 
model.add(AvgPool2D(pool_size=(2, 2))) 
 
model.add(Conv2D(16, (5, 5), padding='valid', activation='tanh')) 
model.add(AvgPool2D(pool_size=(2, 2))) 
 
model.add(Flatten()) 
model.add(Dense(120, activation='tanh')) 
 
model.add(Dense(84, activation='tanh')) 
 
model.add(Dense(10, activation='softmax')) 
 
model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['acc']) 
model.summary()
El algoritmo Adam
El descenso de gradiente estocástico (SGD en inglés) no es el algoritmo más eficiente 
para entrenar una red neuronal. Si el paso es muy pequeño, el entrenamiento podría 
tardar demasiado. Si es demasiado grande, puede que no logre el entrenamiento 
mínimo requerido. El algoritmo Adam automatiza la selección del paso. Escoge 
diferentes parámetros para diferentes neuronas, lo que acelera el entrenamiento del 
modelo.
Para entender cómo funciona este algoritmo, mira esta visualización creada por Emilien 
Dupont de la Universidad de Oxford. Se muestra cuatro algoritmos: SGD** a la 
izquierda, el algoritmo Adam a la derecha y entre ellos hay otros dos algoritmos 
similares a Adam (de los cuales no hablaremos en detalle). De los cuatro, Adam es la 
forma más rápida de encontrar el mínimo.
Vamos a escribir el algoritmo Adam en Keras:
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',  
              metrics=['acc'])
Hay que determinar la clase algoritmo para configurar los hiperparámetros:
from tensorflow.keras.optimizers import Adam 
optimizer = Adam() 
 
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy',  
Resumen del capítulo: Redes neuronales convolucionales
12
              metrics=['acc'])El algoritmo *Adam* es la tasa de aprendizaje. Esta es la pa
rte del descenso de gradiente donde comienza el algoritmo. Se escribe así:
optimizer = Adam(lr=0.01)
La tasa de aprendizaje predeterminada es 0.001. A veces reducirla puede ralentizar el 
aprendizaje, pero eso mejora la calidad del modelo en general.
Generadores de datos
Las matrices se almacenan en la RAM, no en el disco duro del PC. Ahora bien, ¿qué 
pasaría si necesitaras crear una matriz de terabytes de imágenes? ¡Qué cosa tan difícil 
de siquiera imaginar! Después de todo, los recursos de RAM son limitados.
Para lidiar con una cantidad tan grande de imágenes, necesitas implementar la carga 
de datos dinámica.
La librería Keras tiene una herramienta útil para esto, ImageDataGenerator (materiales 
en inglés):
from tensorflow.keras.preprocessing.image import ImageDataGenerator
La clase ImageDataGenerator forma lotes con imágenes y etiquetas de clase de 
acuerdo con las fotos que están en las carpetas. Vamos a ponerla a prueba.
# Generador de datos 
datagen = ImageDataGenerator()
Para extraer datos de una carpeta, llama a la función flow_from_directory().
datagen_flow = datagen.flow_from_directory( 
    # carpeta con el conjunto de datos 
    '/dataset/', 
    # tamaño de la imagen objetivo, 
    target_size=(150, 150),  
    # tamaño del lote 
    batch_size=16, 
    # modo de clase 
    class_mode='sparse', 
Resumen del capítulo: Redes neuronales convolucionales
13
    # configurar un generador de números aleatorios 
    seed=54321)
Se encontraron 1683 imágenes pertenecientes a 12 clases.
El generador de datos encontró 12 clases (carpetas) y un total de 1683 imágenes.
Echemos un vistazo a los argumentos:
target_size=(150, 150) : argumento con el ancho y alto objetivo de la imagen. Las 
carpetas pueden contener imágenes de distintos tamaños, pero las redes 
neuronales necesitan que todas tengan las mismas dimensiones.
batch_size=16 : el número de imágenes en los lotes. Cuantas más imágenes haya, 
más eficaz será el entrenamiento del modelo. No cabrán demasiadas imágenes en 
la memoria de la GPU, así que 16 es el valor perfecto para iniciar.
сlass_mode='sparse' : argumento que indica el modo de output de la etiqueta de 
clase. sparse  significa que las etiquetas corresponderán al número de la carpeta.
Puedes saber cómo se relacionan los números de clase con los nombres de las 
carpetas de esta manera:
# índices de clase 
print(datagen_flow.class_indices)
Llamar al método datagen.flow_from_directory(...)  devolverá un objeto a partir del cual 
se pueden obtener los pares "imagen-etiqueta" usando la función next():
features, target = next(datagen_flow) 
 
print(features.shape)
El resultado es un tensor de cuatro dimensiones con dieciséis imágenes de 150x150 y 
tres canales de colores.
Para entrenar al modelo con estos datos, vamos a pasar el objeto datagen_flow  al 
método fit()**. La época no debería ser demasiado larga. Para limitar el tiempo de 
Resumen del capítulo: Redes neuronales convolucionales
14
entrenamiento, hay que especificar el número de lotes de conjuntos de datos en el 
parámetro steps_per_epoch :
model.fit(datagen_flow, steps_per_epoch=len(datagen_flow))
El método fit()  debe contener conjuntos de entrenamiento y validación. Para esto, 
hay que crear dos generadores de datos para cada conjunto.
# indica que el conjunto de validación contiene 
# 25% de objetos aleatorios 
datagen = ImageDataGenerator(validation_split=0.25) 
 
train_datagen_flow = datagen.flow_from_directory( 
    '/datasets/fruits_small/', 
    target_size=(150, 150), 
    batch_size=16, 
    class_mode='sparse', 
    # indica que este es el generador de datos para el conjunto de entrenamiento 
    subset='training', 
    seed=54321) 
 
val_datagen_flow = datagen.flow_from_directory( 
    '/datasets/fruits_small/', 
    target_size=(150, 150), 
    batch_size=16, 
    class_mode='sparse', 
    # indica que este es el generador de datos para el conjunto de validación 
    subset='validation', 
    seed=54321)
Ahora se inicia el entrenamiento así:
model.fit(train_datagen_flow, 
          validation_data=val_datagen_flow, 
          steps_per_epoch=len(train_datagen_flow), 
          validation_steps=len(val_datagen_flow))
Aumento de datos de imágenes
Si hay muy pocas muestras de entrenamiento, la red podría sobreentrenarse. El 
aumento se utiliza para expandir artificialmente un conjunto de datos mediante la 
transformación de las imágenes existentes y evitar el sobreentrenamiento. Los cambios 
Resumen del capítulo: Redes neuronales convolucionales
15
solo se aplican a los conjuntos de entrenamiento, mientras que los de prueba y 
validación se quedan igual.
El aumento transforma la imagen original, pero aún preserva sus características 
principales. Por ejemplo, se puede rotar o reflejar la imagen.
Hay varios tipos de aumento:
Rotar
Reflejar
Cambiar el brillo y el contraste
Estirar y comprimir
Difuminar y enfocar
Agregar ruido
Puedes aplicar más de un tipo de aumento a una imagen.
Sin embargo, el aumento puede causar problemas. Por ejemplo, la clase de la imagen 
puede cambiar o el resultado podría terminar como una pintura impresionista debido a 
las alteraciones. Esto afecta la calidad del modelo.
Puedes evitar estos problemas si sigues estas recomendaciones:
No hagas aumento en los conjuntos de prueba y validación para evitar distorsionar 
los valores de las métricas.
Agrega aumentos de forma gradual, uno a la vez, y pon atención a la métrica de 
calidad del conjunto de validación.
Siempre deja sin cambiar algunas imágenes del conjunto de datos.
Aumentos en Keras
En ImageDataGenerator hay muchas maneras de agregar aumentos de imagen. Por 
defecto, están deshabilitadas. Hagamos un giro vertical:
datagen = ImageDataGenerator(validation_split=0.25, 
                             rescale=1./255, 
                             vertical_flip=True)
Resumen del capítulo: Redes neuronales convolucionales
16
Deben crearse diferentes generadores para los conjuntos de entrenamiento y 
validación:
train_datagen = ImageDataGenerator( 
    validation_split=0.25, 
    rescale=1./255, 
    horizontal_flip=True) 
 
validation_datagen = ImageDataGenerator( 
    validation_split=0.25, 
    rescale=1./255) 
 
train_datagen_flow = train_datagen.flow_from_directory( 
    '/dataset/', 
    target_size=(150, 150), 
    batch_size=16, 
    class_mode='sparse', 
    subset='training', 
    seed=54321) 
 
val_datagen_flow = validation_datagen.flow_from_directory( 
    '/dataset/', 
    target_size=(150, 150), 
    batch_size=16, 
    class_mode='sparse', 
    subset='validation', 
    seed=54321)
Configura los objetos train_datagen_flow y val_datagen_flow al mismo valor de seed  
para evitar que estos conjuntos compartan elementos.
ResNet en Keras
Importa ResNet50 desde Keras (50 indica el número de capas de la red).
from tensorflow.keras.applications.resnet import ResNet50 
 
model = ResNet50(input_shape=None, 
             classes=1000, 
             include_top=True, 
              weights='imagenet')
Echemos un vistazo a los argumentos:
input_shape : tamaño de la imagen de input. Por ejemplo, (640, 480, 3) .
Resumen del capítulo: Redes neuronales convolucionales
17
classes=1000 : el número de neuronas en la última capa totalmente conectada donde 
sucede la clasificación.
weights='imagenet' : la inicialización de pesos. ImageNet es el nombre de una gran 
base de datos de imágenes que se usaba al entrenar la red para que clasificara 
imágenes en 1000 clases. Si comienzas a entrenar la red en ImageNet y luego 
continúas con la tarea, el resultado será mucho mejor que si solo entrenaras la red 
desde cero. Escribe weights=None  para inicializar los pesos al azar.
include_top=True : indica que hay dos capas al final de ResNet 
(GlobalAveragePooling2D y Dense). Si la configuras en False, estas dos capas no 
estarán presentes.
Vamos a repasar las capas mencionadas anteriormente:
1. GlobalAveragePooling2D funciona como ventana para todo el tensor. Como 
AveragePooling2D, devuelve el valor promedio de un grupo de pixeles dentro de un 
canal. Usamos GlobalAveragePooling2D para tomar el promedio de la información a 
través de la imagen con el objetivo de obtener un pixel con mayor número de canales 
(por ejemplo, 512 para ResNet50).
2. Dense es la capa totalmente conectada responsable de la clasificación.
Aprendamos cómo usar una red que ha sido entrenada previamente en ImageNet. Para 
adaptar ResNet50 a nuestra tarea, vamos a quitarle la parte superior y reconstruirla.
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense 
from tensorflow.keras.models import Sequential 
 
backbone = ResNet50(input_shape=(150, 150, 3), 
                    weights='imagenet',  
                    include_top=False) 
 
model = Sequential() 
model.add(backbone) 
model.add(GlobalAveragePooling2D()) 
model.add(Dense(12, activation='softmax'))
Backbone  (red troncal) es lo que queda de ResNet50.
Pero aquí está la cosa: digamos que hay un conjunto de datos muy pequeño que solo 
contiene 100 imágenes y dos clases. Si entrenas ResNet50 en este conjunto de datos, 
puedes tener la certeza de que se va a entrenar en exceso porque tiene demasiados 
Resumen del capítulo: Redes neuronales convolucionales
18
parámetros (¡cerca de 23 millones!). La red acabará por obtener predicciones perfectas 
en el conjunto de entrenamiento, pero también tendrá predicciones aleatorias en el 
conjunto de prueba.
Para evitar esto, tenemos que "congelar" una parte de la red: algunas capas se 
quedarán con pesos de ImageNet y no entrenarán con descenso de gradiente. Vamos 
a entrenar una o dos capas totalmente conectadas en la parte superior de la red. De 
esta manera, se reducirá el número de parámetros de la red, pero se conservará la 
arquitectura.
Congelemos la red:
backbone = ResNet50(input_shape=(150, 150, 3), 
                    weights='imagenet',  
                    include_top=False) 
 
# congela ResNet50 sin la parte superior 
backbone.trainable = False 
 
model = Sequential() 
model.add(backbone) 
model.add(GlobalAveragePooling2D()) 
model.add(Dense(12, activation='softmax'))
No congelamos la capa totalmente conectada encima de backbone para que la red 
pudiera aprender.
Congelar la red te permite evitar el exceso de entrenamiento e incrementar la tasa de 
aprendizaje de la red (el descenso de gradiente no necesitará contar derivadas para las 
capas congeladas).

### Resumen_del_captulo_Redes_totalmente_conectadas.pdf ###
Resumen del capítulo: Redes totalmente conectadas
1
Resumen del capítulo: Redes 
totalmente conectadas
Tareas con redes neuronales
Supongamos que la observación es una foto y cada pixel es una característica de esa 
observación.
Aquí hay un ejemplo: la imagen de la izquierda es una imagen en blanco y negro de 
baja resolución. La del medio es la misma imagen, pero con tonos de gris de 0 (negro) 
a 255 (blanco). El rango de valores de brillo de pixeles es 0-255, es decir, el posible 
valor de característica. La imagen de la derecha contiene solo los números del rango 
sin la imagen.
Vamos a convertir los valores de pixel en un vector para obtener nuestras 
características:
[255, 255, 255, 255, 237, 217, 239, 255, 255, 255, 255, 255, 255, 255, 255, 190, 75, 29, 2
9, 30, 81, 198, 255, 255, 255, 255, 255, 147, 30, 29, 29, 29, 29, 29, 31, 160, 255, 255, 2
55, 185, 29, 29, 29, 29, 29, 29, 29, 29, 31, 198, 255, 255, 61, 29, 29, 29, 29, 29, 29, 2
9, 29, 29, 74, 255, 255, 108, 121, 121, 121, 121, 121, 121, 121, 121, 121, 102, 219, 255,
 250, 255, 255, 255, 255, 255, 255, 255, 255, 255, 238, 107, 168, 255, 238, 153, 150, 152, 
Resumen del capítulo: Redes totalmente conectadas
2
244, 201, 152, 150, 178, 253, 103, 144, 248, 243, 121, 108, 114, 225, 184, 130, 112, 154,
 235, 103, 62, 197, 255, 227, 168, 231, 149, 230, 196, 179, 251, 183, 29, 29, 105, 255, 25
5, 219, 195, 191, 184, 195, 235, 255, 91, 29, 29, 30, 187, 255, 234, 218, 218, 218, 218, 2
43, 174, 29, 29, 29, 29, 38, 180, 255, 255, 255, 255, 255, 169, 35, 29, 29, 29, 29, 29, 2
9, 82, 153, 174, 150, 76, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,
 29, 29]
Si el tamaño de cada imagen del conjunto de datos es de 1920x1080 pixeles, entonces 
cada imagen se describe mediante 2,073,600 características (1920 multiplicado por 
1080). Los algoritmos clásicos como la potenciación del gradiente no pueden 
administrar el entrenamiento con tantas funciones.
Veamos qué tienen en común las imágenes y los textos:
1. Ambos tienen información redundante.
2. Las características vecinas están relacionadas entre sí.
Librería Keras
Hablemos de Keras, una librería de red neuronal de código abierto. De hecho, Keras 
es más una interfaz para trabajar con otra biblioteca más compleja, TensorFlow. Otra 
librería popular de redes neuronales es PyTorch, la cual ya conoces. Es posible que te 
haya parecido fácil trabajar con esta, ya que usaste un modelo previamente entrenado. 
De todas formas, tanto TensorFlow como PyTorch son difíciles para los programadores 
principiantes.
Vamos a escribir una regresión lineal en Keras. Una regresión lineal es una red 
neuronal, pero con una sola neurona:
# importa Keras 
from tensorflow import keras 
 
# crea el modelo 
model = keras.models.Sequential() 
# indica cómo está organizada la red neuronal 
model.add(keras.layers.Dense(units=1, input_dim=features.shape[1])) 
# indica cómo está entrenada la red neuronal 
model.compile(loss='mean_squared_error', optimizer='sgd') 
 
# entrena el modelo 
model.fit(features, target)
Resumen del capítulo: Redes totalmente conectadas
3
Analicemos el código línea por línea. La primera línea importa Keras desde la librería 
tensorflow. En la plataforma, usamos TensorFlow v2.1.0.
from tensorflow import keras
La siguiente línea inicializa el modelo (es decir, la red neuronal que construiremos). 
Establezcamos la clase del modelo en Sequential . Esta clase se utiliza para modelos 
con capas secuenciales. Una capa es un conjunto de neuronas que comparten la 
misma entrada y salida.
model = keras.models.Sequential()
Nuestra red consistirá en una sola neurona (o el valor en una salida). Tiene n entradas, 
cada una multiplicada por su propio peso. Por ejemplo, x₁ se multiplica por w₁. Hay una 
entrada más y siempre es igual a la unidad. Su peso se designa como b (sesgo). Lo 
que constituye el proceso de entrenamiento de la red neuronal es la selección de pesos 
w y b. Después de sumar todos los productos de los valores de las entradas y los 
pesos, la respuesta de la red neuronal (a) se envía a la salida.
Resumen del capítulo: Redes totalmente conectadas
4
El comando keras.layers.Dense()  crea una capa de neuronas. "Dense" (densa) significa 
que cada entrada estará conectada a cada neurona o salida. El parámetro units  
establece la cantidad de neuronas en la capa, mientras que input_dim  establece la 
cantidad de entradas en la capa. Observa que este parámetro no considera el sesgo.
Para crear una capa para nuestra red, escribe:
# toma el número de entradas del conjunto de entrenamiento 
keras.layers.Dense(units=1, input_dim=features.shape[1])
La capa totalmente conectada establecida por el comando keras.layers.Dense(units=2, 
input_dim=3)  se ve así:
Resumen del capítulo: Redes totalmente conectadas
5
Para agregar la capa totalmente conectada al modelo, llama al método model.add() :
model.add(keras.layers.Dense(units=1, input_dim=features.shape[1]))
Echa un vistazo a esta línea:
 model.compile(loss='mean_squared_error', optimizer='sgd') 
Lo que hace es preparar el modelo para el entrenamiento. Después de este comando, 
la estructura de la red ya no se puede cambiar. Especifica el ECM como la función de 
pérdida de la tarea de regresión para el parámetro loss . Establece el método de 
descenso de gradiente para el parámetro optimizer='sgd' . Recuerda, las redes 
neuronales se entrenan con SGD.
Ahora vamos a ejecutar el entrenamiento:
Resumen del capítulo: Redes totalmente conectadas
6
model.fit(features, target)
Regresión logística
La regresión lineal es una red neuronal con una sola neurona. Se puede decir lo mismo 
sobre la regresión logística. Si las observaciones tienen solo dos clases, la diferencia 
entre la regresión lineal y la regresión logística es casi imperceptible. Necesitamos 
agregar un elemento adicional.
Así es como se ve la regresión lineal:
Y así es como se ve la regresión logística:
Resumen del capítulo: Redes totalmente conectadas
7
El último diagrama tiene la función sigmoide o función de activación, que toma 
cualquier número real como entrada y devuelve un número en el rango de 0 (sin 
activación) a 1 (activación).
En la fórmula, e es el número de Euler, que es aproximadamente igual a 2.718281828.
Este número en el rango de 0 a 1 se puede interpretar como una predicción de una red 
neuronal sobre si la observación pertenece a la clase positiva o a la clase negativa.
Resumen del capítulo: Redes totalmente conectadas
8
Si la suma de los productos de los valores de las entradas y los pesos (z) es muy 
grande, entonces, en la salida sigmoide, obtenemos un número cercano a la unidad:
Resumen del capítulo: Redes totalmente conectadas
9
Pero si, por el contrario, la suma es un número negativo grande, entonces la función 
devuelve un número cercano a cero:
Resumen del capítulo: Redes totalmente conectadas
10
La función de pérdida varía dependiendo del tipo de red neuronal. El ECM se usa en 
tareas de regresión, mientras que la entropía cruzada binaria (BCE) es la adecuada 
para una clasificación binaria. No podemos usar la métrica de exactitud porque no tiene 
un producto, lo cual hace que sea imposible trabajar para SGD.
La BCE se calcula de la siguiente manera:
En la fórmula, p es la probabilidad de respuesta correcta. La base del logaritmo no 
importa porque el cambio de la base es la multiplicación de la función de pérdida por la 
constante, que no cambia el mínimo.
Resumen del capítulo: Redes totalmente conectadas
11
Si el objetivo = 1, entonces la probabilidad de respuesta correcta es:
Si el objetivo = 0, entonces p es:
Para comprender mejor la función BCE, observa su gráfico:
Resumen del capítulo: Redes totalmente conectadas
12
Si la probabilidad de respuesta correcta p es aproximadamente igual a la unidad, 
entonces log(p) es un número positivo cercano a cero. Por lo tanto, el error es 
pequeño. Si la probabilidad de respuesta correcta p≈0, entonces log(p) es un número 
positivo grande. Por lo tanto, el error también es grande.
Regresión logística en Keras
Para obtener una regresión logística, solo necesitamos cambiar el código de regresión 
lineal en dos lugares:
1. Aplica la función de activación a la capa totalmente conectada:
keras.layers.Dense(units=1, input_dim=features_train.shape[1],  
                   activation='sigmoid')
2. Cambia la función de pérdida del ECM a binary_crossentropy :
Resumen del capítulo: Redes totalmente conectadas
13
model.compile(loss='binary_crossentropy', optimizer='sgd')
Redes neuronales totalmente conectadas
Es hora de ver más de cerca las redes neuronales totalmente conectadas. En estas 
redes, las neuronas en cada capa están conectadas con las de las capas previas.
A continuación se muestra un ejemplo de una red totalmente conectada. Debes tener 
en cuenta que todas las capas, excepto las capas de entrada y salida, se llaman capas 
ocultas:
Vamos a analizar la estructura de esta red totalmente conectada. Cada neurona, 
excepto la última, es seguida por una función de activación. ¿Te preguntas por qué? 
Echa un vistazo al ejemplo de dicha red sin el peso b:
Resumen del capítulo: Redes totalmente conectadas
14
Para obtener z, suma todos los productos de los valores de entrada y los pesos:
z1 = x1 * w11 + x2 * w12 
z2 = x1 * w21 + x2 * w22 
z3 = z1 * w31 + z2 * w32
Obtenemos:
z3 = (x1 * w11 + x2 * w12) * w31 + (w21 * x1 + w22 * x2) * w32
Saca a x₁ y x₂ de los corchetes:
z3 = x1 * (w11 * w31 + w21 * w32) + x2 * (w12 * w31 + w22 * w32)
Por comodidad, usaremos una nueva notación, w₁ y w₂:
w1' = w11 * w31 + w21 * w32  
w2' = w12 * w31 + w22 * w32
Resumen del capítulo: Redes totalmente conectadas
15
Obtenemos:
z3 = x1 * w1' + x2 * w2' 
Aquí está el diagrama de esta fórmula:
¿Te parece familiar? ¡Una red multicapa es una red de una sola neurona!
Para solucionar esto, vamos a reintroducir el sigmoide y ver cómo cambia la red:
Resumen del capítulo: Redes totalmente conectadas
16
Escribe la fórmula para esto:
z1 = σ(x1 * w11 + x2 * w12) 
z2 = σ(x1 * w21 + x2 * w22) 
z3 = z1 * w31 + z2 * w32
Obtenemos:
z3 =  σ(x1 * w11 + x2 * w12) * w31 + σ(w21 * x1 + w22 * x2) * w32
Debido a los sigmoides, no podemos sacar a x₁ y x₂ de los corchetes. Esto significa que 
ahora no estamos tratando con una sola neurona. Los sigmoides permiten hacer que la 
red sea más compleja.
Cómo se entrenan las redes neuronales
De forma similar a la regresión lineal, las redes multicapa están entrenadas con un 
descenso de gradiente. Hay parámetros que son los pesos de las neuronas en cada 
Resumen del capítulo: Redes totalmente conectadas
17
capa totalmente conectada. Y el objetivo de entrenamiento es encontrar los parámetros 
que dan como resultado la función de pérdida mínima.
¿Cómo cambia una red neuronal cuando agregamos neuronas y capas? Para 
averiguarlo, resuelve algunas tareas en el sitio web TensorFlow Playground 
(materiales en inglés). Esta plataforma te permite entrenar pequeñas redes neuronales 
en datos de modelo con dos características para tareas de clasificación.
Puedes elegir el conjunto de datos en la parte izquierda de la interfaz TensorFlow 
Playground. En el medio tienes la estructura de tu red y en el lado derecho tienes el 
resultado del entrenamiento. Puedes controlar el proceso de entrenamiento del modelo 
desde el panel superior y cambiar el valor de la época, la tasa de aprendizaje o la 
función de activación.
Observa esta red. Tiene muchas capas y una función de activación sigmoide. Intenta 
entrenar la red. Verás que no aprenderá sin importar el valor de paso que establezcas. 
Vamos a averiguar por qué.
A medida que aumenta el número de capas, el entrenamiento se vuelve menos 
eficiente. Entre más capas haya en la red, menos señal habrá de la entrada a la salida 
de la red. Esto se llama señal de fuga y es causada por el sigmoide, que convierte 
grandes valores en más pequeños una y otra vez.
Para deshacerte del problema puedes intentar otra función de activación, por ejemplo, 
ReLU (unidad lineal rectificada). Así es como se ve:
Resumen del capítulo: Redes totalmente conectadas
18
Y aquí está la fórmula de ReLU:
ReLU lleva todos los valores negativos a 0 y deja los valores positivos sin cambios.
Cambia la función de activación de sigmoide a ReLU en esta red (materiales en inglés). 
Asegúrate de que ahora la red neuronal esté aprendiendo correctamente. Debido a que 
se cambió la función de activación, el área que separa los puntos es poligonal. La 
forma depende de cómo se inicializaron los pesos de la red.
Redes neuronales totalmente conectadas en Keras
Resumen del capítulo: Redes totalmente conectadas
19
Las capas totalmente conectadas en Keras se pueden crear llamando a Dense(). Para 
construir una red multicapa totalmente conectada, debes agregar una capa totalmente 
conectada varias veces. Deja la primera capa oculta con diez neuronas y así la 
segunda capa de salida tendrá una neurona.
model = keras.models.Sequential() 
model.add(keras.layers.Dense(units=10, input_dim=features_train.shape[1],  
                             activation='sigmoid')) 
model.add(keras.layers.Dense(units=1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['acc']) 
 
model.fit(features_train, target_train, epochs=10, verbose=2, 
          validation_data=(features_valid, target_valid))
Trabajar con imágenes en Python
Ya sabes que las imágenes son conjuntos de números. Si la imagen es en blanco y 
negro, cada pixel almacena un número de 0 (negro) a 255 (blanco).
Vamos a abrir esta imagen con las herramientas de la librería PIL (Python Imaging 
Library). Podremos trabajar con ella como una matriz NumPy:
import numpy as np 
from PIL import Image 
Resumen del capítulo: Redes totalmente conectadas
20
 
image = Image.open('image.png') 
image_array = np.array(image) 
print(image_array)
[[255 255 255 255 237 217 239 255 255 255 255 255 255] 
 [255 255 190  75  29  29  30  81 198 255 255 255 255] 
 [255 147  30  29  29  29  29  29  31 160 255 255 255] 
 [185  29  29  29  29  29  29  29  29  31 198 255 255] 
 [ 61  29  29  29  29  29  29  29  29  29  74 255 255] 
 [108 121 121 121 121 121 121 121 121 121 102 219 255] 
 [250 255 255 255 255 255 255 255 255 255 238 107 168] 
 [255 238 153 150 152 244 201 152 150 178 253 103 144] 
 [248 243 121 108 114 225 184 130 112 154 235 103  62] 
 [197 255 227 168 231 149 230 196 179 251 183  29  29] 
 [105 255 255 219 195 191 184 195 235 255  91  29  29] 
 [ 30 187 255 234 218 218 218 218 243 174  29  29  29] 
 [ 29  38 180 255 255 255 255 255 169  35  29  29  29] 
 [ 29  29  29  82 153 174 150  76  29  29  29  29  29] 
 [ 29  29  29  29  29  29  29  29  29  29  29  29  29]]
¡Obtenemos una matriz bidimensional!
Llama a plt.imshow()  (muestra de imagen) para trazar la imagen.
plt.imshow(image_array)
Puedes trazar la imagen en blanco y negro agregando el argumento cmap='gray' . Para 
agregar una barra de color a la imagen, debes llamar a plt.colorbar() .
plt.imshow(image_array, cmap='gray') 
plt.colorbar()
Por lo general, las redes neuronales aprenden mejor cuando reciben imágenes en el 
rango de 0 a 1 como entrada. Para llevar la escala [0, 255] a [0, 1], divide todos los 
valores de la matriz bidimensional entre 255.
image_array = image_array / 255.
Resumen del capítulo: Redes totalmente conectadas
21
Imágenes en color
Las imágenes en color o imágenes RGB consisten en tres canales: rojo, verde y azul. 
De hecho, estas imágenes son matrices tridimensionales con celdas que contienen 
números enteros en el rango de 0 a 255.
En NumPy, las matrices tridimensionales trabajan de la misma forma que las 
bidimensionales.
Compara cómo se crean:
np.array([[0, 255], 
          [255, 0]])
np.array([[[0, 255, 0], [128, 0, 255]],  
          [[12, 89, 0], [5,  89, 245]]])
En una matriz tridimensional obtenida de una imagen, todo es casi igual. La primera 
coordenada es la ID de fila y la segunda es la ID de columna. Sin embargo, aquí 
también tenemos una nueva tercera coordenada, la cual indica el canal.
Resumen del capítulo: Redes totalmente conectadas
22
Entonces, una matriz tridimensional es como una matriz bidimensional de una imagen 
en blanco y negro. La única diferencia es que cada pixel de dicha matriz almacena tres 
números que representan el brillo de cada uno de los tres canales: rojo, verde y azul.
Clasificación multiclase
Vamos a explorar la clasificación multiclase y ver cómo funciona. Esta clasificación 
implica que las observaciones pertenecen a una de varias clases en vez de a una de 
dos clases.
Supongamos que tenemos tres clases. Aquí está la regresión logística representada 
como una red neuronal:
Obtenemos una red totalmente conectada con la capa de salida que contiene no una, 
sino tres neuronas. Cada neurona es responsable de su propia clase. Si el valor en la 
salida z1 es un número positivo grande, la red establecerá la observación en la clase 
"1".
Resumen del capítulo: Redes totalmente conectadas
23
¿Cómo calculamos la función de pérdida? Recuerda la entropía cruzada binaria:
Si el valor objetivo es 1, entonces la probabilidad de respuesta correcta es:
Si el valor objetivo es 0, el valor p es:
Tenemos tres clases, pero esto no afectará el cálculo de la función de pérdida. Y se 
llamará CE (entropía cruzada):
En la fórmula, p es la probabilidad de respuesta correcta devuelta por la red.
¿De dónde sacamos la probabilidad? Antes los sigmoides la obtenían. ¿Qué pasa si 
ponemos un sigmoide después de cada neurona en la capa de salida?
Resumen del capítulo: Redes totalmente conectadas
24
p_1 (first class probability) = σ(z_1) 
p_2 (second class probability) = σ(z_2) 
p_3 (third class probability) = σ(z_3)
Todas las probabilidades están en el rango de 0 a 1, pero la suma de las tres no 
necesariamente es igual a la unidad. Si suponemos que la observación pertenece a 
una sola clase, esperamos obtener esto:
p_1 + p_2 + p_3 = 1
La función de activación que se adapta a este caso se llama SoftMax, que toma varias 
salidas de las redes y devuelve probabilidades que son todas iguales a uno.
Así es como calculamos las probabilidades:
p1 = SoftMax(z1) = e^z1 / (e^z1 + e^z2 + e^z3) 
p2 = SoftMax(z2) = e^z2 / (e^z1 + e^z2 + e^z3) 
p3 = SoftMax(z3) = e^z3 / (e^z1 + e^z2 + e^z3)
Ahora p₁ varía en el rango de 0 a 1.
Ten en cuenta que si z₁ es significativamente mayor que z₂ y z₃, entonces en la fórmula 
e^z1 / (e^z1 + e^z2 + e^z3)  el numerador es aproximadamente igual al denominador, es 
decir, p₁≈1 .
Si z₁ es significativamente menor que z₂ o z₃, entonces en la fórmula e^z1 / (e^z1 + e^z2 
+ e^z3)  el numerador es mucho menor que el denominador, es decir, p₁ ≈0.
Ahora las probabilidades son iguales a uno:
Resumen del capítulo: Redes totalmente conectadas
25
p1 + p2 + p3 = SoftMax(z1) + SoftMax(z2) + SoftMax(z3) = 
 
= e^z1 / (e^z1 + e^z2 + e^z3) + e^z2 / (e^z1 + e^z2 + e^z3) + e^z3 / (e^z1 + e^z2 + e^z3)
 = 
 
= (e^z1 + e^z2 + e^z3) / (e^z1 + e^z2 + e^z3) = 1
Aquí está el diagrama de nuestras redes neuronales con la función de activación 
SoftMax:
¿Por qué el bloque SoftMax del diagrama depende de todas las salidas de la red? 
Porque realmente necesitamos todos los resultados para encontrar todas las 
probabilidades.
Si hay más de tres clases, el número de neuronas en la capa de salida debe ser igual 
al número de clases, y todas las salidas deben estar conectadas a SoftMax.
Resumen del capítulo: Redes totalmente conectadas
26
Las probabilidades de SoftMax en la etapa de entrenamiento pasarán a la entropía 
cruzada, la cual calculará el error. La función de pérdida se minimizará utilizando el 
método de descenso de gradiente. La única condición para que funcione el descenso 
de gradiente es que la función tenga una derivada para todos los parámetros: pesos y 
sesgo de la red neuronal.
Veamos cómo cambia el código. Esta es la inicialización de la última capa para la 
clasificación binaria:
Dense(units=1, activation='sigmoid'))
Y esta es la inicialización para la clasificación multiclase:
Dense(units=3, activation='softmax'))

### Resumen_del_captulo_Regresin_lineal_desde_adentro.pdf ###
Resumen del capítulo: Regresión lineal desde adentro
1
Resumen del capítulo: Regresión 
lineal desde adentro
Modelo de regresión lineal
En la regresión lineal, las características son un vector de números en un espacio n-
dimensional (digamos x). La predicción del modelo (a) se calcula de la siguiente 
manera: el vector de características es un escalar multiplicado por el vector de peso 
(w), luego el valor de sesgo de la predicción se suma a este producto:
El vector 
 y un escalar 
 son parámetros del modelo. Hay n parámetros en el vector 
 y uno en 
. En otras palabras, el número de parámetros es mayor que la longitud 
del vector de características por uno.
Si la longitud del vector de características es igual a uno, entonces solo hay una 
característica en la muestra. Dibujemos esta característica con las respuestas en el 
gráfico:
w
w0
w
w0
Resumen del capítulo: Regresión lineal desde adentro
2
Los gráficos de predicción para la regresión lineal se establecen mediante la ecuación:
Si cambias los parámetros 
 y 
, obtendrás cualquier línea recta (de ahí que el 
modelo tome su nombre):
w
w0
Resumen del capítulo: Regresión lineal desde adentro
3
Objetivo de entrenamiento
Necesitamos analizar el algoritmo de aprendizaje. Nuestra métrica de calidad será 
ECM: el modelo debe alcanzar su valor más bajo en los datos de prueba. El objetivo de 
la tarea de entrenamiento se formula de la siguiente manera: encontrar los parámetros 
del modelo para los cuales el valor de la función de pérdida sea mínimo en el conjunto 
de entrenamiento. Como métrica de calidad, toma respuestas y predicciones correctas 
como entrada. Devuelve valores que representan "pérdidas" (deben minimizarse). En 
nuestra tarea, esta función se iguala al ECM. Pero, por lo general, la función de pérdida 
se usa para el entrenamiento mientras que la métrica de calidad se usa para las 
pruebas.
Vamos a escribir el objetivo de la tarea de entrenamiento en formato vectorial. El 
conjunto de entrenamiento se representa como matriz 
, en ella, las filas corresponden 
a objetos y las columnas corresponden a características. Denotemos los parámetros de 
X
Resumen del capítulo: Regresión lineal desde adentro
4
regresión lineal como 
 y 
. Para obtener el vector de predicción a, multiplica la 
matriz 
 por el vector 
 y agrega el valor de sesgo de predicción 
.
La fórmula es:
Para acortarla, vamos a cambiar la notación. En la matriz 
, agrega una columna que 
consista solo en unos (será la columna 0); y el parámetro 
 se suma al vector 
:
Luego multiplica la matriz 
 por el vector 
. El sesgo de predicción se multiplica por 
un vector de unos (columna cero). Obtenemos el vector de predicción resultante a:
Ahora podemos introducir una nueva notación y: el vector de valores de características 
objetivo para el conjunto de entrenamiento.
w
w0
X
w
w0
X
w0
w
X
w
Resumen del capítulo: Regresión lineal desde adentro
5
Escribe la fórmula para entrenar la regresión lineal de la función de pérdida del ECM.
La función argmin()  encuentra el mínimo y devuelve los índices en los que este se 
alcanzó.
Matriz inversa
Una matriz identidad (también conocida como matriz unitaria) es una matriz cuadrada 
con unos en la diagonal principal y ceros en el resto. Si cualquier matriz 
 se multiplica 
por una matriz identidad (o viceversa), obtendremos la misma matriz 
:
La matriz inversa para una matriz cuadrada A es una matriz 
 con un superíndice -1 
cuyo producto con A es igual a la matriz identidad. La multiplicación se puede realizar 
en cualquier orden:
Las matrices para las que puedes encontrar inversas se llaman matrices invertibles. 
Sin embargo, no todas las matrices tienen una inversa. Esta matriz se llama matriz no 
A
A
A
Resumen del capítulo: Regresión lineal desde adentro
6
invertible.
Las matrices no invertibles son poco comunes. Si generas una matriz aleatoria con la 
función numpy.random.normal() , la probabilidad de obtener una matriz no invertible es 
cercana a cero.
Para encontrar la matriz inversa, llama a la función numpy.linalg.inv() . También te 
ayudará a verificar la invertibilidad de la matriz: si la matriz no es invertible, se detectará 
un error.
Entrenamiento de una regresión lineal
La tarea de entrenar la regresión lineal es:
El valor mínimo del ECM se obtiene cuando los pesos son iguales a este valor:
¿Cómo obtuvimos esta fórmula?
La matriz de características transpuesta se multiplica por sí misma.
Se calcula la matriz inversa a ese resultado.
La matriz inversa se multiplica por la matriz de características transpuesta.
El resultado se multiplica por el vector de los valores objetivo.

### Resumen_del_captulo_Relaciones_entre_tablas.pdf ###
Resumen del capítulo: Relaciones entre tablas
1
Resumen del capítulo: 
Relaciones entre tablas
Tipos de relaciones entre tablas
Cuando una columna contiene los valores del campo de otra tabla, se le llama clave 
foránea. Es responsable de la relación entre las tablas.
Hay tres tipos de relaciones:
una a una
una a muchas
muchas a muchas
En una relación una a una, cada fila de una tabla está conectada con una y solo una 
fila de la otra tabla. Es como si una tabla estuviera dividida en dos. Este es un tipo raro 
de relación y se usa predominantemente por razones de seguridad.
En una relación de una a muchas, cada fila de una tabla coincide con varias filas de 
otra tabla.
En una relación de muchas a muchas, varias filas de una tabla coinciden con varias 
filas de otra tabla. Este tipo de relación produce una tabla de asociación, que combina 
las claves primarias de ambas tablas.
Diagramas ER
Resumen del capítulo: Relaciones entre tablas
2
La estructura de las bases de datos se puede mostrar con diagramas ER (entidad-
relación). Los diagramas muestran tablas y las relaciones entre ellas.
Las tablas se muestran como rectángulos (cajas) con dos partes. El nombre de la tabla 
va en la parte superior.
En la parte inferior vemos listas de campos de la tabla con una indicación de claves con 
las que están relacionados: primarias o foráneas. Las claves suelen estar marcadas 
como PK (primaria) o FK (foránea), pero también pueden estar marcadas con un icono 
de llave, un # u otro símbolo.
Los diagramas ER también muestran relaciones. El final de la línea que conecta dos 
tablas indica si uno o varios valores de una tabla coinciden con los valores de la otra.
Aquí hay una relación de una a muchas:
Resumen del capítulo: Relaciones entre tablas
3
Y aquí hay una relación de una a una:
Resumen del capítulo: Relaciones entre tablas
4
Tipos de usuarios de bases de datos
Muchos empleados están utilizando la base de datos maestra de una empresa al 
mismo tiempo. Cada uno de ellos necesita solo ciertos datos para hacer su trabajo. Por 
lo tanto, las cosas deben organizarse de tal manera que los usuarios no interfieran en 
el trabajo de los demás. De ahí la necesidad de administradores de bases de datos. 
Ellos gestionan el acceso de los usuarios, controlan la carga en el sistema, se 
encargan de la seguridad y hacen copias de seguridad.
Las bases de datos son como criaturas vivas que crecen constantemente. 
Los arquitectos y desarrolladores de bases de datos se aseguran de que estas 
crezcan de manera saludable. Las decisiones de estos especialistas determinan la 
estructura, integridad y plenitud de la base de datos, así como sus posibilidades de 
escalado (añadiendo nuevas tablas, relaciones y funciones). Los arquitectos y 
desarrolladores son responsables del rendimiento de la base de datos.
Los ingenieros de datos son responsables de agregar datos a la base de datos. 
También se les llama especialistas en ETL, ya que extraen, transforman y cargan datos 
en bases de datos.
Los analistas, en este esquema, son usuarios típicos. Escriben consultas a bases de 
datos y recuperan los datos necesarios, los que luego analizan y utilizan para probar 
Resumen del capítulo: Relaciones entre tablas
5
hipótesis. Los analistas trabajan más de cerca que los demás con los datos, y son los 
primeros en encontrar campos o tablas faltantes. Acostúmbrate a comunicar de 
inmediato tales "descubrimientos" a los desarrolladores, si deseas que dichos errores 
se resuelvan rápidamente.
Buscar valores vacíos
En SQL, se dice que las celdas vacías son NULL. El operador IS NULL los busca:
SELECT  
 *  
FROM  
 table_name 
WHERE  
 column_name IS NULL;
¡Ten en cuenta que IS importa! Lo siguiente no funcionará:
SELECT 
 *  
FROM  
 table_name 
WHERE  
 column_name = NULL; -- ¡este código no cumplirá!
Para excluir filas con valores NULL de la selección, usamos el operador NOT:
SELECT  
 *  
FROM  
 table_name 
WHERE  
 column_name IS NOT NULL;
La construcción CASE se utiliza para realizar acciones cuando se cumplen ciertas 
condiciones. Es muy parecido a if-elif-else  en Python:
Resumen del capítulo: Relaciones entre tablas
6
CASE 
    WHEN condition_1 THEN result_1 
    WHEN condition_2 THEN result_2 
    WHEN condition_3 THEN result_3 
    ELSE result_4 
END;
Una condición sigue al operador WHEN. Si una fila de la tabla cumple esta condición, 
el código devuelve el resultado indicado en THEN. Si no, la misma fila se prueba con la 
siguiente condición. Si la fila no cumple con ninguna de las condiciones establecidas en 
WHEN, el código devuelve el valor indicado después de ELSE. A continuación, la 
construcción CASE se cierra con el operador END.
Buscar datos en la tabla
El operador LIKE busca en una tabla valores que sigan un patrón dado. Puedes buscar 
no solo una palabra, sino también un fragmento de ella.
Aquí tienes la sintaxis de las declaraciones LIKE:
column_name LIKE 'regular expression'
Indica la columna necesaria antes de LIKE y escribe una expresión regular después.
Las expresiones regulares en SQL son un poco diferentes a las de Python. Por 
ejemplo, el símbolo _  reemplaza un valor de sustitución (1 carácter) en una expresión 
regular. El símbolo %  reemplaza cualquier cantidad de caracteres. Un rango o 
secuencia de caracteres que debe contener una cadena se escribe entre corchetes [] . 
Si se van a excluir los caracteres, se utiliza la construcción [^] .
Rango o secuencia de caracteres
Si necesitamos encontrar un símbolo de una expresión regular como una subcadena, 
usamos el operador ESCAPE. Ha pasado un símbolo como un signo de exclamación. 
En la expresión regular, el signo de exclamación significa que el símbolo que le sigue 
no es parte de la expresión, sino la subcadena que se busca. Aquí hay un fragmento de 
código que encontrará todas las subcadenas que terminan con el símbolo %  (digamos, 
"100%") en una tabla:
Resumen del capítulo: Relaciones entre tablas
7
column_name LIKE '%!%' ESCAPE '!' 
--busca todas las substrings que terminan con %
JOIN. INNER JOIN.
Es raro que todos los datos se almacenen en una tabla. Los analistas generalmente 
necesitan fusionar tablas; de ahí el operador JOIN.
Hay dos formas de unir tablas: la unión INNER y OUTER.
INNER JOIN devuelve solo aquellas filas que tienen valores coincidentes de una tabla 
a otra (la intersección de las tablas).
Resumen del capítulo: Relaciones entre tablas
8
La unión OUTER recupera todos los datos de una tabla y agrega datos de la otra 
cuando hay filas coincidentes.
INNER JOIN 
INNER JOIN selecciona solo los datos para los que se cumplen las condiciones de 
unión. El orden en que se unen las tablas no afecta el resultado final.
Aquí tienes un ejemplo de consulta con INNER JOIN:
SELECT --enlistar solo los campos necesarios 
 TABLE_1.field_1 AS field_1, 
 TABLE_1.field_2 AS field_2, 
 ... 
 TABLE_2.field_n AS field_n 
FROM 
 TABLE_1 
INNER JOIN TABLE_2 ON TABLE_2.field_1 = TABLE_1.field_2;
Vamos a echar un vistazo más de cerca a la sintaxis.
INNER JOIN es el nombre del método de unión. Luego viene el nombre de la tabla 
que se unirá a la tabla del bloque FROM.
ON precede a la condición de unión: TABLE_2.field_1 = TABLE_1.field_2 . Esto significa 
que solo se unirán las filas de la tabla que cumplan esta condición. En nuestro 
Resumen del capítulo: Relaciones entre tablas
9
caso, la condición es que el field_1  de la segunda tabla coincida con el field_2  de 
la primera.
Dado que los campos en diferentes tablas pueden tener los mismos nombres, se hace 
referencia a ellos tanto por el nombre de la tabla como por el nombre del campo. 
Primero viene el nombre de la tabla, luego el campo: TABLE_1.field_1 .
Outer Join. LEFT JOIN
Hay dos tipos de OUTER JOIN:
LEFT OUTER JOIN
RIGHT OUTER JOIN
Daremos a estos métodos nombres cortos: LEFT JOIN y RIGHT JOIN.
LEFT JOIN seleccionará todos los datos de la tabla de la izquierda junto con las filas de 
la tabla de la derecha que cumplen con la condición de unión. RIGHT JOIN hará lo 
mismo, pero para la tabla de la derecha.
Resumen del capítulo: Relaciones entre tablas
10
Aquí está la sintaxis de una declaración con LEFT JOIN:
SELECT  
 TABLE_1.field_1 AS field_1, 
 TABLE_1.field_2 AS field_2, 
 ... 
 TABLE_2.field_n AS field_n 
FROM 
 TABLE_1 
LEFT JOIN TABLE_2 ON TABLE_2.field = TABLE_1.field;
Al igual que con las consultas INNER JOIN, el nombre de la tabla se indica para cada 
campo. Ten en cuenta que con OUTER JOIN, el orden en que se mencionan las tablas 
tiene importancia.
Outer Join. RIGHT JOIN
Resumen del capítulo: Relaciones entre tablas
11
RIGHT JOIN es el gemelo de LEFT JOIN. Pero a diferencia de su hermano, toma todos 
los datos de la tabla de la derecha y las filas correspondientes de la tabla de la 
izquierda.
Así es como se ve la consulta con RIGHT JOIN:
SELECT  
 TABLE_1.field_1 AS field_1, 
 TABLE_1.field_2 AS field_2, 
 ... 
 TABLE_2.field_n AS field_n 
FROM 
 TABLE_1 
RIGHT JOIN TABLE_2 ON TABLE_1.field = TABLE_2.field;
Unir varias tablas
Resumen del capítulo: Relaciones entre tablas
12
Esta es la sintaxis de una consulta que usa INNER JOIN varias veces:
SELECT --enlistar solo los campos necesarios 
 TABLE_1.field_1 AS field_1, 
 TABLE_1.field_2 AS field_2, 
 ... 
 TABLE_3.field_n AS field_n 
FROM 
 TABLE_1 
INNER JOIN TABLE_2 ON TABLE_2.field = TABLE_1.field 
INNER JOIN TABLE_3 ON TABLE_3.field = TABLE_1.field;
Vamos a unir la segunda tabla, después la tercera, a la primera.
Unir declaraciones
Los operadores UNION y UNION ALL reúnen datos de tablas. La sintaxis es la 
siguiente:
SELECT  
 column_name_1   
FROM  
 table_1 
UNION --( o UNION ALL) 
SELECT  
 column_name_1   
FROM  
 table_2;
Aquí dos declaraciones SELECT - FROM están separadas por el comando UNION.
Estas son las condiciones que se deben cumplir para que UNION funcione:
La primera y la segunda tabla deben coincidir con respecto al número de columnas 
seleccionadas y sus tipos de datos
Los campos deben estar en el mismo orden en la primera y la segunda tabla.
UNION evita duplicar filas cuando genera una tabla.
Resumen del capítulo: Relaciones entre tablas
13

### Resumen_del_captulo_SQL_como_herramienta_para_trabajar_con_datos.pdf ###
Resumen del capítulo: SQL como herramienta para trabajar con datos
1
Resumen del capítulo: SQL 
como herramienta para trabajar 
con datos
Bases de datos y tablas
Una base de datos es un lugar donde se almacenan datos estructurados. Las 
entidades son grupos de objetos que comparten características comunes. Los objetos 
son instancias individuales de entidades.
Vamos a estudiar bases de datos relacionales, donde las entidades son tablas y las 
filas de las tablas son sus objetos.
Para trabajar con bases de datos, necesitas un DBMS o sistema de administración de 
bases de datos. Este es un conjunto de programas que te permite crear una base de 
datos, llenarla con tablas nuevas y editar y mostrar el contenido de las tablas 
existentes. En nuestro curso utilizaremos PostgreSQL, uno de los DBMS más 
populares.
Tablas
Una tabla es un conjunto de filas y columnas. Las columnas se llaman campos. 
Dichos campos contienen las características del objeto. Cada campo tiene un nombre 
único y un tipo de datos específico. Las filas de la tabla se denominan tuplas o 
registros. Cada fila contiene información sobre un objeto en particular. Una celda es 
una unidad donde se cruzan una fila y una columna.
Las claves primarias se utilizan para dar a cada fila un identificador único. Algunas 
tablas utilizan varios campos a la vez como claves principales; en tales casos, se 
denominan claves primarias compuestas.
Tu primera instrucción SQL
SQL es un lenguaje informático diseñado para gestionar datos en bases de datos 
relacionales. La sintaxis de SQL es diferente a la de Python. A continuación se 
describen sus características básicas:
Resumen del capítulo: SQL como herramienta para trabajar con datos
2
1) El comienzo de un comentario de una sola línea se marca con dos guiones: --
-- un comentario de una línea en SQL
2) Un comentario de varias líneas se coloca entre /*  y */ : 
/* un comentario de varias líneas 
tiene 
varias 
líneas */
3) Los comandos se escriben en mayúsculas:
SELECT, WHERE, FROM 
4) Cada declaración (o consulta) termina con un punto y coma  ;  : 
SELECT  
 *  
FROM  
 table_name; 
-- Una declaración que solicita todos los datos de la tabla termina con ";" 
SELECT  
 *  
FROM 
 table_name 
WHERE  
 column_name IN (1,7,9); 
-- Una declaración que selecciona datos por condición también termina con ";"
5) Saltos de línea después de cada palabra clave:
SELECT 
 column_1, 
 column_2,  
 column_3,  
 column_4 
FROM  
 table_name 
WHERE  
 column_1 = value_1 AND 
Resumen del capítulo: SQL como herramienta para trabajar con datos
3
 column_2 = value_2 AND 
 column_4 = value_3;
Para seleccionar datos de tablas, debes escribir una declaración o consulta. Una 
declaración es una solicitud escrita de acuerdo con la sintaxis SQL. Tu instrucción debe 
especificar qué datos seleccionar y cómo procesarlos.
El operador SELECT toma la selección que necesitas. Las instrucciones SELECT se 
ven así:
SELECT  
 column_1, 
 column_2,  
 column_3 ...   
FROM  
 table_name; 
-- Seleccionar columnas de la tabla
Tenemos dos palabras clave en nuestra declaración: SELECT y FROM. SELECT 
especifica las columnas necesarias de la tabla de la base de datos. PPara seleccionar 
todas las columnas de la tabla, agrega el símbolo *  al operador SELECT . FROM 
especifica la tabla de la que se deben tomar los datos.
Data Slices en SQL
El comienzo de la condición utilizada para seleccionar datos se marca con el comando 
WHERE. La condición se evalúa en cada fila de la tabla. En condiciones, se utilizan 
operadores de comparación:
SELECT  
 column_1,  
 column_2 -- seleccionar nombres de columna 
FROM  
 table_name -- especificar la tabla 
WHERE  
 condition; -- definir la condición de selección de fila
El orden de los operadores está definido estrictamente:
Resumen del capítulo: SQL como herramienta para trabajar con datos
4
1) SELECT  
2) FROM
3) WHERE
Al igual que Python, SQL utiliza los operadores lógicos AND , OR  y NOT . Estos permiten 
hacer una selección con varias condiciones:
SELECT  
 *  
FROM  
 table_name 
WHERE  
 condition_1 AND condition_2; 
-- Selecciona las filas donde ambas condiciones sean verdaderas 
SELECT  
 *  
FROM  
 table_name 
WHERE  
 condition_1 OR condition_2; 
-- Selecciona las filas donde una o ambas condiciones sean verdaderas 
SELECT  
 *  
FROM  
 table_name 
WHERE  
 condition_1 AND NOT condition_2; 
-- Selecciona las filas donde condition_1 es verdadera y condition_2 es falsa
Si necesitas hacer una selección de filas para las cuales los valores de un campo se 
encuentran dentro de un cierto rango, usa la instrucción BETWEEN. BETWEEN  incluye los 
límites inicial y final en la selección resultante:
SELECT  
 *  
FROM  
 table_name 
WHERE  
 field_1 BETWEEN value_1 AND value_2; 
-- Seleccionar filas donde el valor de field_1 está entre value_1 y value_2 (incluyente)
Si necesitas hacer una selección de filas para las cuales los valores de un campo 
coinciden con los de una lista, usa el operador IN. Indica la lista de valores después de 
Resumen del capítulo: SQL como herramienta para trabajar con datos
5
IN :
SELECT  
 *  
FROM  
 table_name 
WHERE  
 column_name IN ('value_1','value_2','value_3');
Si los valores son números, se separan entre sí por comas: IN (3,7,9) . Si son cadenas, 
se ponen entre comillas simples y, de nuevo, se separan por comas: IN 
('value_1','value_2','value_3') . La fecha y la hora se indican de la siguiente manera: IN 
('yyyy-mm-dd','yyyy-mm-dd')
Funciones de agregación
Al igual que Python, SQL tiene funciones específicas para calcular el número total de 
filas, sumas, promedios y valores mínimos y máximos. Estas se conocen como 
funciones de agregación. Recopilan o agregan todos los objetos dentro de un grupo 
para calcular un único valor de resumen.
Aquí está la sintaxis de una instrucción con una función de agregación:
SELECT  
 AGGREGATE_FUNCTION(field) AS here_you_are 
-- nombre de la columna donde se almacenará la salida de la función 
FROM 
 table;
Cuando llamas a una función de agregación, esta le da a la columna un nombre difícil 
de manejar. Para evitar esto, usa el comando AS y escribe uno nuevo y más simple.
La función COUNT devuelve el número de filas en una tabla:
SELECT  
 COUNT(*) AS cnt 
FROM 
 table
El número de filas se puede calcular de varias maneras dependiendo de la tarea:
Resumen del capítulo: SQL como herramienta para trabajar con datos
6
COUNT(*) devuelve el número total de filas de la tabla
COUNT(column) devuelve el número de filas en una columna column
COUNT(DISTINCT column) devuelve el número de filas únicas en una columna 
column
La función SUM(column) devuelve la suma de los valores de una column . Ignora los 
valores ausentes.
El AVG (column) devuelve el valor promedio de los valores de column .
Los valores más pequeños y más grandes se pueden encontrar utilizando las funciones 
MIN y MAX.
Conversión de tipos de datos
Algunas funciones de agregación solo se pueden usar con valores numéricos.
Los datos de un campo pueden parecer números, pero en realidad se almacenan como 
cadenas en la base de datos. Esto sucede a menudo y se debe sobre todo a errores en 
el diseño de bases de datos.
Podemos usar una instrucción CAST para convertir el tipo de datos de valores en una 
columna:
CAST (column_name AS data_type)
column_name  es el campo cuyo tipo de datos se va a convertir. data_type  es el tipo 
deseado. También podemos escribir lo siguiente:
column_name :: data_type
Tipos de datos numéricos
número entero: un tipo de número entero similar a int en Python. En PostgreSQL los 
números enteros van desde -2147483648 hasta 2147483647.
real: un número de coma flotante, como float en Python. La precisión numérica para el 
tipo real es de hasta 6 puntos decimales.
Resumen del capítulo: SQL como herramienta para trabajar con datos
7
Tipos de datos de cadena
'Practicum': este es un ejemplo de un valor de tipo cadena. En las instrucciones SQL 
va entre comillas simples.
varchar(n): una cadena de caracteres de longitud variable, donde n es el número 
máximo de caracteres.
texto: una cadena de cualquier longitud. Este tipo es igual que el tipo de string en 
Python.
Fecha y hora
Las fechas y horas van entre comillas simples.
timestamp: una fecha y hora. Análogo a datetime  en pandas. Este formato se usa con 
mayor frecuencia para almacenar eventos que ocurren varias veces al día, como 
registros de usuarios de sitios web.
date: una fecha
Lógico
booleano — un tipo de datos lógicos. Puede tener tres valores en PostgreSQL: TRUE, 
FALSE y NULL (desconocido).

### Resumen_del_captulo_Vectores_y_operaciones_vectoriales.pdf ###
Resumen del capítulo: Vectores y operaciones vectoriales
1
Resumen del capítulo: Vectores 
y operaciones vectoriales
Creación de vectores
En Python, los conjuntos de datos a menudo se representan como listas. En 
matemáticas, un conjunto ordenado de datos numéricos es un vector, o un vector 
aritmético. Las operaciones que se puedan realizar con números, es decir, sumas, 
restas y multiplicaciones, también se podrán realizar con vectores. En Python, las 
operaciones con vectores son cientos de veces más rápidas que las operaciones con 
listas.
Para trabajar con vectores, volvamos a la librería NumPy. Convierte una lista de dos 
números en un vector:
import numpy as np 
 
numbers1 = [2, 3] # Lista de Python 
vector1 = np.array(numbers1) # Matriz de NumPy 
print(vector1)
Los vectores se pueden crear sin una variable temporal:
import numpy as np 
vector2 = np.array([6, 2]) 
print(vector2)
Los vectores se pueden convertir en listas:
numbers2 = list(vector2) # Lista a partir de vector 
print(numbers2)
La columna de la estructura de DataFrame en pandas se convierte en un vector NumPy 
utilizando el atributo values :
Resumen del capítulo: Vectores y operaciones vectoriales
2
import pandas as pd 
 
data = pd.DataFrame([1, 7, 3]) 
print(data[0].values)
Utiliza la función len()  para determinar el tamaño del vector (número de sus 
elementos):
print(len(vector2))
Presentación de vectores
Tracemos un vector bidimensional. Consta de dos números. El primero es la 
coordenada en el eje horizontal x y el segundo es la coordenada en el eje vertical y. El 
vector se representa mediante un punto o una flecha, que une el origen y el punto con 
coordenadas (x, y). Cuando queremos indicar los movimientos usamos flechas. Si 
trabajamos con varios vectores que se encuentran en la misma línea, es mejor utilizar 
un punto para representar un vector.
Los elementos vectoriales también se denominan coordenadas.
import numpy as np 
import matplotlib.pyplot as plt 
 
vector1 = np.array([2, 3]) 
vector2 = np.array([6, 2]) 
 
plt.figure(figsize=(7, 7)) 
plt.axis([0, 7, 0, 7]) 
# El argumento 'ro' establece el estilo del gráfico 
# 'r' - rojo 
# 'o' - círculo 
plt.plot([vector1[0], vector2[0]], [vector1[1], vector2[1]], 'ro')  
plt.grid(True) 
plt.show()
Resumen del capítulo: Vectores y operaciones vectoriales
3
Vamos a utilizar flechas para dibujar los mismos vectores. En lugar de plt.plot() , llama 
a plt.arrow() .
import numpy as np 
import matplotlib.pyplot as plt 
 
vector1 = np.array([2, 3]) 
vector2 = np.array([6, 2]) 
 
plt.figure(figsize=(7, 7)) 
plt.axis([0, 7, 0, 7]) 
plt.arrow(0, 0, vector1[0], vector1[1], head_width=0.3, 
          length_includes_head="True", color='b') 
plt.arrow(0, 0, vector2[0], vector2[1], head_width=0.3, 
          length_includes_head="True", color='g') 
plt.plot(0, 0, 'ro') 
plt.grid(True) 
plt.show()
Resumen del capítulo: Vectores y operaciones vectoriales
4
Suma y resta de vectores
Los vectores del mismo tamaño tienen la misma longitud. El resultado de su suma es el 
vector con cada coordenada igual a la suma de las coordenadas de los vectores 
sumandos. La primera coordenada del vector resultante es igual a la suma de las 
primeras coordenadas y la segunda es la suma de las segundas coordenadas. A la 
hora de restar, cada coordenada del vector resultante es igual a la diferencia entre 
coordenadas de los vectores dados.
Al sumar o restar vectores, se hace la operación para cada elemento de estos.
Resumen del capítulo: Vectores y operaciones vectoriales
5
import numpy as np 
 
vector1 = np.array([2, 3]) 
vector2 = np.array([6, 2]) 
sum_of_vectors = vector1 + vector2 
subtraction_of_vectors = vector2 - vector1
Si trazamos un vector que sea igual al vector1  verde en términos de longitud y 
dirección, desde el final del vector2  azul, obtendremos el vector rojo ( sum_of_vectors ).
Resumen del capítulo: Vectores y operaciones vectoriales
6
El triángulo obtenido en el gráfico anterior nos da el sentido geométrico de la suma de 
vectores. Si cada vector es un movimiento en una dirección determinada, la suma de 
dos vectores es el movimiento a lo largo del primer vector seguido del movimiento a lo 
largo del segundo.
La diferencia entre dos vectores es un paso, por ejemplo, a lo largo del vector2 , 
seguido de un paso en la dirección opuesta al vector1 .
Resumen del capítulo: Vectores y operaciones vectoriales
7
Multiplicación de un vector por un escalar
Además de la suma y la resta, los vectores también se pueden multiplicar por 
escalares. Cada coordenada del vector se multiplica por el mismo número:
En caso de que el número sea negativo, todas las coordenadas también cambiarán de 
signo.
Resumen del capítulo: Vectores y operaciones vectoriales
8
import numpy as np 
 
vector1 = np.array([2, 3]) 
vector3 = 2 * vector1 
vector4 = -1 * vector1
Cuando se multiplican por un número positivo, los vectores mantienen su dirección en 
el plano, pero las flechas cambian de longitud. Cuando se multiplican por un número 
negativo, los vectores cambian al sentido opuesto.
Valor medio de los vectores
Si, por ejemplo, los vectores individuales de un conjunto describen a los clientes en 
función de sus características, entonces el valor medio de los vectores suele describir a 
un cliente típico o estadísticamente promedio. Para el conjunto de vectores 𝑎1, 𝑎2… 
Resumen del capítulo: Vectores y operaciones vectoriales
9
𝑎𝑛 (donde n es el número total de vectores), el valor medio de los vectores es la suma 
de todos los vectores multiplicada por 
Esto da como resultado un nuevo vector a:
Si el conjunto está formado por un solo vector (𝑛=1), será igual a la media: 𝑎=𝑎1. El 
valor medio de dos vectores es 𝑎=0.5(𝑎1+𝑎2). El valor medio de un par de vectores 
bidimensionales es la mitad del segmento que une 𝑎1 y 𝑎2.
import numpy as np 
 
vector1 = np.array([2, 3]) 
vector2 = np.array([6, 2]) 
vector_mean = .5*(vector1+vector2) 
print(vector_mean)
La primera coordenada del nuevo vector es el valor medio de las primeras coordenadas 
del vector1  y del vector2 , y la segunda coordenada es el valor medio de las segundas 
coordenadas del vector1  y del vector2 .
Así es como dibujamos estos vectores en el plano: trazamos el vector vector 1+vector2  
y luego lo multiplicamos por 0.5.
1/n.
Resumen del capítulo: Vectores y operaciones vectoriales
10
Funciones vectorizadas
Las herramientas de NumPy nos permiten realizar varias operaciones con vectores. Si 
usamos la función np.array()  después de multiplicar y dividir dos arreglos del mismo 
tamaño, obtendremos un nuevo vector que también tendrá el mismo tamaño:
import numpy as np 
 
array1 = np.array([2, -4, 6, -8]) 
array2 = np.array([1, 2, 3, 4]) 
array_mult = array1 * array2 
array_div = array1 / array2 
print("Producto de dos matrices: ", array_mult) 
print("Cociente de dos matrices: ", array_div)
Si las operaciones aritméticas se realizan sobre una matriz y un solo número, la acción 
se aplica a cada elemento de la matriz. Y de nuevo, se forma una matriz del mismo 
tamaño.
Resumen del capítulo: Vectores y operaciones vectoriales
11
Para probar el punto, vamos a realizar sumas, restas y divisiones en una matriz con un 
escalar:
import numpy as np 
 
array2 = np.array([1, 2, 3, 4]) 
array2_plus_10 = array2 + 10 
array2_minus_10 = array2 - 10 
array2_div_10 = array2 / 10 
print("Suma: ", array2_plus_10)  
print("Resta: ", array2_minus_10) 
print("Cociente: ", array2_div_10)
El mismo principio de "elemento por elemento" se aplica a las matrices cuando 
tratamos con operaciones matemáticas estándar como las de exponentes o logaritmos.
Vamos a elevar una matriz a la segunda potencia:
import numpy as np 
 
numbers_from_0 =  np.array([0, 1, 2, 3, 4]) 
squares = numbers_from_0**2 
print(squares)
Todo esto también lo podemos hacer con listas a través de bucles, pero las 
operaciones con vectores en NumPy son mucho más rápidas.
Aquí hay un ejemplo: entre los elementos de la matriz values , el valor máximo y 
mínimo son un par de números, MIN y MAX, siempre que MAX>MIN. Para el análisis 
se deben convertir los datos. Cada elemento de la matriz debe convertirse a un número 
en el rango de 0(MIN) a 1(MAX). Esta es la fórmula de la función min_max_scale() :
Resumen del capítulo: Vectores y operaciones vectoriales
12
Para aplicar esta función a todos los elementos de la matriz values , llama a los 
métodos max()  y min() . Estos encontrarán sus valores máximos y mínimos. Como 
resultado, obtenemos una matriz de la misma longitud, pero con elementos 
convertidos:
import numpy as np 
def min_max_scale(values): 
    return (values - min(values)) / (max(values) - min(values)) 
 
print(min_max_scale(our_values))
donde exp() es la función exponente (del lat. exponere, "exponer"). Eleva e, el número 
de Euler, a la potencia del argumento. Este número recibió el nombre del gran 
matemático suizo Leonhard Euler y es aproximadamente igual a 2.718281828.
Resumen del capítulo: Vectores y operaciones vectoriales
13
Realiza la transformación logística:
import numpy as np 
 
def logistic_transform(values): 
    return 1 / (1 + np.exp(- values)) 
 
print(logistic_transform(our_values))
Vectorización de métricas
Almacena un conjunto de valores reales en la variable target  y valores pronosticados 
en la variable predictions . Ambos conjuntos son de tipo np.array .
Utiliza las funciones estándar de NumPy para calcular las métricas de evaluación:
sum()  (para encontrar la suma de los elementos de una matriz)
Resumen del capítulo: Vectores y operaciones vectoriales
14
mean()  (para calcular el valor medio)
Llámalas de la siguiente manera: <nombre de la matriz>.sum()  y <nombre de la 
matriz>.mean() .
Por ejemplo, esta es la fórmula para calcular el error cuadrático medio (ECM):
donde 𝑛 es la longitud de cada matriz y ∑ es la suma de todas las observaciones de la 
muestra (𝑖 varía de 1 a 𝑛). Los elementos ordinales de los vectores target (objetivo) y 
predictions (predicciones) se denotan mediante target_𝑖 y predictions_𝑖. 
Escribe la fórmula utilizando sum() :
def mse1(target, predictions): 
    n = target.size 
    return((target - predictions)**2).sum()/n
La suma de varios números dividida por su cantidad es la media de dichos números. 
Vamos a escribir la fórmula de ECM utilizando mean() :
def mse2(target, predictions): 
    return((target - predictions)**2).mean()
Escribe la función para calcular el EAM utilizando mean() :
Resumen del capítulo: Vectores y operaciones vectoriales
15
import numpy as np 
 
def mae(target, predictions): 
    return np.abs((target - predictions)).mean() 
 
print(mae(target, predictions))
Las funciones vectorizadas se pueden utilizar para calcular el RECM. Aquí tienes la 
fórmula:
import numpy as np 
 
def rmse(target, predictions): 
    return (((target-predictions)**2).mean())**0.5  
 
print(rmse(target, predictions))
